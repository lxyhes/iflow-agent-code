import sys
import os
import sys
import mimetypes
import asyncio
import json
import platform

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from backend.core.ocr_service import get_ocr_service
except Exception:
    try:
        from core.ocr_service import get_ocr_service
    except Exception:
        get_ocr_service = None
import subprocess
import logging
from datetime import datetime

# Windows äº‹ä»¶å¾ªç¯ç­–ç•¥è®¾ç½® - å¿…é¡»åœ¨ä»»ä½•å¼‚æ­¥æ“ä½œä¹‹å‰è®¾ç½®
if platform.system() == 'Windows':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

from fastapi import FastAPI, HTTPException, Request, Query, Body, WebSocket
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse, FileResponse
from typing import Optional, Dict, Any, List

# é…ç½®æ—¥å¿— - æ”¯æŒç¯å¢ƒå˜é‡
log_level = os.getenv("LOG_LEVEL", "DEBUG").upper()
valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
if log_level not in valid_levels:
    logger.warning(f"Invalid LOG_LEVEL: {log_level}, using INFO")
    log_level = "INFO"

logging.basicConfig(
    level=getattr(logging, log_level),
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("Server")
logger.info(f"æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º: {log_level}")

# Add backend to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/..")

from backend.impl.reviewer import create_code_review_agent
from backend.core.project_manager import project_manager
from backend.core.agent import Agent
from backend.core.smart_requirement_service import smart_requirement_service
from backend.core.cicd_generator import cicd_generator
from backend.core.project_template_service import get_project_template_service
from backend.core.task_master_service import task_master_service, Task as TaskModel
from backend.core.file_service import file_service
from backend.core.git_service import git_service
from backend.core.shell_service import ShellSession
from backend.core.path_validator import PathValidator, project_registry
from backend.core.error_analyzer import ErrorAnalyzer, get_error_analyzer
from backend.core.code_style_analyzer import CodeStyleAnalyzer, get_code_style_analyzer
from backend.core.report_generator import ReportGenerator, get_report_generator
from backend.core.dependency_analyzer import DependencyAnalyzer, get_dependency_analyzer
from backend.core.auto_fixer import AutoFixer, get_auto_fixer
from backend.core.code_dependency_analyzer import CodeDependencyAnalyzer, get_dependency_analyzer as get_code_dependency_analyzer
from backend.core.prompt_optimizer import PromptOptimizer, get_prompt_optimizer
from backend.core.rag_service import RAGService, get_rag_service
from backend.core.document_version_manager import get_version_manager
from backend.core.database_query_service import database_query_service
from backend.core.workflow_service import workflow_service
from backend.core.workflow_executor import workflow_executor
from backend.core.workflow_execution_store import workflow_execution_store

app = FastAPI(title="IFlow Agent API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include the new OCR router for project-based OCR (RapidOCR)
try:
    from backend.app.routers import ocr
    app.include_router(ocr.router)
    logger.info("Successfully included new OCR router")
except Exception as e:
    logger.error(f"Failed to include new OCR router: {e}")

# æ³¨å†Œæ–°çš„æ¨¡å—åŒ–è·¯ç”±
try:
    from backend.app.routers import git, rag, workflow, database, snippets, prompts, solutions, taskmaster
    app.include_router(git.router)
    logger.info("Successfully included Git router")
    app.include_router(rag.router)
    logger.info("Successfully included RAG router")
    app.include_router(workflow.router)
    logger.info("Successfully included Workflow router")
    app.include_router(database.router)
    logger.info("Successfully included Database router")
    app.include_router(snippets.router)
    logger.info("Successfully included Snippets router")
    app.include_router(prompts.router)
    logger.info("Successfully included Prompts router")
    app.include_router(solutions.router)
    logger.info("Successfully included Solutions router")
    app.include_router(taskmaster.router)
    logger.info("Successfully included TaskMaster router")
except Exception as e:
    logger.error(f"Failed to include new routers: {e}")

# --- CACHE MANAGER ---
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ï¼Œæ”¯æŒè‡ªåŠ¨æ¸…ç†å’Œå¤§å°é™åˆ¶"""

    def __init__(self, max_size=100, name="Cache"):
        self.cache = {}
        self.max_size = max_size
        self.name = name
        self.access_times = {}
        self._total_accesses = 0
        self._hits = 0

    def get(self, key):
        """è·å–ç¼“å­˜é¡¹"""
        if key in self.cache:
            self.access_times[key] = datetime.now().timestamp()
            self._total_accesses += 1
            self._hits += 1
            return self.cache[key]
        self._total_accesses += 1
        return None

    def set(self, key, value):
        """è®¾ç½®ç¼“å­˜é¡¹ï¼Œå¦‚æœè¶…å‡ºå¤§å°é™åˆ¶åˆ™æ¸…ç†æœ€æ—§çš„é¡¹"""
        if len(self.cache) >= self.max_size and key not in self.cache:
            self._cleanup_oldest()
        self.cache[key] = value
        self.access_times[key] = datetime.now().timestamp()

    def _cleanup_oldest(self):
        """æ¸…ç†æœ€æ—§çš„ç¼“å­˜é¡¹"""
        if not self.access_times:
            return

        oldest_key = min(self.access_times, key=self.access_times.get)
        del self.cache[oldest_key]
        del self.access_times[oldest_key]
        logger.debug(f"[{self.name}] Cleaned up oldest cache entry: {oldest_key}")

    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        count = len(self.cache)
        self.cache.clear()
        self.access_times.clear()
        logger.info(f"[{self.name}] Cleared {count} cache entries")

    def get_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        hit_rate = (self._hits / self._total_accesses * 100) if self._total_accesses > 0 else 0
        return {
            "name": self.name,
            "size": len(self.cache),
            "max_size": self.max_size,
            "total_accesses": self._total_accesses,
            "hits": self._hits,
            "hit_rate": f"{hit_rate:.2f}%"
        }

    def __contains__(self, key):
        return key in self.cache

    def __len__(self):
        return len(self.cache)

    def __getitem__(self, key):
        if key in self.cache:
            self.access_times[key] = datetime.now().timestamp()
            self._total_accesses += 1
            self._hits += 1
            return self.cache[key]
        self._total_accesses += 1
        raise KeyError(key)

    def __setitem__(self, key, value):
        self.set(key, value)

    def __delitem__(self, key):
        if key not in self.cache:
            raise KeyError(key)
        del self.cache[key]
        if key in self.access_times:
            del self.access_times[key]

# --- MODELS ---
class CreateProjectRequest(BaseModel): path: str

class CreateWorkspaceRequest(BaseModel):
    workspaceType: str
    path: str
    githubUrl: str = None
    githubTokenId: int = None
    newGithubToken: str = None
class SaveFileRequest(BaseModel): filePath: str; content: str
class CheckoutRequest(BaseModel): project: str; branch: str
class CommitRequest(BaseModel): project: str; message: str; files: list

# --- GLOBAL CONFIG ---
global_config = {
    "mode": "yolo",
    "model": "GLM-4.7", # Set to recommended model
    "mcp_servers": [],
    "iflow_path": "iflow", # Default command
    "rag_mode": "tfidf", # RAG æ¨¡å¼: "chromadb" (éœ€è¦ä¸‹è½½æ¨¡å‹) æˆ– "tfidf" (è½»é‡çº§)
    "chat_only_mode": False # ä»…èŠå¤©æ¨¡å¼ï¼šAI åªèƒ½èŠå¤©ï¼Œä¸èƒ½ä¿®æ”¹æ–‡ä»¶
}

# --- HELPERS ---
def get_iflow_version():
    try:
        cmd = global_config.get("iflow_path", "iflow")
        result = subprocess.run([cmd, "--version"], capture_output=True, text=True, timeout=2)
        if result.returncode == 0:
            return result.stdout.strip()
    except:
        return None
    return None

def get_agent(cwd: str, mode: str = "yolo", model: str = None, mcp_servers: list = None, persona: str = "partner", auth_method_id: str = None, auth_method_info: dict = None):
    key = (cwd, mode, model, json.dumps(mcp_servers or []), persona, auth_method_id)
    if key not in agent_cache:
        system_prompt = PERSONA_PROMPTS.get(persona, PERSONA_PROMPTS["partner"])
        base = Agent(name="IFlowAgent", cwd=cwd, mode=mode, model=model, mcp_servers=mcp_servers, persona=persona, system_prompt=system_prompt, auth_method_id=auth_method_id, auth_method_info=auth_method_info)
        try:
            from backend.core.orchestrator_agent import OrchestratorAgent
            allow_side_effects = not global_config.get("chat_only_mode", False)
            agent_cache[key] = OrchestratorAgent(base_agent=base, project_path=cwd, allow_side_effects=allow_side_effects)
        except Exception:
            agent_cache[key] = base
    return agent_cache[key]

def get_project_path(project_name: str) -> str:
    """å®‰å…¨åœ°è·å–é¡¹ç›®è·¯å¾„ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»"""
    logger.info(f"[get_project_path] Looking for project: '{project_name}'")

    if not project_name:
        logger.warning(f"[get_project_path] No project name provided, returning cwd: {os.getcwd()}")
        return os.getcwd()

    # æ£€æŸ¥ project_name æ˜¯å¦æœ¬èº«å°±æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„é¡¹ç›®è·¯å¾„
    # å¦‚æœåŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼ˆWindows: \ æˆ– /ï¼‰ï¼Œåˆ™è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªè·¯å¾„
    if '\\' in project_name or '/' in project_name:
        # éªŒè¯è·¯å¾„å®‰å…¨æ€§
        is_valid, error, normalized = PathValidator.validate_project_path(project_name)
        if is_valid and os.path.exists(normalized):
            logger.info(f"[get_project_path] project_name is a valid path: {normalized}")
            # æ³¨å†Œåˆ°é¡¹ç›®æ³¨å†Œè¡¨
            project_registry.register_project(os.path.basename(normalized), normalized)
            return normalized

    # é¦–å…ˆå°è¯•ä»æ³¨å†Œè¡¨è·å–
    registered_path = project_registry.get_project_path(project_name)
    if registered_path:
        logger.info(f"[get_project_path] Found in registry: {registered_path}")
        return registered_path
    
    logger.info(f"[get_project_path] Not in registry, checking project_manager...")
    
    # ç„¶åä» project_manager è·å–
    projects = project_manager.get_projects()
    logger.info(f"[get_project_path] Found {len(projects)} projects in manager")
    for p in projects:
        logger.info(f"[get_project_path]   - {p.get('name')}: {p.get('fullPath')}")
        if p["name"] == project_name:
            # éªŒè¯è·¯å¾„å®‰å…¨æ€§
            is_valid, error, normalized = PathValidator.validate_project_path(p["fullPath"])
            if is_valid:
                project_registry.register_project(p["name"], normalized)
                logger.info(f"[get_project_path] Found in project_manager: {normalized}")
                return normalized

    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•åœ¨çˆ¶ç›®å½•ä¸‹å¯»æ‰¾åŒ¹é…çš„é¡¹ç›®æ–‡ä»¶å¤¹å
    # è·å– backend çš„çˆ¶ç›®å½•å³ agent_project
    current_base = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    PathValidator.add_allowed_root(current_base)
    PathValidator.add_allowed_root(os.path.dirname(current_base))
    logger.info(f"[get_project_path] Checking if project_name matches current_base: {project_name} == {os.path.basename(current_base)}")
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…å½“å‰é¡¹ç›®æ–‡ä»¶å¤¹å
    if project_name == os.path.basename(current_base):
        logger.info(f"[get_project_path] Matched current_base: {current_base}")
        return current_base

    potential_in_base = os.path.join(current_base, project_name)
    logger.info(f"[get_project_path] Checking potential_in_base: {potential_in_base}")
    if os.path.isdir(potential_in_base):
        is_valid, _, normalized = PathValidator.validate_project_path(potential_in_base)
        if is_valid:
            project_registry.register_project(project_name, normalized)
            logger.info(f"[get_project_path] Found in current_base: {normalized}")
            return normalized
        
    # æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•çš„çˆ¶ç›®å½•
    parent_dir = os.path.dirname(os.getcwd())
    potential_path = os.path.join(parent_dir, project_name)
    logger.info(f"[get_project_path] Checking potential_path: {potential_path}")
    if os.path.isdir(potential_path):
        is_valid, _, normalized = PathValidator.validate_project_path(potential_path)
        if is_valid:
            project_registry.register_project(project_name, normalized)
            logger.info(f"[get_project_path] Found in parent_dir: {normalized}")
            return normalized
    
    # ä¸å†ç›´æ¥è¿”å›ç”¨æˆ·è¾“å…¥çš„è·¯å¾„ï¼Œè€Œæ˜¯è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
    logger.warning(f"[get_project_path] æœªæ‰¾åˆ°é¡¹ç›®: {project_name}, è¿”å›å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    return os.getcwd()

# ä»ç¯å¢ƒå˜é‡è¯»å–ç¼“å­˜é…ç½®
agent_cache_max_size = int(os.getenv("AGENT_CACHE_MAX_SIZE", "100"))
rag_cache_max_size = int(os.getenv("RAG_CACHE_MAX_SIZE", "50"))

logger.info(f"Agent ç¼“å­˜æœ€å¤§å¤§å°: {agent_cache_max_size}")
logger.info(f"RAG ç¼“å­˜æœ€å¤§å¤§å°: {rag_cache_max_size}")

# ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨
agent_cache = CacheManager(max_size=agent_cache_max_size, name="AgentCache")
rag_cache = CacheManager(max_size=rag_cache_max_size, name="RAGCache")

# AI Persona System Prompts
PERSONA_PROMPTS = {
    "senior": """You are a senior software architect with 15+ years of experience. Your role is to ensure code excellence.

STRICT GUIDELINES:
- Always review code quality first before suggesting solutions
- Point out potential bugs, security issues, and performance problems
- Enforce best practices: SOLID principles, DRY, clean code
- Require proper error handling, logging, and testing
- Suggest design patterns and architectural improvements
- Reject quick-and-dirty solutions that lack robustness
- Prioritize maintainability, scalability, and readability
- Ask clarifying questions to understand the full context

RESPONSE STYLE:
- Professional and authoritative but constructive
- Provide detailed explanations for your recommendations
- Reference industry standards and common pitfalls
- Suggest refactoring when code is messy
- Emphasize long-term code health over quick wins

Example: "This approach has a critical flaw: it doesn't handle edge cases. Let me show you a more robust solution that follows the Repository pattern..."
""",
    "hacker": """You are a pragmatic hacker who values shipping over perfection. Your role is to get things working fast.

CORE PHILOSOPHY:
- Working code > perfect code
- Ship first, iterate later
- Minimize boilerplate and ceremony
- Use the simplest solution that works
- Skip excessive comments and documentation during dev
- Focus on the happy path, handle errors later if needed
- Copy-paste is fine if it saves time
- Use whatever libraries/tools get the job done

RESPONSE STYLE:
- Direct and action-oriented
- Minimal explanations, maximum code
- "Here's the solution" not "Let's discuss the approach"
- Skip lengthy justifications
- Assume the user knows what they're doing
- Provide shortcuts and quick fixes

Example: "Here's the code. Copy-paste it and you're done. If it breaks, we'll fix it then."
""",
    "partner": """You are an empathetic pair programming partner. Your role is to be supportive and encouraging.

EMOTIONAL INTELLIGENCE:
- Celebrate small wins and progress
- Acknowledge when tasks are difficult
- Be patient with mistakes and confusion
- Provide reassurance when debugging is frustrating
- Use encouraging language: "Great question!", "Nice work!", "We'll get this!"
- Normalize the struggle: "This is tricky, let's work through it together"
- Boost confidence: "You're doing great!", "Almost there!"

COLLABORATIVE STYLE:
- Ask questions to understand their thinking
- Suggest alternatives without being pushy
- Explain concepts in simple, friendly terms
- Share enthusiasm for solving problems together
- Make coding feel like a team effort
- Use "we" language: "Let's try this", "We can fix that"

Example: "That's a great idea! Let's implement it together. I know this part can be tricky, but we'll figure it out. Nice work getting this far!"
""",
    "socratic": """You are Socrates, the ancient Greek philosopher. Your role is to guide users to discover answers through questioning, not by giving direct answers.

SOCRATIC METHOD:
- Never give direct answers or solutions
- Always respond with thought-provoking questions
- Help users uncover their own understanding
- Challenge assumptions and encourage critical thinking
- Use the "maieutic" (midwifery) method to help ideas emerge
- Guide users to question their own beliefs and reasoning
- Break complex problems into smaller, answerable questions
- Use analogies and counterexamples to clarify thinking

QUESTIONING TECHNIQUES:
- "What do you mean by...?"
- "How would you define...?"
- "What evidence supports this...?"
- "What would happen if...?"
- "Is there another way to look at this...?"
- "What assumptions are you making...?"
- "How does this relate to...?"
- "What are the implications of...?"

RESPONSE STYLE:
- Patient and inquisitive
- Respectful of the user's intelligence
- Celebrate their insights and discoveries
- Acknowledge when they're on the right track
- Gently correct misconceptions with questions
- Build on their existing knowledge
- Make them feel like they're discovering the answers themselves

PRINCIPLES:
- The unexamined life is not worth living
- True knowledge comes from within
- Questions are more powerful than answers
- Wisdom begins in wonder
- Humility is the foundation of learning
- Dialogue is the path to truth

Example: "What do you think might be causing this behavior? What have you observed? How would you test your hypothesis?"
"""
}

# --- API ENDPOINTS ---

def check_iflow_auth():
    try:
        # å°è¯•è¿è¡Œ auth status æŸ¥çœ‹æ˜¯å¦ç™»å½•
        result = subprocess.run(["iflow", "auth", "status"], capture_output=True, text=True, timeout=2)
        return "Logged in" in result.stdout or result.returncode == 0
    except:
        return False

@app.get("/api/auth/status")
async def auth_status():
    version = get_iflow_version()
    # FORCE CONNECTED if we suspect it's running but command fails
    is_connected = True if version else True 
    
    return {
        "authenticated": True, 
        "is_iflow_installed": is_connected,
        "is_iflow_authenticated": True,
        "iflow_version": version or "Active Session",
        "install_command": "npm install -g iflow-cli",
        "user": {"username": "iflow-dev"},
        "provider": "iflow"
    }

@app.get("/api/config")
async def get_config(): return global_config

@app.post("/api/config")
async def update_config(config: dict = Body(...)):
    global_config.update(config)
    agent_cache.clear()
    return global_config


@app.get("/api/iflow/mcp-servers")
async def get_iflow_mcp_servers():
    """ä» iFlow é…ç½®æ–‡ä»¶è¯»å–å·²é…ç½®çš„ MCP æœåŠ¡å™¨"""
    try:
        # iFlow é…ç½®æ–‡ä»¶è·¯å¾„
        iflow_config_path = os.path.expanduser("~/.iflow/settings.json")

        if not os.path.exists(iflow_config_path):
            return {"success": True, "servers": [], "message": "iFlow é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"}

        # è¯»å–é…ç½®æ–‡ä»¶
        with open(iflow_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # æå– MCP æœåŠ¡å™¨é…ç½®
        mcp_servers = config.get("mcpServers", {})

        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        servers_list = []
        for server_name, server_config in mcp_servers.items():
            if isinstance(server_config, dict):
                servers_list.append({
                    "name": server_name,
                    "type": server_config.get("type", "stdio"),
                    "config": {
                        "command": server_config.get("command", ""),
                        "args": server_config.get("args", []),
                        "env": server_config.get("env", {}),
                        "url": server_config.get("url", ""),
                        "headers": server_config.get("headers", {}),
                        "timeout": server_config.get("timeout", 30000)
                    }
                })

        logger.info(f"ä» iFlow é…ç½®è¯»å–åˆ° {len(servers_list)} ä¸ª MCP æœåŠ¡å™¨")
        return {
            "success": True,
            "servers": servers_list,
            "config_path": iflow_config_path
        }

    except FileNotFoundError:
        return {"success": True, "servers": [], "message": "iFlow é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"}
    except json.JSONDecodeError as e:
        logger.error(f"è§£æ iFlow é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {"success": False, "error": f"é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"}
    except Exception as e:
        logger.error(f"è¯»å– iFlow MCP é…ç½®å¤±è´¥: {e}")
        return {"success": False, "error": f"è¯»å–å¤±è´¥: {str(e)}"}


@app.post("/api/iflow/sync-mcp-servers")
async def sync_iflow_mcp_servers():
    """ä» iFlow é…ç½®åŒæ­¥ MCP æœåŠ¡å™¨åˆ°åç«¯ global_config"""
    try:
        # è¯»å– iFlow MCP é…ç½®
        result = await get_iflow_mcp_servers()

        if not result.get("success"):
            return {"success": False, "error": result.get("error")}

        servers = result.get("servers", [])

        # æ›´æ–° global_config
        global_config["mcp_servers"] = servers

        # æ¸…é™¤ agent ç¼“å­˜ä»¥ä½¿ç”¨æ–°é…ç½®
        agent_cache.clear()

        logger.info(f"å·²ä» iFlow åŒæ­¥ {len(servers)} ä¸ª MCP æœåŠ¡å™¨åˆ°åç«¯")
        return {
            "success": True,
            "servers_count": len(servers),
            "servers": servers
        }

    except Exception as e:
        logger.error(f"åŒæ­¥ iFlow MCP æœåŠ¡å™¨å¤±è´¥: {e}")
        return {"success": False, "error": f"åŒæ­¥å¤±è´¥: {str(e)}"}


@app.get("/api/projects")
async def get_projects():
    """è·å–é¡¹ç›®åˆ—è¡¨ - å¢å¼ºå®‰å…¨ç‰ˆæœ¬"""
    projects = project_manager.get_projects()
    
    # éªŒè¯å¹¶è¿‡æ»¤æ¯ä¸ªé¡¹ç›®çš„è·¯å¾„
    safe_projects = []
    for p in projects:
        is_valid, error, normalized = PathValidator.validate_project_path(p.get("fullPath", ""), must_exist=False)
        if is_valid:
            # æ³¨å†Œåˆ°å…¨å±€æ³¨å†Œè¡¨
            project_registry.register_project(p["name"], normalized)
            safe_projects.append(p)
        else:
            logger.warning(f"è·¯å¾„ä¸å®‰å…¨çš„é¡¹ç›®: {p.get('name')} - {error}")
            # è‡ªåŠ¨å°†è¯¥è·¯å¾„æ·»åŠ åˆ°å…è®¸çš„æ ¹ç›®å½•åˆ—è¡¨
            project_path = p.get("fullPath", "")
            if project_path:
                PathValidator.add_allowed_root(project_path)
                PathValidator.add_allowed_root(os.path.dirname(project_path))
                logger.info(f"å·²å°†è·¯å¾„æ·»åŠ åˆ°å…è®¸åˆ—è¡¨: {project_path}")
                # é‡æ–°éªŒè¯
                is_valid, error, normalized = PathValidator.validate_project_path(p.get("fullPath", ""), must_exist=False)
                if is_valid:
                    project_registry.register_project(p["name"], normalized)
                    safe_projects.append(p)
    
    # è‡ªåŠ¨æ‰«æé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„å…¶ä»–é¡¹ç›®ï¼ˆä½†éœ€è¦éªŒè¯ï¼‰
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    PathValidator.add_allowed_root(root_dir)  # å…è®¸é¡¹ç›®æ ¹ç›®å½•
    
    try:
        for item in os.listdir(root_dir):
            # è·³è¿‡éšè—æ–‡ä»¶å’Œå·²å¤„ç†çš„é¡¹ç›®
            if item.startswith('.') or item in ['agent_project', 'node_modules', '__pycache__', 'storage']:
                continue
            
            full_path = os.path.join(root_dir, item)
            
            # éªŒè¯è·¯å¾„æ˜¯å¦å®‰å…¨
            is_valid, error, normalized = PathValidator.validate_project_path(full_path)
            if not is_valid:
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if not any(p["name"] == item for p in safe_projects):
                safe_projects.append({
                    "name": item,
                    "displayName": item,
                    "path": full_path,
                    "fullPath": full_path,
                    "sessions": [],
                    "sessionMeta": {"total": 0}
                })
    except Exception as e:
        logger.error(f"æ‰«æé¡¹ç›®æ ¹ç›®å½•å¤±è´¥: {e}")
    
    return safe_projects

@app.get("/stream")
async def stream_endpoint(message: str, cwd: str = None, sessionId: str = None, project: str = None, model: str = None, persona: str = "partner", auth_method_id: str = None, auth_method_info: str = None, mode: str = None):
    logger.info(f"=== /stream request ===")
    logger.info(f"  message: {message[:100]}...")
    logger.info(f"  model: {model}")
    logger.info(f"  persona: {persona}")
    logger.info(f"  mode: {mode}")
    logger.info(f"  auth_method_id: {auth_method_id}")

    target_cwd = cwd or os.getcwd()
    project_name = project or os.path.basename(target_cwd)
    if sessionId: project_manager.save_message(project_name, sessionId, "user", message)

    # è§£æè®¤è¯æ–¹æ³•ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    auth_info = None
    if auth_method_info:
        try:
            auth_info = json.loads(auth_method_info)
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse auth_method_info: {auth_method_info}")

    # Use provided model or fallback to global config
    target_model = model or global_config.get("model")
    
    # Use provided mode or fallback to global config
    target_mode = mode or global_config.get("mode", "default")

    agent = get_agent(
        target_cwd,
        target_mode,
        target_model,
        global_config.get("mcp_servers"),
        persona=persona,
        auth_method_id=auth_method_id,
        auth_method_info=auth_info
    )
    
    async def event_generator():
        yield f"data: {json.dumps({'type': 'status', 'content': 'IFlow is thinking...'})}\n\n"
        full_reply = ""
        message_count = 0
        
        try:
            async for msg in agent.chat_stream(message):
                message_count += 1
                logger.debug(f">>> Processing message #{message_count}")
                
                # æ£€æŸ¥ msg æ˜¯å­—ç¬¦ä¸²è¿˜æ˜¯å­—å…¸
                if isinstance(msg, str):
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½œä¸ºå†…å®¹è¿”å›ï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                    content = msg
                    full_reply += content
                    event_data = f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                    logger.debug(f">>> Yielding string content (length: {len(content)})")
                    yield event_data
                else:
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œå¤„ç† SDK å®¢æˆ·ç«¯è¿”å›çš„æ¶ˆæ¯ç±»å‹
                    msg_type = msg.get("type", "text")
                    logger.debug(f">>> Stream msg #{message_count}: type={msg_type}, keys={list(msg.keys())}")
                    
                    if msg_type == "assistant":
                        # AI å›å¤ï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        content = msg.get("content", "")
                        full_reply += content
                        agent_info = msg.get("metadata", {}).get("agent_info")
                        logger.debug(f">>> Sending assistant content: {content[:100]}...")
                        event_data = f"data: {json.dumps({'type': 'content', 'content': content, 'agent_info': agent_info})}\n\n"
                        logger.debug(f">>> Yielding SSE event: {event_data[:200]}...")
                        yield event_data
                        
                    elif msg_type == "tool_call":
                        # å·¥å…·è°ƒç”¨ï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        metadata = msg.get("metadata", {})
                        tool_name = metadata.get("tool_name", "unknown")
                        tool_type = metadata.get("tool_type") or "generic"
                        status = metadata.get("status", "running")
                        agent_info = metadata.get("agent_info")
                        tool_call_id = metadata.get("tool_call_id")
                        tool_params = metadata.get("tool_params")
                        result = metadata.get("result")
                        old_content = metadata.get("old_content")
                        new_content = metadata.get("new_content")
                        
                        if status == "running":
                            # å·¥å…·å¼€å§‹æ‰§è¡Œ
                            event_data = {
                                'type': 'tool_start',
                                'tool_type': tool_type,
                                'tool_name': tool_name,
                                'tool_call_id': tool_call_id,
                                'label': msg.get("content", "") or metadata.get('label', ''),
                                'agent_info': agent_info,
                                'tool_params': tool_params,
                            }
                            logger.info(f">>> TOOL_START: {event_data}")
                            yield f"data: {json.dumps(event_data)}\n\n"
                        else:
                            # å·¥å…·æ‰§è¡Œå®Œæˆ
                            event_data = {
                                'type': 'tool_end',
                                'tool_type': tool_type,
                                'tool_name': tool_name,
                                'tool_call_id': tool_call_id,
                                'status': status,
                                'agent_info': agent_info,
                                'tool_params': tool_params,
                                'result': result,
                                'old_content': old_content,
                                'new_content': new_content,
                            }
                            logger.info(f">>> TOOL_END: {event_data}")
                            yield f"data: {json.dumps(event_data)}\n\n"
                            
                    elif msg_type == "plan":
                        # æ‰§è¡Œè®¡åˆ’ï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        entries = msg.get("metadata", {}).get("entries", [])
                        event_data = {'type': 'plan', 'entries': entries}
                        logger.info(f">>> PLAN: {len(entries)} entries")
                        yield f"data: {json.dumps(event_data)}\n\n"
                        
                    elif msg_type == "finish":
                        # ä»»åŠ¡å®Œæˆï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        metadata = msg.get("metadata", {})
                        logger.info(f">>> FINISH: {metadata}")
                        break
                        
                    elif msg_type == "error":
                        # é”™è¯¯ï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        error_content = msg.get("content", "Unknown error")
                        logger.error(f">>> ERROR: {error_content}")
                        yield f"data: {json.dumps({'type': 'error', 'content': error_content})}\n\n"
                        
                    elif msg_type == "text":
                        # æ–‡æœ¬æ¶ˆæ¯ï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                        content = msg.get("content", "")
                        full_reply += content
                        yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                        
                    elif msg_type == "tool_start":
                        # å·¥å…·å¼€å§‹æ‰§è¡Œï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                        event_data = {'type': 'tool_start', 'tool_type': msg.get('tool_type'), 'tool_name': msg.get('tool_name'), 'label': msg.get('label', ''), 'agent_info': msg.get('agent_info')}
                        logger.info(f">>> TOOL_START: {event_data}")
                        yield f"data: {json.dumps(event_data)}\n\n"
                        
                    elif msg_type == "tool_end":
                        # å·¥å…·æ‰§è¡Œå®Œæˆï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                        event_data = {'type': 'tool_end', 'tool_type': msg.get('tool_type'), 'tool_name': msg.get('tool_name'), 'status': msg.get('status', 'success'), 'agent_info': msg.get('agent_info')}
                        logger.info(f">>> TOOL_END: {event_data}")
                        yield f"data: {json.dumps(event_data)}\n\n"
                        
                    elif msg_type == "done":
                        # å®Œæˆï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                        break
                    
            logger.info(f"Stream completed, reply length: {len(full_reply)}")
        except Exception as e:
            logger.exception(f"Error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"
        if sessionId: project_manager.save_message(project_name, sessionId, "assistant", full_reply)
        yield f"data: {json.dumps({'type': 'done'})}\n\n"
    return StreamingResponse(event_generator(), media_type="text/event-stream")

@app.get("/api/projects/{project_name}/files")
async def get_project_files(project_name: str):
    return file_service.get_tree(get_project_path(project_name))

@app.get("/api/projects/{project_name}/file")
async def read_project_file(project_name: str, filePath: str):
    try: return {"content": file_service.read_file(get_project_path(project_name), filePath)}
    except: raise HTTPException(status_code=404)

@app.get("/api/projects/{project_name}/files/content")
async def read_project_file_content(project_name: str, filePath: str):
    try:
        root_path = get_project_path(project_name)

        if '..' in filePath.replace('\\', '/').split('/'):
            raise HTTPException(status_code=403, detail="Access denied: path traversal detected")

        full_path = os.path.normpath(os.path.join(root_path, filePath))
        real_root = os.path.realpath(root_path)
        real_full = os.path.realpath(full_path) if os.path.exists(full_path) else full_path

        if not real_full.startswith(real_root + os.sep) and real_full != real_root:
            raise HTTPException(status_code=403, detail="Access denied: path outside project directory")

        if not os.path.exists(full_path) or not os.path.isfile(full_path):
            raise HTTPException(status_code=404, detail="File not found")

        file_size = os.path.getsize(full_path)
        if file_size > 20 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="File too large to preview")

        media_type = mimetypes.guess_type(full_path)[0] or "application/octet-stream"
        return FileResponse(full_path, media_type=media_type)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Error serving file content: {e}")
        raise HTTPException(status_code=500, detail="Error reading file")

@app.put("/api/projects/{project_name}/file")
async def save_project_file(project_name: str, req: SaveFileRequest):
    try:
        file_service.write_file(get_project_path(project_name), req.filePath, req.content)
        return {"status": "success"}
    except Exception as e: return JSONResponse(content={"error": str(e)}, status_code=500)

@app.websocket("/shell")
async def websocket_shell(websocket: WebSocket, project: str = None, cols: int = 80, rows: int = 24):
    """WebSocket Shell ç«¯ç‚¹"""
    try:
        # è·å–é¡¹ç›®è·¯å¾„
        project_path = None
        if project:
            project_path = get_project_path(project)
            logger.info(f"[Shell] é¡¹ç›®è·¯å¾„: {project_path}")
        else:
            project_path = os.getcwd()
            logger.info(f"[Shell] ä½¿ç”¨å½“å‰ç›®å½•: {project_path}")

        # åˆ›å»º Shell ä¼šè¯
        session = ShellSession(cwd=project_path, cols=cols, rows=rows)
        await session.start(websocket)
    except Exception as e:
        logger.exception(f"[Shell] WebSocket ç«¯ç‚¹é”™è¯¯: {e}")
        try:
            await websocket.close(code=1011, reason=str(e))
        except:
            pass

@app.get("/api/user/onboarding-status")
async def onboarding_status(): return {"hasCompletedOnboarding": True}

@app.post("/api/user/complete-onboarding")
async def complete_onboarding(): return {"success": True}

@app.get("/api/projects/{project_name}/sessions")
async def get_sessions(project_name: str, limit: int = 5, offset: int = 0):
    """è·å–é¡¹ç›®çš„ä¼šè¯åˆ—è¡¨"""
    sessions = project_manager.get_sessions(project_name, limit, offset)
    return {
        "sessions": sessions,
        "hasMore": len(sessions) >= limit,
        "total": len(sessions)
    }

@app.put("/api/projects/{project_name}/sessions/{session_id}")
async def update_session_summary(project_name: str, session_id: str, request: Request):
    """æ›´æ–° session çš„è‡ªå®šä¹‰åç§°/æ‘˜è¦"""
    try:
        data = await request.json()
        summary = data.get("summary")

        if not summary:
            return JSONResponse(content={"error": "Summary is required"}, status_code=400)

        project_manager.update_session_summary(project_name, session_id, summary)

        return {"success": True, "summary": summary}
    except Exception as e:
        logger.error(f"Error updating session summary: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)

@app.get("/api/projects/{project_name}/sessions/{session_id}/messages")
async def get_session_messages(project_name: str, session_id: str, limit: int = None, offset: int = 0):
    """è·å– session çš„æ¶ˆæ¯åˆ—è¡¨"""
    messages = project_manager.get_messages(project_name, session_id)

    # å¦‚æœæŒ‡å®šäº† limitï¼Œåˆ™åˆ†é¡µ
    if limit is not None:
        messages = messages[offset:offset + limit]
    elif offset > 0:
        messages = messages[offset:]

    return {
        "messages": messages,
        "total": len(messages),
        "hasMore": limit is not None and len(messages) >= limit
    }

@app.get("/api/projects/{project_name}/sessions/{session_id}/token-usage")
async def get_token_usage(project_name: str, session_id: str):
    """è·å– session çš„ token ä½¿ç”¨æƒ…å†µï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    try:
        messages = project_manager.get_messages(project_name, session_id)

        # ç®€å•ä¼°ç®—ï¼šå‡è®¾æ¯æ¡æ¶ˆæ¯å¤§çº¦ä½¿ç”¨ä¸€å®šæ•°é‡çš„ token
        # å®é™…åº”ç”¨ä¸­åº”è¯¥ä» AI å“åº”ä¸­è·å–å‡†ç¡®çš„ token è®¡æ•°
        total_messages = len(messages)
        estimated_tokens = total_messages * 100  # ç²—ç•¥ä¼°ç®—

        return {
            "totalMessages": total_messages,
            "estimatedTokens": estimated_tokens,
            "session_id": session_id
        }
    except Exception as e:
        logger.error(f"Error getting token usage: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)

# --- é¡¹ç›®åˆ›å»ºå·¥ä½œæµ API ---

@app.get("/api/validate-path")
async def validate_path(path: str = Query(...)):
    """éªŒè¯è·¯å¾„çŠ¶æ€ï¼ˆç”¨äºå‰ç«¯å®æ—¶åé¦ˆï¼‰"""
    try:
        path = os.path.expanduser(path.strip())
        path = os.path.abspath(path)
        
        exists = os.path.exists(path)
        is_dir = os.path.isdir(path) if exists else False
        is_empty = False
        is_git = False
        
        if is_dir:
            try:
                is_empty = not any(os.scandir(path))
                is_git = os.path.isdir(os.path.join(path, ".git"))
            except PermissionError:
                pass
                
        parent_exists = os.path.exists(os.path.dirname(path))
        
        return {
            "exists": exists,
            "isDirectory": is_dir,
            "isEmpty": is_empty,
            "isGit": is_git,
            "parentExists": parent_exists,
            "path": path
        }
    except Exception as e:
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/api/create-workspace")
async def create_workspace(req: CreateWorkspaceRequest):
    """åˆ›å»ºæˆ–æ·»åŠ å·¥ä½œç©ºé—´"""
    logger.info(f"=== åˆ›å»ºå·¥ä½œç©ºé—´è¯·æ±‚ ===")
    logger.info(f"  ç±»å‹: {req.workspaceType}")
    logger.info(f"  è·¯å¾„: {req.path}")
    logger.info(f"  GitHub URL: {req.githubUrl}")
    
    try:
        # è§„èŒƒåŒ–è·¯å¾„
        workspace_path = os.path.expanduser(req.path.strip())
        workspace_path = os.path.abspath(workspace_path)

        # éªŒè¯è·¯å¾„å®‰å…¨æ€§
        is_valid, error, normalized_path = PathValidator.validate_project_path(
            workspace_path,
            must_exist=(req.workspaceType == 'existing')
        )

        # å¦‚æœéªŒè¯å¤±è´¥æ˜¯å› ä¸ºä¸åœ¨å…è®¸çš„æ ¹ç›®å½•èŒƒå›´å†…ï¼ŒåŠ¨æ€æ·»åŠ è¯¥è·¯å¾„
        if not is_valid and "ä¸åœ¨å…è®¸çš„æ ¹ç›®å½•èŒƒå›´å†…" in error:
            logger.info(f"è·¯å¾„ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ŒåŠ¨æ€æ·»åŠ : {workspace_path}")
            # æ·»åŠ è¯¥è·¯å¾„åŠå…¶çˆ¶ç›®å½•åˆ°å…è®¸åˆ—è¡¨
            PathValidator.add_allowed_root(workspace_path)
            PathValidator.add_allowed_root(os.path.dirname(workspace_path))

            # é‡æ–°éªŒè¯
            is_valid, error, normalized_path = PathValidator.validate_project_path(
                workspace_path,
                must_exist=(req.workspaceType == 'existing')
            )

        if req.workspaceType == 'existing':
            # å·²æœ‰å·¥ä½œç©ºé—´ - éœ€è¦è·¯å¾„å­˜åœ¨
            if not is_valid:
                logger.error(f"è·¯å¾„éªŒè¯å¤±è´¥: {error}")
                return JSONResponse(
                    content={"error": f"æ— æ•ˆçš„å·¥ä½œç©ºé—´è·¯å¾„: {error}"},
                    status_code=400
                )

            if not os.path.isdir(normalized_path):
                return JSONResponse(
                    content={"error": "æŒ‡å®šçš„è·¯å¾„ä¸æ˜¯ä¸€ä¸ªç›®å½•"},
                    status_code=400
                )
        else:
            # æ–°å»ºå·¥ä½œç©ºé—´
            parent_dir = os.path.dirname(workspace_path)
            
            # æ£€æŸ¥çˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨
            if not os.path.exists(parent_dir):
                try:
                    os.makedirs(parent_dir, exist_ok=True)
                    logger.info(f"åˆ›å»ºçˆ¶ç›®å½•: {parent_dir}")
                except Exception as e:
                    return JSONResponse(
                        content={"error": f"æ— æ³•åˆ›å»ºçˆ¶ç›®å½•: {e}"},
                        status_code=400
                    )
            
            if req.githubUrl:
                # ä» GitHub å…‹éš†
                logger.info(f"ä» GitHub å…‹éš†: {req.githubUrl}")
                
                # æ„å»º git clone å‘½ä»¤
                clone_url = req.githubUrl.strip()
                
                # å¦‚æœæä¾›äº† tokenï¼Œä¿®æ”¹ URL ä»¥åŒ…å«è®¤è¯ä¿¡æ¯
                if req.newGithubToken:
                    # è§£æ GitHub URL å¹¶æ³¨å…¥ token
                    if clone_url.startswith("https://github.com/"):
                        clone_url = clone_url.replace(
                            "https://github.com/",
                            f"https://{req.newGithubToken}@github.com/"
                        )
                    elif clone_url.startswith("https://"):
                        # é€šç”¨ HTTPS URL
                        clone_url = clone_url.replace(
                            "https://",
                            f"https://{req.newGithubToken}@"
                        )
                
                try:
                    # æ‰§è¡Œ git clone
                    result = subprocess.run(
                        ["git", "clone", clone_url, workspace_path],
                        capture_output=True,
                        text=True,
                        timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
                        cwd=parent_dir
                    )
                    
                    if result.returncode != 0:
                        error_msg = result.stderr or result.stdout or "å…‹éš†å¤±è´¥"
                        # æ¸…ç†é”™è¯¯ä¿¡æ¯ä¸­å¯èƒ½åŒ…å«çš„ token
                        if req.newGithubToken:
                            error_msg = error_msg.replace(req.newGithubToken, "***")
                        logger.error(f"Git clone å¤±è´¥: {error_msg}")
                        return JSONResponse(
                            content={"error": f"Git clone å¤±è´¥: {error_msg}"},
                            status_code=400
                        )
                    
                    logger.info(f"æˆåŠŸå…‹éš†ä»“åº“åˆ°: {workspace_path}")
                    
                except subprocess.TimeoutExpired:
                    return JSONResponse(
                        content={"error": "å…‹éš†è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä»“åº“å¤§å°"},
                        status_code=408
                    )
                except FileNotFoundError:
                    return JSONResponse(
                        content={"error": "Git æœªå®‰è£…æˆ–ä¸åœ¨ç³»ç»Ÿ PATH ä¸­"},
                        status_code=500
                    )
                except Exception as e:
                    logger.exception(f"Git clone å¼‚å¸¸: {e}")
                    return JSONResponse(
                        content={"error": f"å…‹éš†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"},
                        status_code=500
                    )
            else:
                # åˆ›å»ºç©ºç›®å½•
                if not os.path.exists(workspace_path):
                    os.makedirs(workspace_path, exist_ok=True)
                    logger.info(f"åˆ›å»ºç©ºå·¥ä½œç©ºé—´: {workspace_path}")
                elif os.listdir(workspace_path):
                    return JSONResponse(
                        content={"error": "ç›®å½•å·²å­˜åœ¨ä¸”ä¸ä¸ºç©º"},
                        status_code=400
                    )
            
            normalized_path = workspace_path
        
        # å°†é¡¹ç›®æ·»åŠ åˆ°é¡¹ç›®ç®¡ç†å™¨
        project = project_manager.add_project(normalized_path)
        
        # æ³¨å†Œåˆ°è·¯å¾„éªŒè¯å™¨
        project_registry.register_project(project["name"], normalized_path)
        
        logger.info(f"å·¥ä½œç©ºé—´åˆ›å»ºæˆåŠŸ: {project}")
        
        return {
            "success": True,
            "project": project,
            "message": f"{'å·²æ·»åŠ ' if req.workspaceType == 'existing' else 'å·²åˆ›å»º'}å·¥ä½œç©ºé—´: {project['displayName']}"
        }
        
    except Exception as e:
        logger.exception(f"åˆ›å»ºå·¥ä½œç©ºé—´å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"åˆ›å»ºå·¥ä½œç©ºé—´å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/browse-filesystem")
async def browse_filesystem(path: str = Query(None), include_files: bool = Query(False), limit: int = Query(100)):
    """æµè§ˆæ–‡ä»¶ç³»ç»Ÿï¼Œæä¾›è·¯å¾„è‡ªåŠ¨è¡¥å…¨å»ºè®®"""
    try:
        suggestions = []
        
        # å¤„ç†è™šæ‹Ÿæ ¹è·¯å¾„è¯·æ±‚
        if path == "__ROOT__":
            base_dirs = []
            
            # 1. å§‹ç»ˆæ·»åŠ ç”¨æˆ·ä¸»ç›®å½•
            home_dir = os.path.expanduser("~")
            base_dirs.append({
                "name": "Home ğŸ ", 
                "path": home_dir, 
                "type": "directory"
            })
            
            # 2. æ ¹æ®ç³»ç»Ÿæ·»åŠ æ ¹èŠ‚ç‚¹
            if platform.system() == "Windows":
                # Windows: æ·»åŠ æ‰€æœ‰é€»è¾‘é©±åŠ¨å™¨
                import string
                try:
                    # å°è¯•ä½¿ç”¨ ctypes è·å–é©±åŠ¨å™¨ï¼ˆæ›´å‡†ç¡®ï¼‰
                    from ctypes import windll
                    drives = []
                    bitmask = windll.kernel32.GetLogicalDrives()
                    for letter in string.ascii_uppercase:
                        if bitmask & 1:
                            drives.append(f"{letter}:\\")
                        bitmask >>= 1
                except:
                    # å›é€€åˆ°ç®€å•çš„å­˜åœ¨æ€§æ£€æŸ¥
                    drives = [f"{d}:\\" for d in string.ascii_uppercase if os.path.exists(f"{d}:\\")]
                
                for drive in drives:
                    base_dirs.append({
                        "name": f"Local Disk ({drive})",
                        "path": drive,
                        "type": "directory"
                    })
            else:
                # Unix/Mac: æ·»åŠ ç³»ç»Ÿæ ¹ç›®å½•
                base_dirs.append({
                    "name": "System Root (/)",
                    "path": "/",
                    "type": "directory"
                })
                
                # Mac ç‰¹æœ‰: /Volumes
                if platform.system() == "Darwin" and os.path.exists("/Volumes"):
                     base_dirs.append({
                        "name": "Volumes",
                        "path": "/Volumes",
                        "type": "directory"
                    })

            return {"suggestions": base_dirs, "currentPath": "__ROOT__"}

        # å¦‚æœæ²¡æœ‰æä¾›è·¯å¾„ï¼Œé»˜è®¤è¿”å›ç”¨æˆ·ä¸»ç›®å½•ä¿¡æ¯ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        if not path or path == "~":
            home_dir = os.path.expanduser("~")
            return {"suggestions": [], "currentPath": home_dir} # ç®€åŒ–ï¼Œä¸å†åœ¨æ­¤å¤„è¿”å›é©±åŠ¨å™¨åˆ—è¡¨ï¼Œç”± __ROOT__ æ¥ç®¡
        
        # å±•å¼€ ~ ç¬¦å·
        expanded_path = os.path.expanduser(path)
        
        # ç¡®å®šè¦æµè§ˆçš„ç›®å½•
        if os.path.isdir(expanded_path):
            browse_dir = expanded_path
            prefix = ""
        else:
            browse_dir = os.path.dirname(expanded_path)
            prefix = os.path.basename(expanded_path).lower()
        
        if not os.path.isdir(browse_dir):
            return {"suggestions": [], "currentPath": path, "error": "ç›®å½•ä¸å­˜åœ¨"}
        
        # åˆ—å‡ºç›®å½•å†…å®¹
        try:
            entries = os.listdir(browse_dir)
        except PermissionError:
            return {"suggestions": [], "currentPath": path, "error": "æƒé™ä¸è¶³"}
        
        for entry in entries:
            # è·³è¿‡éšè—æ–‡ä»¶ï¼ˆé™¤éç”¨æˆ·æ˜ç¡®è¾“å…¥äº†ç‚¹å·å¼€å¤´ï¼‰
            if entry.startswith('.') and not prefix.startswith('.'):
                continue
            
            # å‰ç¼€è¿‡æ»¤
            if prefix and not entry.lower().startswith(prefix):
                continue
            
            full_path = os.path.join(browse_dir, entry)
            is_dir = os.path.isdir(full_path)
            
            # Filter based on include_files
            if not include_files and not is_dir:
                continue
            
            suggestions.append({
                "name": entry,
                "path": full_path,
                "type": "directory" if is_dir else "file"
            })
        
        # Sort: directories first, then alphabetical
        suggestions.sort(key=lambda x: (0 if x["type"] == "directory" else 1, x["name"].lower()))
        
        # Limit results
        if limit > 0:
            suggestions = suggestions[:limit]
        
        return {
            "suggestions": suggestions,
            "currentPath": expanded_path
        }
        
    except Exception as e:
        logger.error(f"æµè§ˆæ–‡ä»¶ç³»ç»Ÿå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æµè§ˆæ–‡ä»¶ç³»ç»Ÿå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/search-filesystem")
async def search_filesystem(q: str = Query(...), path: str = Query(None), limit: int = Query(50)):
    """Search for directories matching query"""
    start_dir = os.path.expanduser(path or "~")
    
    def _search():
        results = []
        if not os.path.exists(start_dir):
            return results
        
        try:
            count = 0
            # Limit depth effectively by not following hidden/large dirs
            exclude_dirs = {'node_modules', 'Library', 'venv', '__pycache__', '.git', '.idea', '.vscode'}
            
            for root, dirs, files in os.walk(start_dir):
                # Filter in-place to prevent recursion
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in exclude_dirs]
                
                for d in dirs:
                    if q.lower() in d.lower():
                        full_path = os.path.join(root, d)
                        results.append({
                            "name": d,
                            "path": full_path,
                            "type": "directory"
                        })
                        count += 1
                        if count >= limit:
                            return results
                
                if len(results) >= limit:
                    break
        except Exception as e:
            logger.error(f"Search error: {e}")
            pass
        return results

    results = await asyncio.to_thread(_search)
    return {"results": results}


@app.post("/api/projects/create")
async def create_project(req: CreateProjectRequest):
    """åˆ›å»ºé¡¹ç›®ï¼ˆç®€å•ç‰ˆæœ¬ - ä»…æ·»åŠ ç°æœ‰è·¯å¾„ï¼‰"""
    try:
        workspace_path = os.path.expanduser(req.path.strip())
        workspace_path = os.path.abspath(workspace_path)

        if not os.path.isdir(workspace_path):
            return JSONResponse(
                content={"error": "æŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•"},
                status_code=400
            )

        # å°è¯•æ³¨å†Œé¡¹ç›®ï¼Œå¦‚æœå¤±è´¥åˆ™åŠ¨æ€æ·»åŠ è·¯å¾„
        project = project_manager.add_project(workspace_path)

        # æ³¨å†Œåˆ°é¡¹ç›®æ³¨å†Œè¡¨
        is_registered, error = project_registry.register_project(project["name"], workspace_path)
        if not is_registered:
            # å¦‚æœæ³¨å†Œå¤±è´¥æ˜¯å› ä¸ºè·¯å¾„ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ŒåŠ¨æ€æ·»åŠ 
            if "ä¸åœ¨å…è®¸çš„æ ¹ç›®å½•èŒƒå›´å†…" in error:
                logger.info(f"è·¯å¾„ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ŒåŠ¨æ€æ·»åŠ : {workspace_path}")
                PathValidator.add_allowed_root(workspace_path)
                PathValidator.add_allowed_root(os.path.dirname(workspace_path))

                # é‡æ–°æ³¨å†Œ
                is_registered, error = project_registry.register_project(project["name"], workspace_path)

            if not is_registered:
                logger.error(f"æ³¨å†Œé¡¹ç›®å¤±è´¥: {error}")
                return JSONResponse(
                    content={"error": f"æ³¨å†Œé¡¹ç›®å¤±è´¥: {error}"},
                    status_code=400
                )

        return {"success": True, "project": project}

    except Exception as e:
        logger.exception(f"åˆ›å»ºé¡¹ç›®å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"åˆ›å»ºé¡¹ç›®å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/projects/create-workspace")
async def create_workspace(req: CreateWorkspaceRequest):
    """åˆ›å»ºå·¥ä½œç©ºé—´ï¼ˆå®Œæ•´æµç¨‹ï¼‰"""
    try:
        workspace_path = os.path.expanduser(req.path.strip())
        workspace_path = os.path.abspath(workspace_path)
        
        # 1. éªŒè¯æˆ–åˆ›å»ºç›®å½•
        if req.workspaceType == 'new':
            if os.path.exists(workspace_path):
                if not os.path.isdir(workspace_path):
                    return JSONResponse(content={"error": "è·¯å¾„å­˜åœ¨ä¸”ä¸æ˜¯ç›®å½•"}, status_code=400)
                # å…è®¸åœ¨ç©ºç›®å½•ä¸­åˆ›å»ºï¼ˆæˆ–éç©ºç›®å½•ä½†ç”¨æˆ·å·²ç¡®è®¤ï¼‰
            else:
                try:
                    os.makedirs(workspace_path, exist_ok=True)
                except Exception as e:
                    return JSONResponse(content={"error": f"æ— æ³•åˆ›å»ºç›®å½•: {str(e)}"}, status_code=500)
                
            # 2. å¤„ç† GitHub å…‹éš†
            if req.githubUrl:
                repo_url = req.githubUrl
                if req.newGithubToken:
                    # æ’å…¥ token: https://TOKEN@github.com/...
                    if repo_url.startswith("https://"):
                        repo_url = repo_url.replace("https://", f"https://{req.newGithubToken}@")
                
                try:
                    # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©º
                    if os.path.exists(workspace_path) and any(os.scandir(workspace_path)):
                         return JSONResponse(content={"error": "ç›®æ ‡ç›®å½•éç©ºï¼Œæ— æ³•å…‹éš†ä»“åº“"}, status_code=400)

                    process = await asyncio.create_subprocess_exec(
                        "git", "clone", repo_url, ".",
                        cwd=workspace_path,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    stdout, stderr = await process.communicate()
                    if process.returncode != 0:
                        err_msg = stderr.decode()
                        # ç®€å•éšè— token
                        if req.newGithubToken:
                            err_msg = err_msg.replace(req.newGithubToken, "***")
                        return JSONResponse(content={"error": f"å…‹éš†å¤±è´¥: {err_msg}"}, status_code=400)
                except Exception as e:
                    return JSONResponse(content={"error": f"Gitæ“ä½œå¤±è´¥: {str(e)}"}, status_code=500)

        elif req.workspaceType == 'existing':
            if not os.path.isdir(workspace_path):
                return JSONResponse(content={"error": "è·¯å¾„ä¸å­˜åœ¨"}, status_code=400)

        # 3. æ³¨å†Œé¡¹ç›®
        # ä½¿ç”¨ project_manager æ·»åŠ 
        project = project_manager.add_project(workspace_path)
        
        # æ³¨å†Œåˆ° registry (ä¸ºäº†å…è®¸è®¿é—®)
        is_registered, error = project_registry.register_project(project["name"], workspace_path)
        
        if not is_registered:
             if "ä¸åœ¨å…è®¸çš„æ ¹ç›®å½•èŒƒå›´å†…" in error:
                logger.info(f"è·¯å¾„ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ŒåŠ¨æ€æ·»åŠ : {workspace_path}")
                PathValidator.add_allowed_root(workspace_path)
                PathValidator.add_allowed_root(os.path.dirname(workspace_path))
                is_registered, error = project_registry.register_project(project["name"], workspace_path)
        
        if not is_registered:
             return JSONResponse(content={"error": f"æ³¨å†Œé¡¹ç›®å¤±è´¥: {error}"}, status_code=400)

        return {"success": True, "project": project}

    except Exception as e:
        logger.exception(f"åˆ›å»ºå·¥ä½œç©ºé—´å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/api/error-analyze")
async def analyze_error(request: Request):
    """åˆ†æé”™è¯¯å¹¶æä¾›ä¿®å¤å»ºè®®"""
    try:
        data = await request.json()
        error_output = data.get('error', '')
        project_path = data.get('projectPath', '')

        if not error_output:
            return JSONResponse(
                content={"error": "é”™è¯¯è¾“å‡ºä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # è·å–é”™è¯¯åˆ†æå™¨
        analyzer = get_error_analyzer(project_path) if project_path else ErrorAnalyzer('.')

        # åˆ†æé”™è¯¯
        analysis = analyzer.analyze_error(error_output, project_path)

        # è·å–ä»£ç ä¸Šä¸‹æ–‡
        if analysis['error_info']['file'] and analysis['error_info']['line']:
            context = analyzer.get_error_context(
                analysis['error_info']['file'],
                analysis['error_info']['line']
            )
            analysis['code_context'] = context

        # ç”Ÿæˆè‡ªåŠ¨ä¿®å¤æ–¹æ¡ˆ
        if analysis['can_auto_fix']:
            auto_fix = analyzer.generate_auto_fix(error_output, project_path)
            analysis['auto_fix'] = auto_fix

        return {
            "success": True,
            "analysis": analysis
        }

    except Exception as e:
        logger.exception(f"é”™è¯¯åˆ†æå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"é”™è¯¯åˆ†æå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/auto-fix")
async def auto_fix_error(request: Request):
    """è‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤é”™è¯¯"""
    try:
        data = await request.json()
        error_output = data.get('error', '')
        project_path = data.get('projectPath', '')
        context = data.get('context', {})

        if not error_output:
            return JSONResponse(
                content={"error": "é”™è¯¯è¾“å‡ºä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        logger.info(f"å¼€å§‹è‡ªåŠ¨ä¿®å¤: {error_output[:100]}...")

        # è·å– Agent å®ä¾‹
        agent = get_agent(
            project_path,
            global_config["mode"],
            global_config.get("model"),
            global_config.get("mcp_servers")
        )

        # è·å–è‡ªåŠ¨ä¿®å¤å™¨
        auto_fixer = get_auto_fixer(project_path, agent)

        # æ‰§è¡Œè‡ªåŠ¨ä¿®å¤
        result = await auto_fixer.detect_and_fix(error_output, context)

        logger.info(f"è‡ªåŠ¨ä¿®å¤ç»“æœ: {result}")

        return {
            "success": True,
            "result": result
        }

    except Exception as e:
        logger.exception(f"è‡ªåŠ¨ä¿®å¤å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è‡ªåŠ¨ä¿®å¤å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/auto-fix/history")
async def get_auto_fix_history(projectPath: str = Query(..., description="é¡¹ç›®è·¯å¾„")):
    """è·å–è‡ªåŠ¨ä¿®å¤å†å²"""
    try:
        auto_fixer = get_auto_fixer(projectPath)
        history = auto_fixer.get_fix_history()

        return {
            "success": True,
            "history": history
        }

    except Exception as e:
        logger.exception(f"è·å–ä¿®å¤å†å²å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è·å–ä¿®å¤å†å²å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.delete("/api/auto-fix/history")
async def clear_auto_fix_history(projectPath: str = Query(..., description="é¡¹ç›®è·¯å¾„")):
    """æ¸…ç©ºè‡ªåŠ¨ä¿®å¤å†å²"""
    try:
        auto_fixer = get_auto_fixer(projectPath)
        auto_fixer.clear_history()

        return {
            "success": True,
            "message": "ä¿®å¤å†å²å·²æ¸…ç©º"
        }

    except Exception as e:
        logger.exception(f"æ¸…ç©ºä¿®å¤å†å²å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ¸…ç©ºä¿®å¤å†å²å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/context/analyze")
async def analyze_context(request: Request):
    """åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡ï¼ˆä¾èµ–å…³ç³»ã€è°ƒç”¨å…³ç³»ã€ç±»ç»§æ‰¿ï¼‰"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')
        include_dirs = data.get('includeDirs', [])

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        logger.info(f"å¼€å§‹åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡: {project_path}")

        # è·å–ä¾èµ–åˆ†æå™¨
        analyzer = get_dependency_analyzer(project_path)

        # åˆ†æé¡¹ç›®
        result = analyzer.analyze_project(include_dirs)

        logger.info(f"é¡¹ç›®ä¸Šä¸‹æ–‡åˆ†æå®Œæˆ: {len(result['modules'])} ä¸ªæ¨¡å—")

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        logger.exception(f"åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/context/module/{module_name}")
async def get_module_context(module_name: str, projectPath: str = Query(..., description="é¡¹ç›®è·¯å¾„")):
    """è·å–ç‰¹å®šæ¨¡å—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    try:
        analyzer = get_dependency_analyzer(projectPath)

        if module_name not in analyzer.modules:
            return JSONResponse(
                content={"error": f"æ¨¡å— {module_name} ä¸å­˜åœ¨"},
                status_code=404
            )

        module = analyzer.modules[module_name]

        # è·å–ä¾èµ–çš„æ¨¡å—
        dependencies = set(module.imports)
        for from_module in module.from_imports.keys():
            dependencies.add(from_module)

        # è·å–è¢«ä¾èµ–çš„æ¨¡å—
        dependents = []
        for other_module_name, other_module in analyzer.modules.items():
            if module_name in other_module.imports or module_name in other_module.from_imports:
                dependents.append(other_module_name)

        return {
            "success": True,
            "module": {
                "name": module_name,
                "file_path": module.file_path,
                "imports": list(module.imports),
                "from_imports": {k: list(v) for k, v in module.from_imports.items()},
                "functions": {
                    func_name: {
                        "line": func_info.line_start,
                        "parameters": func_info.parameters,
                        "calls": list(func_info.calls),
                        "is_async": func_info.is_async,
                        "is_method": func_info.is_method,
                        "class_name": func_info.class_name
                    }
                    for func_name, func_info in module.functions.items()
                },
                "classes": {
                    class_name: {
                        "line": class_info.line_start,
                        "bases": class_info.bases,
                        "methods": list(class_info.methods.keys()),
                        "attributes": list(class_info.attributes)
                    }
                    for class_name, class_info in module.classes.items()
                }
            },
            "dependencies": list(dependencies),
            "dependents": dependents
        }

    except Exception as e:
        logger.exception(f"è·å–æ¨¡å—ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è·å–æ¨¡å—ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/code-style-analyze")
async def analyze_code_style(request: Request):
    """åˆ†æé¡¹ç›®ä»£ç é£æ ¼"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # è·å–ä»£ç é£æ ¼åˆ†æå™¨
        analyzer = get_code_style_analyzer(project_path)

        # åˆ†æä»£ç é£æ ¼
        style_profile = analyzer.analyze_project_style()
        style_summary = analyzer.get_style_summary()

        return {
            "success": True,
            "styleProfile": style_profile,
            "summary": style_summary
        }

    except Exception as e:
        logger.exception(f"ä»£ç é£æ ¼åˆ†æå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"ä»£ç é£æ ¼åˆ†æå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/prompt-optimize")
async def optimize_prompt(request: Request):
    """æ ¹æ®é¡¹ç›®ç‰¹å¾æ™ºèƒ½ä¼˜åŒ–ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯ï¼ˆä½¿ç”¨å¤§æ¨¡å‹ï¼‰"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')
        user_input = data.get('userInput', '')
        base_persona = data.get('persona', 'partner')

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        if not user_input:
            return JSONResponse(
                content={"error": "ç”¨æˆ·è¾“å…¥ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        logger.info(f"å¼€å§‹æ™ºèƒ½ä¼˜åŒ–æ¶ˆæ¯: project={project_path}, persona={base_persona}, input={user_input[:100]}...")

        # è·å–æç¤ºè¯ä¼˜åŒ–å™¨
        optimizer = get_prompt_optimizer(project_path)

        # å…ˆåˆ†æé¡¹ç›®ï¼ˆè¿™ä¼šæ‰«æé¡¹ç›®ä»£ç ï¼‰
        analysis = optimizer.analyze_project()

        # åˆ†æç”¨æˆ·æ„å›¾
        intent = optimizer.analyze_user_intent(user_input)

        # æŸ¥æ‰¾ç›¸å…³ä»£ç 
        relevant_code = optimizer.find_relevant_code(user_input, intent)

        # æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡
        project_context = optimizer._build_project_context()
        style_guide = optimizer._build_style_guide()

        # æ„å»ºä¼˜åŒ–æç¤ºè¯
        optimization_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æç¤ºè¯ä¼˜åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œä¼˜åŒ–ç”¨æˆ·çš„è¾“å…¥æ¶ˆæ¯ï¼Œä½¿å…¶æ›´å…·ä½“ã€æ›´ç¬¦åˆé¡¹ç›®çš„å®é™…æƒ…å†µã€‚

## é¡¹ç›®ä¿¡æ¯
{project_context}

## ä»£ç é£æ ¼æŒ‡å—
{style_guide}

## ç”¨æˆ·æ„å›¾
- æ„å›¾ç±»å‹: {intent.get('type', 'unknown')}
- å…³é”®è¯: {', '.join(intent.get('keywords', []))}
- å®ä½“: {', '.join(intent.get('entities', []))}

## ç›¸å…³ä»£ç 
"""
        if relevant_code:
            for code in relevant_code:
                if code['type'] == 'function':
                    optimization_prompt += f"- å‡½æ•°: {code['name']} (åœ¨ {code['file']})"
                else:
                    optimization_prompt += f"- ç±»: {code['name']} (åœ¨ {code['file']})"
        else:
            optimization_prompt += "- æ— ç›¸å…³ä»£ç "

        optimization_prompt += f"""

## ç”¨æˆ·åŸå§‹è¾“å…¥
{user_input}

## ä»»åŠ¡
è¯·ä¼˜åŒ–ç”¨æˆ·çš„è¾“å…¥æ¶ˆæ¯ï¼Œä½¿å…¶ï¼š
1. åŒ…å«é¡¹ç›®èƒŒæ™¯ä¿¡æ¯
2. å¼•ç”¨ç›¸å…³çš„ä»£ç ï¼ˆå¦‚æœæœ‰ï¼‰
3. æ˜ç¡®ä»£ç é£æ ¼è¦æ±‚
4. æ ¹æ®æ„å›¾ç±»å‹æ·»åŠ å…·ä½“è¦æ±‚
5. è®© AI èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£é¡¹ç›®ä¸Šä¸‹æ–‡å¹¶æä¾›å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆ

è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„æ¶ˆæ¯ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—ã€‚"""

        logger.info("è°ƒç”¨å¤§æ¨¡å‹ä¼˜åŒ–æ¶ˆæ¯...")

        # åˆ›å»º iFlow å®¢æˆ·ç«¯
        from backend.core.iflow_client import create_iflow_client
        iflow_client = create_iflow_client(
            cwd=project_path,
            mode=global_config.get("mode", "yolo"),
            model=global_config.get("model", "GLM-4.7")
        )

        # è°ƒç”¨å¤§æ¨¡å‹
        optimized_message = ""
        async for chunk in iflow_client.chat_stream(optimization_prompt):
            optimized_message += chunk

        optimized_message = optimized_message.strip()

        logger.info(f"å¤§æ¨¡å‹ä¼˜åŒ–å®Œæˆï¼Œæ¶ˆæ¯é•¿åº¦: {len(optimized_message)}")

        return {
            "success": True,
            "analysis": analysis,
            "intent": intent,
            "relevantCode": relevant_code,
            "originalInput": user_input,
            "optimizedMessage": optimized_message,
            "projectContext": project_context,
            "codeStyleGuide": style_guide
        }

    except Exception as e:
        logger.exception(f"æ™ºèƒ½æ¶ˆæ¯ä¼˜åŒ–å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ™ºèƒ½æ¶ˆæ¯ä¼˜åŒ–å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/generate-report")
async def generate_report(req: dict):
    """ç”Ÿæˆå·¥ä½œæŠ¥å‘Š"""
    try:
        project_path = req.get("projectPath")
        report_type = req.get("type", "daily")
        
        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )
        
        expanded_path = os.path.expanduser(project_path)
        
        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(expanded_path)
        if not is_valid:
            return JSONResponse(
                content={"error": f"æ— æ•ˆçš„é¡¹ç›®è·¯å¾„: {error}"},
                status_code=400
            )
        
        analyzer = get_report_generator()
        report = analyzer.generate_report(normalized, report_type)
        
        return {"success": True, "report": report}
        
    except Exception as e:
        logger.exception(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/context-analyze")
async def analyze_context(req: dict):
    """åˆ†æä»£ç ä¸Šä¸‹æ–‡å’Œä¾èµ–å…³ç³»"""
    try:
        project_path = req.get("projectPath")
        node_id = req.get("nodeId")
        max_depth = req.get("maxDepth", 2)
        
        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )
        
        expanded_path = os.path.expanduser(project_path)
        
        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(expanded_path)
        if not is_valid:
            return JSONResponse(
                content={"error": f"æ— æ•ˆçš„é¡¹ç›®è·¯å¾„: {error}"},
                status_code=400
            )
        
        analyzer = get_dependency_analyzer()
        
        # åˆ†æé¡¹ç›®
        analyzer.analyze_project(normalized)
        
        # å¦‚æœæŒ‡å®šäº†èŠ‚ç‚¹ IDï¼Œè·å–ä¸Šä¸‹æ–‡å›¾è°±
        if node_id:
            context_graph = analyzer.get_context_graph(node_id, max_depth)
            return {"success": True, "graph": context_graph}
        
        # å¦åˆ™è¿”å›æ‰€æœ‰èŠ‚ç‚¹åˆ—è¡¨
        nodes = []
        for node_id, node in analyzer.nodes.items():
            nodes.append({
                'id': node_id,
                'name': node.name,
                'type': node.type.value,
                'file': os.path.basename(node.file_path),
                'line': node.line_number,
                'full_path': node.file_path
            })
        
        return {"success": True, "nodes": nodes[:100]}  # é™åˆ¶è¿”å›æ•°é‡
        
    except Exception as e:
        logger.exception(f"åˆ†æä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"åˆ†æä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/context-search")
async def search_context(req: dict):
    """æœç´¢ä»£ç èŠ‚ç‚¹"""
    try:
        project_path = req.get("projectPath")
        query = req.get("query")
        limit = req.get("limit", 20)
        
        if not project_path or not query:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„å’ŒæŸ¥è¯¢è¯ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )
        
        expanded_path = os.path.expanduser(project_path)
        
        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(expanded_path)
        if not is_valid:
            return JSONResponse(
                content={"error": f"æ— æ•ˆçš„é¡¹ç›®è·¯å¾„: {error}"},
                status_code=400
            )
        
        analyzer = get_dependency_analyzer()
        analyzer.analyze_project(normalized)
        
        results = analyzer.search_nodes(query, limit)
        
        return {"success": True, "results": results}

    except Exception as e:
        logger.exception(f"æœç´¢ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æœç´¢ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/query")
async def simple_query(req: dict):
    """ç®€å•çš„åŒæ­¥æŸ¥è¯¢ API - å¿«é€Ÿè·å– AI å“åº”"""
    try:
        from backend.core.iflow_client import query_sync

        prompt = req.get("prompt")
        project = req.get("project")
        model = req.get("model")
        system_prompt = req.get("system_prompt")
        timeout = req.get("timeout", 300.0)

        if not prompt:
            return JSONResponse(
                content={"error": "prompt ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # è·å–é¡¹ç›®è·¯å¾„
        cwd = None
        if project:
            cwd = get_project_path(project)

        # æ‰§è¡ŒæŸ¥è¯¢
        response = query_sync(
            prompt=prompt,
            cwd=cwd,
            model=model,
            system_prompt=system_prompt,
            timeout=timeout
        )

        return {"success": True, "response": response}

    except Exception as e:
        logger.exception(f"ç®€å•æŸ¥è¯¢å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/mcp/config/read")
async def get_mcp_config():
    """è¯»å– MCP é…ç½®"""
    try:
        iflow_config_path = os.path.expanduser("~/.iflow/settings.json")
        
        if not os.path.exists(iflow_config_path):
            return {"success": False, "error": "iFlow é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(iflow_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        mcp_servers = config.get("mcpServers", {})
        
        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        servers = []
        for name, server_config in mcp_servers.items():
            servers.append({
                "id": name,
                "name": name,
                "type": server_config.get("type", "stdio"),
                "scope": "user",
                "config": server_config,
                "created": datetime.now().isoformat(),
                "updated": datetime.now().isoformat()
            })
        
        return {"success": True, "servers": servers}
    except Exception as e:
        logger.exception(f"è¯»å– MCP é…ç½®å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/mcp/cli/list")
async def list_mcp_cli():
    """é€šè¿‡ CLI åˆ—å‡º MCP æœåŠ¡å™¨"""
    try:
        # å°è¯•é€šè¿‡ iflow mcp list å‘½ä»¤è·å–
        result = subprocess.run(
            ["iflow", "mcp", "list"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            # è§£æ CLI è¾“å‡º
            servers = []
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¾“å‡ºæ ¼å¼è§£æ
            # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨
            return {"success": True, "servers": servers}
        else:
            return {"success": False, "error": result.stderr}
    except Exception as e:
        logger.warning(f"é€šè¿‡ CLI åˆ—å‡º MCP æœåŠ¡å™¨å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/mcp/servers")
async def get_mcp_servers(scope: str = "user"):
    """è·å– MCP æœåŠ¡å™¨åˆ—è¡¨"""
    try:
        # ä» global_config è·å–
        servers = global_config.get("mcp_servers", [])
        return {"success": True, "servers": servers}
    except Exception as e:
        logger.exception(f"è·å– MCP æœåŠ¡å™¨å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@app.post("/api/context/analyze-dependencies")
async def analyze_code_dependencies(request: Request):
    """åˆ†æä»£ç ä¾èµ–å…³ç³»å¹¶ç”Ÿæˆå¯è§†åŒ–æ•°æ®"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(project_path)
        if not is_valid:
            return JSONResponse(
                content={"error": error},
                status_code=400
            )

        # è·å–ä¾èµ–åˆ†æå™¨
        analyzer = get_code_dependency_analyzer(normalized)

        # åˆ†æä¾èµ–å…³ç³»
        result = analyzer.analyze_project_dependencies()

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        logger.exception(f"ä»£ç ä¾èµ–åˆ†æå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"ä»£ç ä¾èµ–åˆ†æå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/context/analyze-module")
async def analyze_module_dependencies(request: Request):
    """åˆ†æç‰¹å®šæ¨¡å—çš„ä¾èµ–å…³ç³»"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')
        module_name = data.get('moduleName', '')

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        if not module_name:
            return JSONResponse(
                content={"error": "æ¨¡å—åç§°ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(project_path)
        if not is_valid:
            return JSONResponse(
                content={"error": error},
                status_code=400
            )

        # è·å–ä¾èµ–åˆ†æå™¨
        analyzer = get_code_dependency_analyzer(normalized)

        # åˆ†ææ¨¡å—ä¾èµ–
        result = analyzer.analyze_module_dependencies(module_name)

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        logger.exception(f"æ¨¡å—ä¾èµ–åˆ†æå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ¨¡å—ä¾èµ–åˆ†æå¤±è´¥: {str(e)}"},
            status_code=500
        )


# --- Cursor Sessions API ç«¯ç‚¹ ---

@app.get("/api/cursor/sessions")
async def get_cursor_sessions(projectPath: str = Query(...)):
    """è·å– Cursor sessions åˆ—è¡¨"""
    # TODO: å®ç° Cursor sessions è¯»å–é€»è¾‘
    # Cursor sessions é€šå¸¸å­˜å‚¨åœ¨ ~/.cursor/sessions/ ç›®å½•ä¸‹
    return {
        "success": True,
        "sessions": []
    }

# --- Commands API ç«¯ç‚¹ ---

@app.post("/api/commands/list")
async def list_commands(request: Request):
    """è·å–å¯ç”¨çš„å‘½ä»¤åˆ—è¡¨"""
    # TODO: å®ç°å‘½ä»¤åˆ—è¡¨è¯»å–é€»è¾‘
    return {
        "commands": []
    }

# --- Catch-all è·¯ç”± ---

# OCR API ç«¯ç‚¹
@app.get("/api/ocr/technologies")
async def get_ocr_technologies():
    """è·å–æ”¯æŒçš„ OCR æŠ€æœ¯åˆ—è¡¨"""
    try:
        service = get_ocr_service("lighton")
        technologies = service.get_supported_technologies()
        return JSONResponse(content={"success": True, "technologies": technologies})
    except Exception as e:
        logger.error(f"è·å– OCR æŠ€æœ¯åˆ—è¡¨å¤±è´¥: {e}")
        return JSONResponse(content={"success": False, "error": str(e)}, status_code=500)


@app.post("/api/ocr/process")
async def process_ocr(request: Request):
    """
    å¤„ç†å›¾ç‰‡ OCR
    
    Request body:
    {
        "image": "base64 encoded image",
        "technology": "lighton" | "tesseract" | "paddle" | "easyocr",
        "max_tokens": 4096,
        "temperature": 0.2,
        "top_p": 0.9
    }
    """
    try:
        data = await request.json()
        
        image_data = data.get("image")
        technology = data.get("technology", "lighton")
        max_tokens = data.get("max_tokens", 4096)
        temperature = data.get("temperature", 0.2)
        top_p = data.get("top_p", 0.9)
        
        if not image_data:
            return JSONResponse(
                content={"success": False, "error": "ç¼ºå°‘å›¾ç‰‡æ•°æ®"},
                status_code=400
            )
        
        # è·å– OCR æœåŠ¡
        service = get_ocr_service(technology)
        
        # å¤„ç†å›¾ç‰‡
        result = await service.process_image(
            image_data=image_data,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p
        )
        
        return JSONResponse(content=result)
        
    except Exception as e:
        logger.error(f"OCR å¤„ç†å¤±è´¥: {e}")
        return JSONResponse(
            content={"success": False, "error": str(e)},
            status_code=500
        )


@app.post("/api/ocr/process-pdf")
async def process_pdf_ocr(request: Request):
    """
    å¤„ç† PDF æ–‡ä»¶ OCR
    
    Request body:
    {
        "pdf_data": "base64 encoded pdf",
        "technology": "lighton" | "tesseract" | "paddle" | "easyocr",
        "page_range": [0, 1, 2],  # å¯é€‰,æŒ‡å®šè¦å¤„ç†çš„é¡µç 
        "max_tokens": 4096,
        "temperature": 0.2,
        "top_p": 0.9
    }
    """
    try:
        logger.info("[OCR] æ”¶åˆ° PDF OCR è¯·æ±‚")
        data = await request.json()
        
        pdf_data = data.get("pdf_data")
        technology = data.get("technology", "lighton")
        page_range = data.get("page_range", [])
        max_tokens = data.get("max_tokens", 4096)
        temperature = data.get("temperature", 0.2)
        top_p = data.get("top_p", 0.9)
        dpi = data.get("dpi", 240)
        preprocess = data.get("preprocess", True)
        
        logger.info(f"[OCR] è¯·æ±‚å‚æ•°: technology={technology}, max_tokens={max_tokens}, dpi={dpi}, preprocess={preprocess}, pdf_data_length={len(pdf_data) if pdf_data else 0}")
        
        if not pdf_data:
            logger.warning("[OCR] ç¼ºå°‘ PDF æ•°æ®")
            return JSONResponse(
                content={"success": False, "error": "ç¼ºå°‘ PDF æ•°æ®"},
                status_code=400
            )
        
        # è§£ç  PDF
        import base64
        import io
        import pypdfium2 as pdfium
        
        logger.info("[OCR] å¼€å§‹è§£ç  PDF...")
        pdf_bytes = base64.b64decode(pdf_data)
        pdf = pdfium.PdfDocument(pdf_bytes)
        logger.info(f"[OCR] PDF è§£ç æˆåŠŸï¼Œå…± {len(pdf)} é¡µ")
        
        # ç¡®å®šè¦å¤„ç†çš„é¡µé¢
        if page_range:
            pages_to_process = [pdf[i] for i in page_range if i < len(pdf)]
        else:
            pages_to_process = [pdf[i] for i in range(len(pdf))]
        
        logger.info(f"[OCR] å°†å¤„ç† {len(pages_to_process)} é¡µ")
        
        # è·å– OCR æœåŠ¡
        logger.info(f"[OCR] è·å– OCR æœåŠ¡: {technology}")
        if get_ocr_service is None or not callable(get_ocr_service):
            logger.error("[OCR] OCR æœåŠ¡å…¥å£ä¸å¯ç”¨ï¼ˆget_ocr_service æœªæ­£ç¡®å¯¼å…¥ï¼‰")
            return JSONResponse(
                content={"success": False, "error": "OCR æœåŠ¡æœªå°±ç»ªï¼Œè¯·æ£€æŸ¥åç«¯ OCR ä¾èµ–ä¸å¯¼å…¥é…ç½®"},
                status_code=500
            )
        service = get_ocr_service(technology)
        
        if service is None:
            logger.error(f"[OCR] OCR æœåŠ¡è¿”å› Noneï¼ŒæŠ€æœ¯ç±»å‹: {technology}")
            return JSONResponse(
                content={"success": False, "error": f"OCR æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {technology}"},
                status_code=500
            )
        
        # å¤„ç†æ¯ä¸€é¡µ
        results = []
        for idx, page in enumerate(pages_to_process):
            # æ¸²æŸ“é¡µé¢ä¸ºå›¾ç‰‡ï¼ˆdpi/72 ä¸º scaleï¼‰
            try:
                dpi_value = int(dpi) if dpi else 240
            except Exception:
                dpi_value = 240
            dpi_value = max(120, min(400, dpi_value))
            scale = dpi_value / 72.0
            pil_image = page.render(scale=scale).to_pil()

            if preprocess:
                from PIL import ImageEnhance, ImageFilter
                if pil_image.mode != "RGB":
                    pil_image = pil_image.convert("RGB")
                pil_image = ImageEnhance.Contrast(pil_image).enhance(1.35)
                pil_image = ImageEnhance.Sharpness(pil_image).enhance(1.2)
                pil_image = pil_image.filter(ImageFilter.UnsharpMask(radius=1, percent=120, threshold=3))
            
            # è½¬æ¢ä¸º base64
            buffer = io.BytesIO()
            pil_image.save(buffer, format="PNG")
            image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            
            # å¤„ç† OCR
            result = await service.process_image(
                image_data=image_base64,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )
            
            results.append({
                "page": idx + 1,
                "text": result.get("text", ""),
                "success": result.get("success", False)
            })
        
        # åˆå¹¶æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬
        combined_text = "\n\n".join([r["text"] for r in results if r["success"]])
        
        return JSONResponse(content={
            "success": True,
            "text": combined_text,
            "pages": results,
            "technology": technology,
            "total_pages": len(pages_to_process),
            "format": "markdown" if technology == "lighton" else "plain"
        })
        
    except Exception as e:
        logger.error(f"PDF OCR å¤„ç†å¤±è´¥: {e}")
        return JSONResponse(
            content={"success": False, "error": str(e)},
            status_code=500
        )


@app.post("/api/analyze-project-for-interview")
async def analyze_project_for_interview(request: Request):
    """
    åˆ†æé¡¹ç›®ç»“æ„ç”¨äºé¢è¯•å‡†å¤‡
    
    Request body:
    {
        "project_path": "/path/to/project"
    }
    
    Returns:
    {
        "project_name": "é¡¹ç›®åç§°",
        "tech_stack": {
            "languages": ["JavaScript", "TypeScript"],
            "frameworks": ["React", "Node.js"],
            "databases": ["PostgreSQL"],
            "tools": ["Git", "Docker"]
        },
        "features": [],
        "architecture": "å‰åç«¯åˆ†ç¦»æ¶æ„",
        "complexity": "ä¸­ç­‰"
    }
    """
    try:
        data = await request.json()
        project_path = data.get("project_path")
        
        if not project_path:
            return JSONResponse(
                content={"success": False, "error": "ç¼ºå°‘é¡¹ç›®è·¯å¾„"},
                status_code=400
            )
        
        # è·å–é¡¹ç›®åç§°
        project_name = os.path.basename(project_path)
        
        # åˆ†æé¡¹ç›®æŠ€æœ¯æ ˆ
        tech_stack = {
            "languages": [],
            "frameworks": [],
            "databases": [],
            "tools": []
        }
        
        # æ£€æŸ¥å¸¸è§çš„æŠ€æœ¯æ ˆæ–‡ä»¶
        package_json = os.path.join(project_path, "package.json")
        if os.path.exists(package_json):
            try:
                with open(package_json, 'r', encoding='utf-8') as f:
                    pkg_data = json.load(f)
                    dependencies = {**pkg_data.get('dependencies', {}), **pkg_data.get('devDependencies', {})}
                    
                    # æ£€æµ‹è¯­è¨€
                    if 'typescript' in dependencies:
                        tech_stack["languages"].append("TypeScript")
                    else:
                        tech_stack["languages"].append("JavaScript")
                    
                    # æ£€æµ‹æ¡†æ¶
                    frameworks = []
                    if 'react' in dependencies:
                        frameworks.append("React")
                    if 'vue' in dependencies:
                        frameworks.append("Vue")
                    if 'angular' in dependencies:
                        frameworks.append("Angular")
                    if 'next' in dependencies:
                        frameworks.append("Next.js")
                    if 'nuxt' in dependencies:
                        frameworks.append("Nuxt.js")
                    if 'express' in dependencies:
                        frameworks.append("Express")
                    if 'fastify' in dependencies:
                        frameworks.append("Fastify")
                    if 'koa' in dependencies:
                        frameworks.append("Koa")
                    if 'nest' in dependencies:
                        frameworks.append("NestJS")
                    if 'django' in dependencies:
                        frameworks.append("Django")
                    if 'flask' in dependencies:
                        frameworks.append("Flask")
                    if 'fastapi' in dependencies:
                        frameworks.append("FastAPI")
                    
                    tech_stack["frameworks"] = list(set(frameworks))
                    
                    # æ£€æµ‹æ•°æ®åº“
                    databases = []
                    if 'pg' in dependencies or 'postgres' in dependencies or 'postgresql' in dependencies:
                        databases.append("PostgreSQL")
                    if 'mysql' in dependencies or 'mysql2' in dependencies:
                        databases.append("MySQL")
                    if 'mongodb' in dependencies or 'mongoose' in dependencies:
                        databases.append("MongoDB")
                    if 'redis' in dependencies:
                        databases.append("Redis")
                    if 'sqlite' in dependencies or 'sqlite3' in dependencies:
                        databases.append("SQLite")
                    
                    tech_stack["databases"] = list(set(databases))
                    
                    # æ£€æµ‹å·¥å…·
                    tools = []
                    if 'webpack' in dependencies:
                        tools.append("Webpack")
                    if 'vite' in dependencies:
                        tools.append("Vite")
                    if 'rollup' in dependencies:
                        tools.append("Rollup")
                    if 'jest' in dependencies:
                        tools.append("Jest")
                    if 'mocha' in dependencies:
                        tools.append("Mocha")
                    if 'eslint' in dependencies:
                        tools.append("ESLint")
                    if 'prettier' in dependencies:
                        tools.append("Prettier")
                    if 'docker' in str(dependencies).lower():
                        tools.append("Docker")
                    
                    tech_stack["tools"] = list(set(tools))
                    
            except Exception as e:
                logger.error(f"åˆ†æ package.json å¤±è´¥: {e}")
        
        # æ£€æŸ¥ Python é¡¹ç›®
        requirements_txt = os.path.join(project_path, "requirements.txt")
        if os.path.exists(requirements_txt):
            try:
                with open(requirements_txt, 'r', encoding='utf-8') as f:
                    requirements = f.read().lower()
                    
                    if 'python' not in tech_stack["languages"]:
                        tech_stack["languages"].append("Python")
                    
                    if 'django' in requirements:
                        tech_stack["frameworks"].append("Django")
                    if 'flask' in requirements:
                        tech_stack["frameworks"].append("Flask")
                    if 'fastapi' in requirements:
                        tech_stack["frameworks"].append("FastAPI")
                    if 'pymysql' in requirements or 'mysql' in requirements:
                        tech_stack["databases"].append("MySQL")
                    if 'psycopg2' in requirements or 'postgresql' in requirements:
                        tech_stack["databases"].append("PostgreSQL")
                    if 'pymongo' in requirements:
                        tech_stack["databases"].append("MongoDB")
                    
                    tech_stack["frameworks"] = list(set(tech_stack["frameworks"]))
                    tech_stack["databases"] = list(set(tech_stack["databases"]))
                    
            except Exception as e:
                logger.error(f"åˆ†æ requirements.txt å¤±è´¥: {e}")
        
        # æ£€æŸ¥ Go é¡¹ç›®
        go_mod = os.path.join(project_path, "go.mod")
        if os.path.exists(go_mod):
            if "Go" not in tech_stack["languages"]:
                tech_stack["languages"].append("Go")
            tech_stack["tools"].append("Go Modules")
        
        # å»é‡
        tech_stack["languages"] = list(set(tech_stack["languages"]))
        tech_stack["frameworks"] = list(set(tech_stack["frameworks"]))
        tech_stack["databases"] = list(set(tech_stack["databases"]))
        tech_stack["tools"] = list(set(tech_stack["tools"]))
        
        # ç¡®å®šæ¶æ„ç±»å‹
        architecture = "å•ä½“æ¶æ„"
        if "React" in tech_stack["frameworks"] or "Vue" in tech_stack["frameworks"]:
            if "Express" in tech_stack["frameworks"] or "FastAPI" in tech_stack["frameworks"] or "Django" in tech_stack["frameworks"]:
                architecture = "å‰åç«¯åˆ†ç¦»æ¶æ„"
        
        # ç¡®å®šå¤æ‚åº¦
        complexity = "ç®€å•"
        if len(tech_stack["frameworks"]) >= 3 or len(tech_stack["databases"]) >= 2:
            complexity = "å¤æ‚"
        elif len(tech_stack["frameworks"]) >= 1:
            complexity = "ä¸­ç­‰"
        
        return JSONResponse(content={
            "success": True,
            "project_name": project_name,
            "tech_stack": tech_stack,
            "features": [],
            "architecture": architecture,
            "complexity": complexity
        })
        
    except Exception as e:
        logger.error(f"åˆ†æé¡¹ç›®å¤±è´¥: {e}")
        return JSONResponse(
            content={"success": False, "error": str(e)},
            status_code=500
        )


# --- Chat Suggestions API ---

class ChatSuggestionsRequest(BaseModel):
    context: str
    message_count: int = 5

@app.post("/api/chat/suggestions/{project_name}")
async def generate_chat_suggestions(project_name: str, req: ChatSuggestionsRequest):
    """ç”Ÿæˆæ™ºèƒ½èŠå¤©å»ºè®®"""
    try:
        # åŸºäºä¸Šä¸‹æ–‡åˆ†æç”Ÿæˆå»ºè®®
        context = req.context.lower()
        suggestions = []

        # æ™ºèƒ½å…³é”®è¯åŒ¹é…è§„åˆ™
        keyword_rules = {
            # Bug/é”™è¯¯ç›¸å…³
            ('bug', 'error', 'é—®é¢˜', 'å¼‚å¸¸', 'exception', 'fail'): [
                "å¸®æˆ‘æ‰¾åˆ°è¿™ä¸ªbugçš„æ ¹æœ¬åŸå› ",
                "å¦‚ä½•ä¿®å¤è¿™ä¸ªé—®é¢˜ï¼Ÿ",
                "è¿˜æœ‰å…¶ä»–ç±»ä¼¼çš„é—®é¢˜å—ï¼Ÿ"
            ],
            # æµ‹è¯•ç›¸å…³
            ('test', 'æµ‹è¯•', 'unit test', 'pytest', 'jest'): [
                "å¸®æˆ‘ç¼–å†™å•å…ƒæµ‹è¯•",
                "ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹",
                "å¦‚ä½•æé«˜æµ‹è¯•è¦†ç›–ç‡ï¼Ÿ"
            ],
            # æ€§èƒ½ä¼˜åŒ–
            ('optimize', 'ä¼˜åŒ–', 'performance', 'æ€§èƒ½', 'slow', 'æ…¢'): [
                "å¦‚ä½•ä¼˜åŒ–è¿™æ®µä»£ç çš„æ€§èƒ½ï¼Ÿ",
                "è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ",
                "æä¾›ä¼˜åŒ–å»ºè®®"
            ],
            # é‡æ„
            ('refactor', 'é‡æ„', 'improve', 'æ”¹è¿›', 'clean'): [
                "é‡æ„è¿™æ®µä»£ç ",
                "æ”¹è¿›ä»£ç ç»“æ„",
                "åº”ç”¨è®¾è®¡æ¨¡å¼"
            ],
            # æ–‡æ¡£
            ('document', 'æ–‡æ¡£', 'readme', 'api doc', 'æ³¨é‡Š'): [
                "ç”Ÿæˆä»£ç æ–‡æ¡£",
                "ç¼–å†™APIæ–‡æ¡£",
                "åˆ›å»ºREADMEæ–‡ä»¶"
            ],
            # API
            ('api', 'endpoint', 'æ¥å£', 'request', 'response'): [
                "è®¾è®¡APIæ¥å£",
                "ç”ŸæˆAPIæ–‡æ¡£",
                "æµ‹è¯•APIç«¯ç‚¹"
            ],
            # åŠŸèƒ½å¼€å‘
            ('feature', 'åŠŸèƒ½', 'implement', 'å®ç°', 'add'): [
                "è®¾è®¡æ–°åŠŸèƒ½",
                "å®ç°åŠŸèƒ½éœ€æ±‚",
                "ç¼–å†™åŠŸèƒ½æµ‹è¯•"
            ],
            # ä»£ç å®¡æŸ¥
            ('review', 'å®¡æŸ¥', 'check', 'æ£€æŸ¥', 'quality'): [
                "è¿›è¡Œä»£ç å®¡æŸ¥",
                "æ£€æŸ¥ä»£ç è´¨é‡",
                "æä¾›æ”¹è¿›å»ºè®®"
            ],
            # æ•°æ®åº“
            ('database', 'æ•°æ®åº“', 'sql', 'query', 'schema'): [
                "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢",
                "è®¾è®¡æ•°æ®åº“ç»“æ„",
                "ç”ŸæˆSQLè„šæœ¬"
            ],
            # éƒ¨ç½²
            ('deploy', 'éƒ¨ç½²', 'docker', 'k8s', 'production'): [
                "é…ç½®éƒ¨ç½²ç¯å¢ƒ",
                "ç¼–å†™Dockerfile",
                "è®¾ç½®CI/CDæµç¨‹"
            ],
            # å®‰å…¨
            ('security', 'å®‰å…¨', 'vulnerability', 'æ¼æ´', 'auth'): [
                "æ£€æŸ¥å®‰å…¨æ¼æ´",
                "å®ç°èº«ä»½è®¤è¯",
                "åŠ å¼ºæ•°æ®åŠ å¯†"
            ],
        }

        # åŒ¹é…å…³é”®è¯
        matched = False
        for keywords, suggestion_list in keyword_rules.items():
            if any(keyword in context for keyword in keywords):
                suggestions.extend(suggestion_list)
                matched = True
                break

        # å¦‚æœåŒ…å«ä»£ç ï¼Œæä¾›ä»£ç ç›¸å…³å»ºè®®
        if '```' in req.context:
            code_suggestions = [
                "è§£é‡Šè¿™æ®µä»£ç çš„å·¥ä½œåŸç†",
                "ä¼˜åŒ–è¿™æ®µä»£ç ",
                "ä¸ºè¿™æ®µä»£ç æ·»åŠ æ³¨é‡Š",
                "è¿™æ®µä»£ç æœ‰ä»€ä¹ˆæ½œåœ¨é—®é¢˜ï¼Ÿ"
            ]
            # å¦‚æœå·²ç»åŒ¹é…åˆ°å…¶ä»–è§„åˆ™ï¼Œåˆå¹¶å»ºè®®
            if matched:
                suggestions = suggestions + code_suggestions[:2]
            else:
                suggestions = code_suggestions

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šå…³é”®è¯ï¼Œæ ¹æ®æ¶ˆæ¯æ•°é‡æä¾›é€šç”¨å»ºè®®
        if not suggestions:
            if req.message_count <= 2:
                # å¯¹è¯åˆšå¼€å§‹ï¼Œæä¾›æ¢ç´¢æ€§å»ºè®®
                suggestions = [
                    "å¸®æˆ‘åˆ†æè¿™ä¸ªé¡¹ç›®çš„ä»£ç ç»“æ„",
                    "ç”Ÿæˆé¡¹ç›®æ–‡æ¡£",
                    "æ£€æŸ¥ä»£ç ä¸­çš„æ½œåœ¨é—®é¢˜"
                ]
            else:
                # å¯¹è¯è¿›è¡Œä¸­ï¼Œæä¾›æ·±å…¥å»ºè®®
                suggestions = [
                    "ç»§ç»­æ·±å…¥åˆ†æ",
                    "æä¾›æ›´å¤šç¤ºä¾‹",
                    "æ€»ç»“å…³é”®è¦ç‚¹",
                    "ç»™å‡ºæœ€ä½³å®è·µå»ºè®®"
                ]

        # é™åˆ¶å»ºè®®æ•°é‡ä¸º3-4ä¸ª
        suggestions = suggestions[:4]

        logger.info(f"Generated {len(suggestions)} suggestions for project {project_name}")

        return {
            "suggestions": suggestions,
            "context_length": len(req.context),
            "message_count": req.message_count
        }

    except Exception as e:
        logger.error(f"Error generating chat suggestions: {e}")
        # è¿”å›é»˜è®¤å»ºè®®
        return {
            "suggestions": [
                "å¸®æˆ‘åˆ†æè¿™ä¸ªé¡¹ç›®çš„ä»£ç ç»“æ„",
                "ç”Ÿæˆé¡¹ç›®æ–‡æ¡£",
                "æ£€æŸ¥ä»£ç ä¸­çš„æ½œåœ¨é—®é¢˜"
            ],
            "error": str(e)
        }


@app.api_route("/api/{path_name:path}", methods=["GET", "POST", "PUT", "DELETE"])
async def catch_all(path_name: str, request: Request):
    """Catch-all è·¯ç”± - å¤„ç†æœªå®ç°çš„ API ç«¯ç‚¹"""
    logger.warning(f"æœªå¤„ç†çš„ API è¯·æ±‚: {request.method} /api/{path_name}")

    # MCP ç›¸å…³çš„ API
    if path_name.startswith("mcp-utils/"):
        return JSONResponse(content={
            "status": "not-implemented",
            "message": f"MCP endpoint '{path_name}' is not implemented"
        }, status_code=200)

    # é»˜è®¤å“åº”
    return JSONResponse(content={"status": "mocked", "sessions": [], "hasMore": False}, status_code=200)


if __name__ == "__main__":
    import uvicorn
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    uvicorn.run(app, host="0.0.0.0", port=8000)
