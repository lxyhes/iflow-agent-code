import sys
import os
import mimetypes
import asyncio
import json
import platform
import subprocess
import logging
from datetime import datetime

# Windows äº‹ä»¶å¾ªç¯ç­–ç•¥è®¾ç½® - å¿…é¡»åœ¨ä»»ä½•å¼‚æ­¥æ“ä½œä¹‹å‰è®¾ç½®
if platform.system() == 'Windows':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

from fastapi import FastAPI, HTTPException, Request, Query, Body, WebSocket
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse, FileResponse

# é…ç½®æ—¥å¿— - æ”¯æŒç¯å¢ƒå˜é‡
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
if log_level not in valid_levels:
    logger.warning(f"Invalid LOG_LEVEL: {log_level}, using INFO")
    log_level = "INFO"

logging.basicConfig(
    level=getattr(logging, log_level),
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("Server")
logger.info(f"æ—¥å¿—çº§åˆ«è®¾ç½®ä¸º: {log_level}")

# Add backend to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/..")

from backend.impl.reviewer import create_code_review_agent
from backend.core.project_manager import project_manager
from backend.core.agent import Agent
from backend.core.smart_requirement_service import smart_requirement_service
from backend.core.cicd_generator import cicd_generator
from backend.core.project_template_service import get_project_template_service
from backend.core.task_master_service import task_master_service, Task as TaskModel
from backend.core.file_service import file_service
from backend.core.git_service import git_service
from backend.core.shell_service import ShellSession
from backend.core.path_validator import PathValidator, project_registry
from backend.core.error_analyzer import ErrorAnalyzer, get_error_analyzer
from backend.core.code_style_analyzer import CodeStyleAnalyzer, get_code_style_analyzer
from backend.core.report_generator import ReportGenerator, get_report_generator
from backend.core.dependency_analyzer import DependencyAnalyzer, get_dependency_analyzer
from backend.core.auto_fixer import AutoFixer, get_auto_fixer
from backend.core.code_dependency_analyzer import CodeDependencyAnalyzer, get_dependency_analyzer as get_code_dependency_analyzer
from backend.core.prompt_optimizer import PromptOptimizer, get_prompt_optimizer
from backend.core.rag_service import RAGService, get_rag_service
from backend.core.document_version_manager import get_version_manager
from backend.core.database_query_service import database_query_service
from backend.core.workflow_service import workflow_service
from backend.core.workflow_executor import workflow_executor

app = FastAPI(title="IFlow Agent API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- CACHE MANAGER ---
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ï¼Œæ”¯æŒè‡ªåŠ¨æ¸…ç†å’Œå¤§å°é™åˆ¶"""

    def __init__(self, max_size=100, name="Cache"):
        self.cache = {}
        self.max_size = max_size
        self.name = name
        self.access_times = {}
        self._total_accesses = 0
        self._hits = 0

    def get(self, key):
        """è·å–ç¼“å­˜é¡¹"""
        if key in self.cache:
            self.access_times[key] = datetime.now().timestamp()
            self._total_accesses += 1
            self._hits += 1
            return self.cache[key]
        self._total_accesses += 1
        return None

    def set(self, key, value):
        """è®¾ç½®ç¼“å­˜é¡¹ï¼Œå¦‚æœè¶…å‡ºå¤§å°é™åˆ¶åˆ™æ¸…ç†æœ€æ—§çš„é¡¹"""
        if len(self.cache) >= self.max_size and key not in self.cache:
            self._cleanup_oldest()
        self.cache[key] = value
        self.access_times[key] = datetime.now().timestamp()

    def _cleanup_oldest(self):
        """æ¸…ç†æœ€æ—§çš„ç¼“å­˜é¡¹"""
        if not self.access_times:
            return

        oldest_key = min(self.access_times, key=self.access_times.get)
        del self.cache[oldest_key]
        del self.access_times[oldest_key]
        logger.debug(f"[{self.name}] Cleaned up oldest cache entry: {oldest_key}")

    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        count = len(self.cache)
        self.cache.clear()
        self.access_times.clear()
        logger.info(f"[{self.name}] Cleared {count} cache entries")

    def get_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        hit_rate = (self._hits / self._total_accesses * 100) if self._total_accesses > 0 else 0
        return {
            "name": self.name,
            "size": len(self.cache),
            "max_size": self.max_size,
            "total_accesses": self._total_accesses,
            "hits": self._hits,
            "hit_rate": f"{hit_rate:.2f}%"
        }

    def __contains__(self, key):
        return key in self.cache

    def __len__(self):
        return len(self.cache)

# --- MODELS ---
class CreateProjectRequest(BaseModel): path: str

class CreateWorkspaceRequest(BaseModel):
    workspaceType: str
    path: str
    githubUrl: str = None
    githubTokenId: int = None
    newGithubToken: str = None
class SaveFileRequest(BaseModel): filePath: str; content: str
class CheckoutRequest(BaseModel): project: str; branch: str
class CommitRequest(BaseModel): project: str; message: str; files: list

# --- GLOBAL CONFIG ---
global_config = {
    "mode": "yolo",
    "model": "GLM-4.7", # Set to recommended model
    "mcp_servers": [],
    "iflow_path": "iflow", # Default command
    "rag_mode": "tfidf" # RAG æ¨¡å¼: "chromadb" (éœ€è¦ä¸‹è½½æ¨¡å‹) æˆ– "tfidf" (è½»é‡çº§)
}

# --- HELPERS ---
def get_iflow_version():
    try:
        cmd = global_config.get("iflow_path", "iflow")
        result = subprocess.run([cmd, "--version"], capture_output=True, text=True, timeout=2)
        if result.returncode == 0:
            return result.stdout.strip()
    except:
        return None
    return None

def get_agent(cwd: str, mode: str = "yolo", model: str = None, mcp_servers: list = None, persona: str = "partner", auth_method_id: str = None, auth_method_info: dict = None):
    key = (cwd, mode, model, json.dumps(mcp_servers or []), persona, auth_method_id)
    if key not in agent_cache:
        system_prompt = PERSONA_PROMPTS.get(persona, PERSONA_PROMPTS["partner"])
        agent_cache[key] = Agent(name="IFlowAgent", cwd=cwd, mode=mode, model=model, mcp_servers=mcp_servers, persona=persona, system_prompt=system_prompt, auth_method_id=auth_method_id, auth_method_info=auth_method_info)
    return agent_cache[key]

def get_project_path(project_name: str) -> str:
    """å®‰å…¨åœ°è·å–é¡¹ç›®è·¯å¾„ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»"""
    logger.info(f"[get_project_path] Looking for project: '{project_name}'")

    if not project_name:
        logger.warning(f"[get_project_path] No project name provided, returning cwd: {os.getcwd()}")
        return os.getcwd()

    # æ£€æŸ¥ project_name æ˜¯å¦æœ¬èº«å°±æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„é¡¹ç›®è·¯å¾„
    # å¦‚æœåŒ…å«è·¯å¾„åˆ†éš”ç¬¦ï¼ˆWindows: \ æˆ– /ï¼‰ï¼Œåˆ™è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªè·¯å¾„
    if '\\' in project_name or '/' in project_name:
        # éªŒè¯è·¯å¾„å®‰å…¨æ€§
        is_valid, error, normalized = PathValidator.validate_project_path(project_name)
        if is_valid and os.path.exists(normalized):
            logger.info(f"[get_project_path] project_name is a valid path: {normalized}")
            # æ³¨å†Œåˆ°é¡¹ç›®æ³¨å†Œè¡¨
            project_registry.register_project(os.path.basename(normalized), normalized)
            return normalized

    # é¦–å…ˆå°è¯•ä»æ³¨å†Œè¡¨è·å–
    registered_path = project_registry.get_project_path(project_name)
    if registered_path:
        logger.info(f"[get_project_path] Found in registry: {registered_path}")
        return registered_path
    
    logger.info(f"[get_project_path] Not in registry, checking project_manager...")
    
    # ç„¶åä» project_manager è·å–
    projects = project_manager.get_projects()
    logger.info(f"[get_project_path] Found {len(projects)} projects in manager")
    for p in projects:
        logger.info(f"[get_project_path]   - {p.get('name')}: {p.get('fullPath')}")
        if p["name"] == project_name:
            # éªŒè¯è·¯å¾„å®‰å…¨æ€§
            is_valid, error, normalized = PathValidator.validate_project_path(p["fullPath"])
            if is_valid:
                project_registry.register_project(p["name"], normalized)
                logger.info(f"[get_project_path] Found in project_manager: {normalized}")
                return normalized

    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•åœ¨çˆ¶ç›®å½•ä¸‹å¯»æ‰¾åŒ¹é…çš„é¡¹ç›®æ–‡ä»¶å¤¹å
    # è·å– backend çš„çˆ¶ç›®å½•å³ agent_project
    current_base = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    logger.info(f"[get_project_path] Checking if project_name matches current_base: {project_name} == {os.path.basename(current_base)}")
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…å½“å‰é¡¹ç›®æ–‡ä»¶å¤¹å
    if project_name == os.path.basename(current_base):
        logger.info(f"[get_project_path] Matched current_base: {current_base}")
        return current_base
        
    # æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•çš„çˆ¶ç›®å½•
    parent_dir = os.path.dirname(os.getcwd())
    potential_path = os.path.join(parent_dir, project_name)
    logger.info(f"[get_project_path] Checking potential_path: {potential_path}")
    if os.path.isdir(potential_path):
        is_valid, _, normalized = PathValidator.validate_project_path(potential_path)
        if is_valid:
            project_registry.register_project(project_name, normalized)
            logger.info(f"[get_project_path] Found in parent_dir: {normalized}")
            return normalized
    
    # ä¸å†ç›´æ¥è¿”å›ç”¨æˆ·è¾“å…¥çš„è·¯å¾„ï¼Œè€Œæ˜¯è¿”å›å®‰å…¨çš„é»˜è®¤å€¼
    logger.warning(f"[get_project_path] æœªæ‰¾åˆ°é¡¹ç›®: {project_name}, è¿”å›å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    return os.getcwd()

# ä»ç¯å¢ƒå˜é‡è¯»å–ç¼“å­˜é…ç½®
agent_cache_max_size = int(os.getenv("AGENT_CACHE_MAX_SIZE", "100"))
rag_cache_max_size = int(os.getenv("RAG_CACHE_MAX_SIZE", "50"))

logger.info(f"Agent ç¼“å­˜æœ€å¤§å¤§å°: {agent_cache_max_size}")
logger.info(f"RAG ç¼“å­˜æœ€å¤§å¤§å°: {rag_cache_max_size}")

# ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨
agent_cache = CacheManager(max_size=agent_cache_max_size, name="AgentCache")
rag_cache = CacheManager(max_size=rag_cache_max_size, name="RAGCache")

# AI Persona System Prompts
PERSONA_PROMPTS = {
    "senior": """You are a senior software architect with 15+ years of experience. Your role is to ensure code excellence.

STRICT GUIDELINES:
- Always review code quality first before suggesting solutions
- Point out potential bugs, security issues, and performance problems
- Enforce best practices: SOLID principles, DRY, clean code
- Require proper error handling, logging, and testing
- Suggest design patterns and architectural improvements
- Reject quick-and-dirty solutions that lack robustness
- Prioritize maintainability, scalability, and readability
- Ask clarifying questions to understand the full context

RESPONSE STYLE:
- Professional and authoritative but constructive
- Provide detailed explanations for your recommendations
- Reference industry standards and common pitfalls
- Suggest refactoring when code is messy
- Emphasize long-term code health over quick wins

Example: "This approach has a critical flaw: it doesn't handle edge cases. Let me show you a more robust solution that follows the Repository pattern..."
""",
    "hacker": """You are a pragmatic hacker who values shipping over perfection. Your role is to get things working fast.

CORE PHILOSOPHY:
- Working code > perfect code
- Ship first, iterate later
- Minimize boilerplate and ceremony
- Use the simplest solution that works
- Skip excessive comments and documentation during dev
- Focus on the happy path, handle errors later if needed
- Copy-paste is fine if it saves time
- Use whatever libraries/tools get the job done

RESPONSE STYLE:
- Direct and action-oriented
- Minimal explanations, maximum code
- "Here's the solution" not "Let's discuss the approach"
- Skip lengthy justifications
- Assume the user knows what they're doing
- Provide shortcuts and quick fixes

Example: "Here's the code. Copy-paste it and you're done. If it breaks, we'll fix it then."
""",
    "partner": """You are an empathetic pair programming partner. Your role is to be supportive and encouraging.

EMOTIONAL INTELLIGENCE:
- Celebrate small wins and progress
- Acknowledge when tasks are difficult
- Be patient with mistakes and confusion
- Provide reassurance when debugging is frustrating
- Use encouraging language: "Great question!", "Nice work!", "We'll get this!"
- Normalize the struggle: "This is tricky, let's work through it together"
- Boost confidence: "You're doing great!", "Almost there!"

COLLABORATIVE STYLE:
- Ask questions to understand their thinking
- Suggest alternatives without being pushy
- Explain concepts in simple, friendly terms
- Share enthusiasm for solving problems together
- Make coding feel like a team effort
- Use "we" language: "Let's try this", "We can fix that"

Example: "That's a great idea! Let's implement it together. I know this part can be tricky, but we'll figure it out. Nice work getting this far!"
"""
}

# --- API ENDPOINTS ---

def check_iflow_auth():
    try:
        # å°è¯•è¿è¡Œ auth status æŸ¥çœ‹æ˜¯å¦ç™»å½•
        result = subprocess.run(["iflow", "auth", "status"], capture_output=True, text=True, timeout=2)
        return "Logged in" in result.stdout or result.returncode == 0
    except:
        return False

@app.get("/api/auth/status")
async def auth_status():
    version = get_iflow_version()
    # FORCE CONNECTED if we suspect it's running but command fails
    is_connected = True if version else True 
    
    return {
        "authenticated": True, 
        "is_iflow_installed": is_connected,
        "is_iflow_authenticated": True,
        "iflow_version": version or "Active Session",
        "install_command": "npm install -g iflow-cli",
        "user": {"username": "iflow-dev"},
        "provider": "iflow"
    }

@app.get("/api/config")
async def get_config(): return global_config

@app.post("/api/config")
async def update_config(config: dict = Body(...)):
    global_config.update(config)
    agent_cache.clear()
    return global_config


@app.get("/api/iflow/mcp-servers")
async def get_iflow_mcp_servers():
    """ä» iFlow é…ç½®æ–‡ä»¶è¯»å–å·²é…ç½®çš„ MCP æœåŠ¡å™¨"""
    try:
        # iFlow é…ç½®æ–‡ä»¶è·¯å¾„
        iflow_config_path = os.path.expanduser("~/.iflow/settings.json")

        if not os.path.exists(iflow_config_path):
            return {"success": True, "servers": [], "message": "iFlow é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"}

        # è¯»å–é…ç½®æ–‡ä»¶
        with open(iflow_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # æå– MCP æœåŠ¡å™¨é…ç½®
        mcp_servers = config.get("mcpServers", {})

        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        servers_list = []
        for server_name, server_config in mcp_servers.items():
            if isinstance(server_config, dict):
                servers_list.append({
                    "name": server_name,
                    "type": server_config.get("type", "stdio"),
                    "config": {
                        "command": server_config.get("command", ""),
                        "args": server_config.get("args", []),
                        "env": server_config.get("env", {}),
                        "url": server_config.get("url", ""),
                        "headers": server_config.get("headers", {}),
                        "timeout": server_config.get("timeout", 30000)
                    }
                })

        logger.info(f"ä» iFlow é…ç½®è¯»å–åˆ° {len(servers_list)} ä¸ª MCP æœåŠ¡å™¨")
        return {
            "success": True,
            "servers": servers_list,
            "config_path": iflow_config_path
        }

    except FileNotFoundError:
        return {"success": True, "servers": [], "message": "iFlow é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"}
    except json.JSONDecodeError as e:
        logger.error(f"è§£æ iFlow é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {"success": False, "error": f"é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"}
    except Exception as e:
        logger.error(f"è¯»å– iFlow MCP é…ç½®å¤±è´¥: {e}")
        return {"success": False, "error": f"è¯»å–å¤±è´¥: {str(e)}"}


@app.post("/api/iflow/sync-mcp-servers")
async def sync_iflow_mcp_servers():
    """ä» iFlow é…ç½®åŒæ­¥ MCP æœåŠ¡å™¨åˆ°åç«¯ global_config"""
    try:
        # è¯»å– iFlow MCP é…ç½®
        result = await get_iflow_mcp_servers()

        if not result.get("success"):
            return {"success": False, "error": result.get("error")}

        servers = result.get("servers", [])

        # æ›´æ–° global_config
        global_config["mcp_servers"] = servers

        # æ¸…é™¤ agent ç¼“å­˜ä»¥ä½¿ç”¨æ–°é…ç½®
        agent_cache.clear()

        logger.info(f"å·²ä» iFlow åŒæ­¥ {len(servers)} ä¸ª MCP æœåŠ¡å™¨åˆ°åç«¯")
        return {
            "success": True,
            "servers_count": len(servers),
            "servers": servers
        }

    except Exception as e:
        logger.error(f"åŒæ­¥ iFlow MCP æœåŠ¡å™¨å¤±è´¥: {e}")
        return {"success": False, "error": f"åŒæ­¥å¤±è´¥: {str(e)}"}


@app.get("/api/projects")
async def get_projects():
    """è·å–é¡¹ç›®åˆ—è¡¨ - å¢å¼ºå®‰å…¨ç‰ˆæœ¬"""
    projects = project_manager.get_projects()
    
    # éªŒè¯å¹¶è¿‡æ»¤æ¯ä¸ªé¡¹ç›®çš„è·¯å¾„
    safe_projects = []
    for p in projects:
        is_valid, error, normalized = PathValidator.validate_project_path(p.get("fullPath", ""), must_exist=False)
        if is_valid:
            # æ³¨å†Œåˆ°å…¨å±€æ³¨å†Œè¡¨
            project_registry.register_project(p["name"], normalized)
            safe_projects.append(p)
        else:
            logger.warning(f"è·¯å¾„ä¸å®‰å…¨çš„é¡¹ç›®: {p.get('name')} - {error}")
            # è‡ªåŠ¨å°†è¯¥è·¯å¾„æ·»åŠ åˆ°å…è®¸çš„æ ¹ç›®å½•åˆ—è¡¨
            project_path = p.get("fullPath", "")
            if project_path:
                PathValidator.add_allowed_root(project_path)
                PathValidator.add_allowed_root(os.path.dirname(project_path))
                logger.info(f"å·²å°†è·¯å¾„æ·»åŠ åˆ°å…è®¸åˆ—è¡¨: {project_path}")
                # é‡æ–°éªŒè¯
                is_valid, error, normalized = PathValidator.validate_project_path(p.get("fullPath", ""), must_exist=False)
                if is_valid:
                    project_registry.register_project(p["name"], normalized)
                    safe_projects.append(p)
    
    # è‡ªåŠ¨æ‰«æé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„å…¶ä»–é¡¹ç›®ï¼ˆä½†éœ€è¦éªŒè¯ï¼‰
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    PathValidator.add_allowed_root(root_dir)  # å…è®¸é¡¹ç›®æ ¹ç›®å½•
    
    try:
        for item in os.listdir(root_dir):
            # è·³è¿‡éšè—æ–‡ä»¶å’Œå·²å¤„ç†çš„é¡¹ç›®
            if item.startswith('.') or item in ['agent_project', 'node_modules', '__pycache__', 'storage']:
                continue
            
            full_path = os.path.join(root_dir, item)
            
            # éªŒè¯è·¯å¾„æ˜¯å¦å®‰å…¨
            is_valid, error, normalized = PathValidator.validate_project_path(full_path)
            if not is_valid:
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if not any(p["name"] == item for p in safe_projects):
                safe_projects.append({
                    "name": item,
                    "displayName": item,
                    "path": full_path,
                    "fullPath": full_path,
                    "sessions": [],
                    "sessionMeta": {"total": 0}
                })
    except Exception as e:
        logger.error(f"æ‰«æé¡¹ç›®æ ¹ç›®å½•å¤±è´¥: {e}")
    
    return safe_projects

@app.get("/stream")
async def stream_endpoint(message: str, cwd: str = None, sessionId: str = None, project: str = None, model: str = None, persona: str = "partner", auth_method_id: str = None, auth_method_info: str = None):
    logger.info(f"=== /stream request ===")
    logger.info(f"  message: {message[:100]}...")
    logger.info(f"  model: {model}")
    logger.info(f"  persona: {persona}")
    logger.info(f"  auth_method_id: {auth_method_id}")

    target_cwd = cwd or os.getcwd()
    project_name = project or os.path.basename(target_cwd)
    if sessionId: project_manager.save_message(project_name, sessionId, "user", message)

    # è§£æè®¤è¯æ–¹æ³•ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    auth_info = None
    if auth_method_info:
        try:
            auth_info = json.loads(auth_method_info)
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse auth_method_info: {auth_method_info}")

    # Use provided model or fallback to global config
    target_model = model or global_config.get("model")

    agent = get_agent(
        target_cwd,
        global_config["mode"],
        target_model,
        global_config.get("mcp_servers"),
        persona=persona,
        auth_method_id=auth_method_id,
        auth_method_info=auth_info
    )
    
    async def event_generator():
        yield f"data: {json.dumps({'type': 'status', 'content': 'IFlow is thinking...'})}\n\n"
        full_reply = ""
        try:
            async for msg in agent.chat_stream(message):
                # æ£€æŸ¥ msg æ˜¯å­—ç¬¦ä¸²è¿˜æ˜¯å­—å…¸
                if isinstance(msg, str):
                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½œä¸ºå†…å®¹è¿”å›ï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                    content = msg
                    full_reply += content
                    yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                else:
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œå¤„ç† SDK å®¢æˆ·ç«¯è¿”å›çš„æ¶ˆæ¯ç±»å‹
                    msg_type = msg.get("type", "text")
                    logger.debug(f">>> Stream msg: type={msg_type}, keys={list(msg.keys())}")
                    
                    if msg_type == "assistant":
                        # AI å›å¤ï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        content = msg.get("content", "")
                        full_reply += content
                        agent_info = msg.get("metadata", {}).get("agent_info")
                        yield f"data: {json.dumps({'type': 'content', 'content': content, 'agent_info': agent_info})}\n\n"
                        
                    elif msg_type == "tool_call":
                        # å·¥å…·è°ƒç”¨ï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        metadata = msg.get("metadata", {})
                        tool_name = metadata.get("tool_name", "unknown")
                        status = metadata.get("status", "running")
                        agent_info = metadata.get("agent_info")
                        
                        if status == "running":
                            # å·¥å…·å¼€å§‹æ‰§è¡Œ
                            event_data = {'type': 'tool_start', 'tool_type': 'generic', 'tool_name': tool_name, 'label': metadata.get('label', ''), 'agent_info': agent_info}
                            logger.info(f">>> TOOL_START: {event_data}")
                            yield f"data: {json.dumps(event_data)}\n\n"
                        else:
                            # å·¥å…·æ‰§è¡Œå®Œæˆ
                            event_data = {'type': 'tool_end', 'tool_type': 'generic', 'tool_name': tool_name, 'status': status, 'agent_info': agent_info}
                            logger.info(f">>> TOOL_END: {event_data}")
                            yield f"data: {json.dumps(event_data)}\n\n"
                            
                    elif msg_type == "plan":
                        # æ‰§è¡Œè®¡åˆ’ï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        entries = msg.get("metadata", {}).get("entries", [])
                        event_data = {'type': 'plan', 'entries': entries}
                        logger.info(f">>> PLAN: {len(entries)} entries")
                        yield f"data: {json.dumps(event_data)}\n\n"
                        
                    elif msg_type == "finish":
                        # ä»»åŠ¡å®Œæˆï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        metadata = msg.get("metadata", {})
                        logger.info(f">>> FINISH: {metadata}")
                        break
                        
                    elif msg_type == "error":
                        # é”™è¯¯ï¼ˆSDK å®¢æˆ·ç«¯ï¼‰
                        error_content = msg.get("content", "Unknown error")
                        logger.error(f">>> ERROR: {error_content}")
                        yield f"data: {json.dumps({'type': 'error', 'content': error_content})}\n\n"
                        
                    elif msg_type == "text":
                        # æ–‡æœ¬æ¶ˆæ¯ï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                        content = msg.get("content", "")
                        full_reply += content
                        yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                        
                    elif msg_type == "tool_start":
                        # å·¥å…·å¼€å§‹æ‰§è¡Œï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                        event_data = {'type': 'tool_start', 'tool_type': msg.get('tool_type'), 'tool_name': msg.get('tool_name'), 'label': msg.get('label', ''), 'agent_info': msg.get('agent_info')}
                        logger.info(f">>> TOOL_START: {event_data}")
                        yield f"data: {json.dumps(event_data)}\n\n"
                        
                    elif msg_type == "tool_end":
                        # å·¥å…·æ‰§è¡Œå®Œæˆï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                        event_data = {'type': 'tool_end', 'tool_type': msg.get('tool_type'), 'tool_name': msg.get('tool_name'), 'status': msg.get('status', 'success'), 'agent_info': msg.get('agent_info')}
                        logger.info(f">>> TOOL_END: {event_data}")
                        yield f"data: {json.dumps(event_data)}\n\n"
                        
                    elif msg_type == "done":
                        # å®Œæˆï¼ˆæ—§å®¢æˆ·ç«¯å…¼å®¹ï¼‰
                        break
                    
            logger.info(f"Stream completed, reply length: {len(full_reply)}")
        except Exception as e:
            logger.exception(f"Error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"
        if sessionId: project_manager.save_message(project_name, sessionId, "assistant", full_reply)
        yield f"data: {json.dumps({'type': 'done'})}\n\n"
    return StreamingResponse(event_generator(), media_type="text/event-stream")

@app.get("/api/projects/{project_name}/files")
async def get_project_files(project_name: str):
    return file_service.get_tree(get_project_path(project_name))

@app.get("/api/projects/{project_name}/file")
async def read_project_file(project_name: str, filePath: str):
    try: return {"content": file_service.read_file(get_project_path(project_name), filePath)}
    except: raise HTTPException(status_code=404)

@app.get("/api/projects/{project_name}/files/content")
async def read_project_file_content(project_name: str, filePath: str):
    try:
        root_path = get_project_path(project_name)

        if '..' in filePath.replace('\\', '/').split('/'):
            raise HTTPException(status_code=403, detail="Access denied: path traversal detected")

        full_path = os.path.normpath(os.path.join(root_path, filePath))
        real_root = os.path.realpath(root_path)
        real_full = os.path.realpath(full_path) if os.path.exists(full_path) else full_path

        if not real_full.startswith(real_root + os.sep) and real_full != real_root:
            raise HTTPException(status_code=403, detail="Access denied: path outside project directory")

        if not os.path.exists(full_path) or not os.path.isfile(full_path):
            raise HTTPException(status_code=404, detail="File not found")

        file_size = os.path.getsize(full_path)
        if file_size > 20 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="File too large to preview")

        media_type = mimetypes.guess_type(full_path)[0] or "application/octet-stream"
        return FileResponse(full_path, media_type=media_type)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Error serving file content: {e}")
        raise HTTPException(status_code=500, detail="Error reading file")

@app.put("/api/projects/{project_name}/file")
async def save_project_file(project_name: str, req: SaveFileRequest):
    try:
        file_service.write_file(get_project_path(project_name), req.filePath, req.content)
        return {"status": "success"}
    except Exception as e: return JSONResponse(content={"error": str(e)}, status_code=500)

@app.get("/api/git/status")
async def get_git_status(project: str = Query(None)):
    path = get_project_path(project)
    logger.info(f"Getting git status for project '{project}' at path: '{path}'")
    return await git_service.get_status(path)

@app.get("/api/git/branches")
async def get_branches(project: str = Query(None)):
    branches = await git_service.get_branches(get_project_path(project))
    return {"branches": branches}

@app.get("/api/git/remote-status")
async def get_remote_status(project: str = Query(None)):
    return await git_service.get_remote_status(get_project_path(project))

@app.get("/api/git/diff")
async def get_diff(project: str = Query(None), file: str = Query(None)):
    diff = await git_service.get_diff(get_project_path(project), file)
    return {"diff": diff}

@app.get("/api/git/commits")
async def get_commits(project: str = Query(None), limit: int = 10):
    commits = await git_service.get_commits(get_project_path(project), limit)
    return {"commits": commits}

@app.get("/api/git/commit-diff")
async def get_commit_diff(project: str = Query(None), commit: str = Query(None)):
    diff = await git_service.get_commit_diff(get_project_path(project), commit)
    return {"diff": diff}

@app.post("/api/git/checkout")
async def checkout_branch(req: CheckoutRequest):
    await git_service.checkout(get_project_path(req.project), req.branch)
    return {"success": True}

@app.post("/api/git/create-branch")
async def create_new_branch(req: CheckoutRequest):
    await git_service.create_branch(get_project_path(req.project), req.branch)
    return {"success": True}

@app.post("/api/git/commit")
async def commit_changes(req: CommitRequest):
    output = await git_service.commit(get_project_path(req.project), req.message, req.files)
    return {"success": True, "output": output}

@app.get("/api/git/file-with-diff")
async def get_file_with_diff(project: str = Query(None), file: str = Query(None)):
    logger.info(f"[GitDiff] project={project}, file={file}")
    path = get_project_path(project)
    current_content = ""
    try:
        current_content = file_service.read_file(path, file)
    except Exception as e:
        logger.warning(f"[GitDiff] Failed to read current file: {e}")
        pass # File might be deleted

    old_content = await git_service.get_file_at_head(path, file)

    return {
        "currentContent": current_content,
        "oldContent": old_content
    }

@app.websocket("/shell")
async def websocket_shell(websocket: WebSocket, project: str = None):
    """WebSocket Shell ç«¯ç‚¹"""
    try:
        # è·å–é¡¹ç›®è·¯å¾„
        project_path = None
        if project:
            project_path = get_project_path(project)
            logger.info(f"[Shell] é¡¹ç›®è·¯å¾„: {project_path}")
        else:
            project_path = os.getcwd()
            logger.info(f"[Shell] ä½¿ç”¨å½“å‰ç›®å½•: {project_path}")

        # åˆ›å»º Shell ä¼šè¯
        session = ShellSession(cwd=project_path)
        await session.start(websocket)
    except Exception as e:
        logger.exception(f"[Shell] WebSocket ç«¯ç‚¹é”™è¯¯: {e}")
        try:
            await websocket.close(code=1011, reason=str(e))
        except:
            pass

@app.get("/api/user/onboarding-status")
async def onboarding_status(): return {"hasCompletedOnboarding": True}

@app.post("/api/user/complete-onboarding")
async def complete_onboarding(): return {"success": True}

@app.get("/api/projects/{project_name}/sessions")
async def get_sessions(project_name: str, limit: int = 5, offset: int = 0):
    """è·å–é¡¹ç›®çš„ä¼šè¯åˆ—è¡¨"""
    sessions = project_manager.get_sessions(project_name, limit, offset)
    return {
        "sessions": sessions,
        "hasMore": len(sessions) >= limit,
        "total": len(sessions)
    }

@app.put("/api/projects/{project_name}/sessions/{session_id}")
async def update_session_summary(project_name: str, session_id: str, request: Request):
    """æ›´æ–° session çš„è‡ªå®šä¹‰åç§°/æ‘˜è¦"""
    try:
        data = await request.json()
        summary = data.get("summary")

        if not summary:
            return JSONResponse(content={"error": "Summary is required"}, status_code=400)

        project_manager.update_session_summary(project_name, session_id, summary)

        return {"success": True, "summary": summary}
    except Exception as e:
        logger.error(f"Error updating session summary: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)

@app.get("/api/projects/{project_name}/sessions/{session_id}/messages")
async def get_session_messages(project_name: str, session_id: str, limit: int = None, offset: int = 0):
    """è·å– session çš„æ¶ˆæ¯åˆ—è¡¨"""
    messages = project_manager.get_messages(project_name, session_id)

    # å¦‚æœæŒ‡å®šäº† limitï¼Œåˆ™åˆ†é¡µ
    if limit is not None:
        messages = messages[offset:offset + limit]
    elif offset > 0:
        messages = messages[offset:]

    return {
        "messages": messages,
        "total": len(messages),
        "hasMore": limit is not None and len(messages) >= limit
    }

@app.get("/api/projects/{project_name}/sessions/{session_id}/token-usage")
async def get_token_usage(project_name: str, session_id: str):
    """è·å– session çš„ token ä½¿ç”¨æƒ…å†µï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    try:
        messages = project_manager.get_messages(project_name, session_id)

        # ç®€å•ä¼°ç®—ï¼šå‡è®¾æ¯æ¡æ¶ˆæ¯å¤§çº¦ä½¿ç”¨ä¸€å®šæ•°é‡çš„ token
        # å®é™…åº”ç”¨ä¸­åº”è¯¥ä» AI å“åº”ä¸­è·å–å‡†ç¡®çš„ token è®¡æ•°
        total_messages = len(messages)
        estimated_tokens = total_messages * 100  # ç²—ç•¥ä¼°ç®—

        return {
            "totalMessages": total_messages,
            "estimatedTokens": estimated_tokens,
            "session_id": session_id
        }
    except Exception as e:
        logger.error(f"Error getting token usage: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)

# --- é¡¹ç›®åˆ›å»ºå·¥ä½œæµ API ---

@app.get("/api/validate-path")
async def validate_path(path: str = Query(...)):
    """éªŒè¯è·¯å¾„çŠ¶æ€ï¼ˆç”¨äºå‰ç«¯å®æ—¶åé¦ˆï¼‰"""
    try:
        path = os.path.expanduser(path.strip())
        path = os.path.abspath(path)
        
        exists = os.path.exists(path)
        is_dir = os.path.isdir(path) if exists else False
        is_empty = False
        is_git = False
        
        if is_dir:
            try:
                is_empty = not any(os.scandir(path))
                is_git = os.path.isdir(os.path.join(path, ".git"))
            except PermissionError:
                pass
                
        parent_exists = os.path.exists(os.path.dirname(path))
        
        return {
            "exists": exists,
            "isDirectory": is_dir,
            "isEmpty": is_empty,
            "isGit": is_git,
            "parentExists": parent_exists,
            "path": path
        }
    except Exception as e:
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/api/create-workspace")
async def create_workspace(req: CreateWorkspaceRequest):
    """åˆ›å»ºæˆ–æ·»åŠ å·¥ä½œç©ºé—´"""
    logger.info(f"=== åˆ›å»ºå·¥ä½œç©ºé—´è¯·æ±‚ ===")
    logger.info(f"  ç±»å‹: {req.workspaceType}")
    logger.info(f"  è·¯å¾„: {req.path}")
    logger.info(f"  GitHub URL: {req.githubUrl}")
    
    try:
        # è§„èŒƒåŒ–è·¯å¾„
        workspace_path = os.path.expanduser(req.path.strip())
        workspace_path = os.path.abspath(workspace_path)

        # éªŒè¯è·¯å¾„å®‰å…¨æ€§
        is_valid, error, normalized_path = PathValidator.validate_project_path(
            workspace_path,
            must_exist=(req.workspaceType == 'existing')
        )

        # å¦‚æœéªŒè¯å¤±è´¥æ˜¯å› ä¸ºä¸åœ¨å…è®¸çš„æ ¹ç›®å½•èŒƒå›´å†…ï¼ŒåŠ¨æ€æ·»åŠ è¯¥è·¯å¾„
        if not is_valid and "ä¸åœ¨å…è®¸çš„æ ¹ç›®å½•èŒƒå›´å†…" in error:
            logger.info(f"è·¯å¾„ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ŒåŠ¨æ€æ·»åŠ : {workspace_path}")
            # æ·»åŠ è¯¥è·¯å¾„åŠå…¶çˆ¶ç›®å½•åˆ°å…è®¸åˆ—è¡¨
            PathValidator.add_allowed_root(workspace_path)
            PathValidator.add_allowed_root(os.path.dirname(workspace_path))

            # é‡æ–°éªŒè¯
            is_valid, error, normalized_path = PathValidator.validate_project_path(
                workspace_path,
                must_exist=(req.workspaceType == 'existing')
            )

        if req.workspaceType == 'existing':
            # å·²æœ‰å·¥ä½œç©ºé—´ - éœ€è¦è·¯å¾„å­˜åœ¨
            if not is_valid:
                logger.error(f"è·¯å¾„éªŒè¯å¤±è´¥: {error}")
                return JSONResponse(
                    content={"error": f"æ— æ•ˆçš„å·¥ä½œç©ºé—´è·¯å¾„: {error}"},
                    status_code=400
                )

            if not os.path.isdir(normalized_path):
                return JSONResponse(
                    content={"error": "æŒ‡å®šçš„è·¯å¾„ä¸æ˜¯ä¸€ä¸ªç›®å½•"},
                    status_code=400
                )
        else:
            # æ–°å»ºå·¥ä½œç©ºé—´
            parent_dir = os.path.dirname(workspace_path)
            
            # æ£€æŸ¥çˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨
            if not os.path.exists(parent_dir):
                try:
                    os.makedirs(parent_dir, exist_ok=True)
                    logger.info(f"åˆ›å»ºçˆ¶ç›®å½•: {parent_dir}")
                except Exception as e:
                    return JSONResponse(
                        content={"error": f"æ— æ³•åˆ›å»ºçˆ¶ç›®å½•: {e}"},
                        status_code=400
                    )
            
            if req.githubUrl:
                # ä» GitHub å…‹éš†
                logger.info(f"ä» GitHub å…‹éš†: {req.githubUrl}")
                
                # æ„å»º git clone å‘½ä»¤
                clone_url = req.githubUrl.strip()
                
                # å¦‚æœæä¾›äº† tokenï¼Œä¿®æ”¹ URL ä»¥åŒ…å«è®¤è¯ä¿¡æ¯
                if req.newGithubToken:
                    # è§£æ GitHub URL å¹¶æ³¨å…¥ token
                    if clone_url.startswith("https://github.com/"):
                        clone_url = clone_url.replace(
                            "https://github.com/",
                            f"https://{req.newGithubToken}@github.com/"
                        )
                    elif clone_url.startswith("https://"):
                        # é€šç”¨ HTTPS URL
                        clone_url = clone_url.replace(
                            "https://",
                            f"https://{req.newGithubToken}@"
                        )
                
                try:
                    # æ‰§è¡Œ git clone
                    result = subprocess.run(
                        ["git", "clone", clone_url, workspace_path],
                        capture_output=True,
                        text=True,
                        timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
                        cwd=parent_dir
                    )
                    
                    if result.returncode != 0:
                        error_msg = result.stderr or result.stdout or "å…‹éš†å¤±è´¥"
                        # æ¸…ç†é”™è¯¯ä¿¡æ¯ä¸­å¯èƒ½åŒ…å«çš„ token
                        if req.newGithubToken:
                            error_msg = error_msg.replace(req.newGithubToken, "***")
                        logger.error(f"Git clone å¤±è´¥: {error_msg}")
                        return JSONResponse(
                            content={"error": f"Git clone å¤±è´¥: {error_msg}"},
                            status_code=400
                        )
                    
                    logger.info(f"æˆåŠŸå…‹éš†ä»“åº“åˆ°: {workspace_path}")
                    
                except subprocess.TimeoutExpired:
                    return JSONResponse(
                        content={"error": "å…‹éš†è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä»“åº“å¤§å°"},
                        status_code=408
                    )
                except FileNotFoundError:
                    return JSONResponse(
                        content={"error": "Git æœªå®‰è£…æˆ–ä¸åœ¨ç³»ç»Ÿ PATH ä¸­"},
                        status_code=500
                    )
                except Exception as e:
                    logger.exception(f"Git clone å¼‚å¸¸: {e}")
                    return JSONResponse(
                        content={"error": f"å…‹éš†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"},
                        status_code=500
                    )
            else:
                # åˆ›å»ºç©ºç›®å½•
                if not os.path.exists(workspace_path):
                    os.makedirs(workspace_path, exist_ok=True)
                    logger.info(f"åˆ›å»ºç©ºå·¥ä½œç©ºé—´: {workspace_path}")
                elif os.listdir(workspace_path):
                    return JSONResponse(
                        content={"error": "ç›®å½•å·²å­˜åœ¨ä¸”ä¸ä¸ºç©º"},
                        status_code=400
                    )
            
            normalized_path = workspace_path
        
        # å°†é¡¹ç›®æ·»åŠ åˆ°é¡¹ç›®ç®¡ç†å™¨
        project = project_manager.add_project(normalized_path)
        
        # æ³¨å†Œåˆ°è·¯å¾„éªŒè¯å™¨
        project_registry.register_project(project["name"], normalized_path)
        
        logger.info(f"å·¥ä½œç©ºé—´åˆ›å»ºæˆåŠŸ: {project}")
        
        return {
            "success": True,
            "project": project,
            "message": f"{'å·²æ·»åŠ ' if req.workspaceType == 'existing' else 'å·²åˆ›å»º'}å·¥ä½œç©ºé—´: {project['displayName']}"
        }
        
    except Exception as e:
        logger.exception(f"åˆ›å»ºå·¥ä½œç©ºé—´å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"åˆ›å»ºå·¥ä½œç©ºé—´å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/browse-filesystem")
async def browse_filesystem(path: str = Query(None), include_files: bool = Query(False), limit: int = Query(100)):
    """æµè§ˆæ–‡ä»¶ç³»ç»Ÿï¼Œæä¾›è·¯å¾„è‡ªåŠ¨è¡¥å…¨å»ºè®®"""
    try:
        suggestions = []
        
        # å¤„ç†è™šæ‹Ÿæ ¹è·¯å¾„è¯·æ±‚
        if path == "__ROOT__":
            base_dirs = []
            
            # 1. å§‹ç»ˆæ·»åŠ ç”¨æˆ·ä¸»ç›®å½•
            home_dir = os.path.expanduser("~")
            base_dirs.append({
                "name": "Home ğŸ ", 
                "path": home_dir, 
                "type": "directory"
            })
            
            # 2. æ ¹æ®ç³»ç»Ÿæ·»åŠ æ ¹èŠ‚ç‚¹
            if platform.system() == "Windows":
                # Windows: æ·»åŠ æ‰€æœ‰é€»è¾‘é©±åŠ¨å™¨
                import string
                try:
                    # å°è¯•ä½¿ç”¨ ctypes è·å–é©±åŠ¨å™¨ï¼ˆæ›´å‡†ç¡®ï¼‰
                    from ctypes import windll
                    drives = []
                    bitmask = windll.kernel32.GetLogicalDrives()
                    for letter in string.ascii_uppercase:
                        if bitmask & 1:
                            drives.append(f"{letter}:\\")
                        bitmask >>= 1
                except:
                    # å›é€€åˆ°ç®€å•çš„å­˜åœ¨æ€§æ£€æŸ¥
                    drives = [f"{d}:\\" for d in string.ascii_uppercase if os.path.exists(f"{d}:\\")]
                
                for drive in drives:
                    base_dirs.append({
                        "name": f"Local Disk ({drive})",
                        "path": drive,
                        "type": "directory"
                    })
            else:
                # Unix/Mac: æ·»åŠ ç³»ç»Ÿæ ¹ç›®å½•
                base_dirs.append({
                    "name": "System Root (/)",
                    "path": "/",
                    "type": "directory"
                })
                
                # Mac ç‰¹æœ‰: /Volumes
                if platform.system() == "Darwin" and os.path.exists("/Volumes"):
                     base_dirs.append({
                        "name": "Volumes",
                        "path": "/Volumes",
                        "type": "directory"
                    })

            return {"suggestions": base_dirs, "currentPath": "__ROOT__"}

        # å¦‚æœæ²¡æœ‰æä¾›è·¯å¾„ï¼Œé»˜è®¤è¿”å›ç”¨æˆ·ä¸»ç›®å½•ä¿¡æ¯ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        if not path or path == "~":
            home_dir = os.path.expanduser("~")
            return {"suggestions": [], "currentPath": home_dir} # ç®€åŒ–ï¼Œä¸å†åœ¨æ­¤å¤„è¿”å›é©±åŠ¨å™¨åˆ—è¡¨ï¼Œç”± __ROOT__ æ¥ç®¡
        
        # å±•å¼€ ~ ç¬¦å·
        expanded_path = os.path.expanduser(path)
        
        # ç¡®å®šè¦æµè§ˆçš„ç›®å½•
        if os.path.isdir(expanded_path):
            browse_dir = expanded_path
            prefix = ""
        else:
            browse_dir = os.path.dirname(expanded_path)
            prefix = os.path.basename(expanded_path).lower()
        
        if not os.path.isdir(browse_dir):
            return {"suggestions": [], "currentPath": path, "error": "ç›®å½•ä¸å­˜åœ¨"}
        
        # åˆ—å‡ºç›®å½•å†…å®¹
        try:
            entries = os.listdir(browse_dir)
        except PermissionError:
            return {"suggestions": [], "currentPath": path, "error": "æƒé™ä¸è¶³"}
        
        for entry in entries:
            # è·³è¿‡éšè—æ–‡ä»¶ï¼ˆé™¤éç”¨æˆ·æ˜ç¡®è¾“å…¥äº†ç‚¹å·å¼€å¤´ï¼‰
            if entry.startswith('.') and not prefix.startswith('.'):
                continue
            
            # å‰ç¼€è¿‡æ»¤
            if prefix and not entry.lower().startswith(prefix):
                continue
            
            full_path = os.path.join(browse_dir, entry)
            is_dir = os.path.isdir(full_path)
            
            # Filter based on include_files
            if not include_files and not is_dir:
                continue
            
            suggestions.append({
                "name": entry,
                "path": full_path,
                "type": "directory" if is_dir else "file"
            })
        
        # Sort: directories first, then alphabetical
        suggestions.sort(key=lambda x: (0 if x["type"] == "directory" else 1, x["name"].lower()))
        
        # Limit results
        if limit > 0:
            suggestions = suggestions[:limit]
        
        return {
            "suggestions": suggestions,
            "currentPath": expanded_path
        }
        
    except Exception as e:
        logger.error(f"æµè§ˆæ–‡ä»¶ç³»ç»Ÿå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æµè§ˆæ–‡ä»¶ç³»ç»Ÿå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/search-filesystem")
async def search_filesystem(q: str = Query(...), path: str = Query(None), limit: int = Query(50)):
    """Search for directories matching query"""
    start_dir = os.path.expanduser(path or "~")
    
    def _search():
        results = []
        if not os.path.exists(start_dir):
            return results
        
        try:
            count = 0
            # Limit depth effectively by not following hidden/large dirs
            exclude_dirs = {'node_modules', 'Library', 'venv', '__pycache__', '.git', '.idea', '.vscode'}
            
            for root, dirs, files in os.walk(start_dir):
                # Filter in-place to prevent recursion
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in exclude_dirs]
                
                for d in dirs:
                    if q.lower() in d.lower():
                        full_path = os.path.join(root, d)
                        results.append({
                            "name": d,
                            "path": full_path,
                            "type": "directory"
                        })
                        count += 1
                        if count >= limit:
                            return results
                
                if len(results) >= limit:
                    break
        except Exception as e:
            logger.error(f"Search error: {e}")
            pass
        return results

    results = await asyncio.to_thread(_search)
    return {"results": results}


@app.post("/api/projects/create")
async def create_project(req: CreateProjectRequest):
    """åˆ›å»ºé¡¹ç›®ï¼ˆç®€å•ç‰ˆæœ¬ - ä»…æ·»åŠ ç°æœ‰è·¯å¾„ï¼‰"""
    try:
        workspace_path = os.path.expanduser(req.path.strip())
        workspace_path = os.path.abspath(workspace_path)

        if not os.path.isdir(workspace_path):
            return JSONResponse(
                content={"error": "æŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•"},
                status_code=400
            )

        # å°è¯•æ³¨å†Œé¡¹ç›®ï¼Œå¦‚æœå¤±è´¥åˆ™åŠ¨æ€æ·»åŠ è·¯å¾„
        project = project_manager.add_project(workspace_path)

        # æ³¨å†Œåˆ°é¡¹ç›®æ³¨å†Œè¡¨
        is_registered, error = project_registry.register_project(project["name"], workspace_path)
        if not is_registered:
            # å¦‚æœæ³¨å†Œå¤±è´¥æ˜¯å› ä¸ºè·¯å¾„ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ŒåŠ¨æ€æ·»åŠ 
            if "ä¸åœ¨å…è®¸çš„æ ¹ç›®å½•èŒƒå›´å†…" in error:
                logger.info(f"è·¯å¾„ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ŒåŠ¨æ€æ·»åŠ : {workspace_path}")
                PathValidator.add_allowed_root(workspace_path)
                PathValidator.add_allowed_root(os.path.dirname(workspace_path))

                # é‡æ–°æ³¨å†Œ
                is_registered, error = project_registry.register_project(project["name"], workspace_path)

            if not is_registered:
                logger.error(f"æ³¨å†Œé¡¹ç›®å¤±è´¥: {error}")
                return JSONResponse(
                    content={"error": f"æ³¨å†Œé¡¹ç›®å¤±è´¥: {error}"},
                    status_code=400
                )

        return {"success": True, "project": project}

    except Exception as e:
        logger.exception(f"åˆ›å»ºé¡¹ç›®å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"åˆ›å»ºé¡¹ç›®å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/projects/create-workspace")
async def create_workspace(req: CreateWorkspaceRequest):
    """åˆ›å»ºå·¥ä½œç©ºé—´ï¼ˆå®Œæ•´æµç¨‹ï¼‰"""
    try:
        workspace_path = os.path.expanduser(req.path.strip())
        workspace_path = os.path.abspath(workspace_path)
        
        # 1. éªŒè¯æˆ–åˆ›å»ºç›®å½•
        if req.workspaceType == 'new':
            if os.path.exists(workspace_path):
                if not os.path.isdir(workspace_path):
                    return JSONResponse(content={"error": "è·¯å¾„å­˜åœ¨ä¸”ä¸æ˜¯ç›®å½•"}, status_code=400)
                # å…è®¸åœ¨ç©ºç›®å½•ä¸­åˆ›å»ºï¼ˆæˆ–éç©ºç›®å½•ä½†ç”¨æˆ·å·²ç¡®è®¤ï¼‰
            else:
                try:
                    os.makedirs(workspace_path, exist_ok=True)
                except Exception as e:
                    return JSONResponse(content={"error": f"æ— æ³•åˆ›å»ºç›®å½•: {str(e)}"}, status_code=500)
                
            # 2. å¤„ç† GitHub å…‹éš†
            if req.githubUrl:
                repo_url = req.githubUrl
                if req.newGithubToken:
                    # æ’å…¥ token: https://TOKEN@github.com/...
                    if repo_url.startswith("https://"):
                        repo_url = repo_url.replace("https://", f"https://{req.newGithubToken}@")
                
                try:
                    # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©º
                    if os.path.exists(workspace_path) and any(os.scandir(workspace_path)):
                         return JSONResponse(content={"error": "ç›®æ ‡ç›®å½•éç©ºï¼Œæ— æ³•å…‹éš†ä»“åº“"}, status_code=400)

                    process = await asyncio.create_subprocess_exec(
                        "git", "clone", repo_url, ".",
                        cwd=workspace_path,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    stdout, stderr = await process.communicate()
                    if process.returncode != 0:
                        err_msg = stderr.decode()
                        # ç®€å•éšè— token
                        if req.newGithubToken:
                            err_msg = err_msg.replace(req.newGithubToken, "***")
                        return JSONResponse(content={"error": f"å…‹éš†å¤±è´¥: {err_msg}"}, status_code=400)
                except Exception as e:
                    return JSONResponse(content={"error": f"Gitæ“ä½œå¤±è´¥: {str(e)}"}, status_code=500)

        elif req.workspaceType == 'existing':
            if not os.path.isdir(workspace_path):
                return JSONResponse(content={"error": "è·¯å¾„ä¸å­˜åœ¨"}, status_code=400)

        # 3. æ³¨å†Œé¡¹ç›®
        # ä½¿ç”¨ project_manager æ·»åŠ 
        project = project_manager.add_project(workspace_path)
        
        # æ³¨å†Œåˆ° registry (ä¸ºäº†å…è®¸è®¿é—®)
        is_registered, error = project_registry.register_project(project["name"], workspace_path)
        
        if not is_registered:
             if "ä¸åœ¨å…è®¸çš„æ ¹ç›®å½•èŒƒå›´å†…" in error:
                logger.info(f"è·¯å¾„ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ŒåŠ¨æ€æ·»åŠ : {workspace_path}")
                PathValidator.add_allowed_root(workspace_path)
                PathValidator.add_allowed_root(os.path.dirname(workspace_path))
                is_registered, error = project_registry.register_project(project["name"], workspace_path)
        
        if not is_registered:
             return JSONResponse(content={"error": f"æ³¨å†Œé¡¹ç›®å¤±è´¥: {error}"}, status_code=400)

        return {"success": True, "project": project}

    except Exception as e:
        logger.exception(f"åˆ›å»ºå·¥ä½œç©ºé—´å¤±è´¥: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/api/error-analyze")
async def analyze_error(request: Request):
    """åˆ†æé”™è¯¯å¹¶æä¾›ä¿®å¤å»ºè®®"""
    try:
        data = await request.json()
        error_output = data.get('error', '')
        project_path = data.get('projectPath', '')

        if not error_output:
            return JSONResponse(
                content={"error": "é”™è¯¯è¾“å‡ºä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # è·å–é”™è¯¯åˆ†æå™¨
        analyzer = get_error_analyzer(project_path) if project_path else ErrorAnalyzer('.')

        # åˆ†æé”™è¯¯
        analysis = analyzer.analyze_error(error_output, project_path)

        # è·å–ä»£ç ä¸Šä¸‹æ–‡
        if analysis['error_info']['file'] and analysis['error_info']['line']:
            context = analyzer.get_error_context(
                analysis['error_info']['file'],
                analysis['error_info']['line']
            )
            analysis['code_context'] = context

        # ç”Ÿæˆè‡ªåŠ¨ä¿®å¤æ–¹æ¡ˆ
        if analysis['can_auto_fix']:
            auto_fix = analyzer.generate_auto_fix(error_output, project_path)
            analysis['auto_fix'] = auto_fix

        return {
            "success": True,
            "analysis": analysis
        }

    except Exception as e:
        logger.exception(f"é”™è¯¯åˆ†æå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"é”™è¯¯åˆ†æå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/auto-fix")
async def auto_fix_error(request: Request):
    """è‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤é”™è¯¯"""
    try:
        data = await request.json()
        error_output = data.get('error', '')
        project_path = data.get('projectPath', '')
        context = data.get('context', {})

        if not error_output:
            return JSONResponse(
                content={"error": "é”™è¯¯è¾“å‡ºä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        logger.info(f"å¼€å§‹è‡ªåŠ¨ä¿®å¤: {error_output[:100]}...")

        # è·å– Agent å®ä¾‹
        agent = get_agent(
            project_path,
            global_config["mode"],
            global_config.get("model"),
            global_config.get("mcp_servers")
        )

        # è·å–è‡ªåŠ¨ä¿®å¤å™¨
        auto_fixer = get_auto_fixer(project_path, agent)

        # æ‰§è¡Œè‡ªåŠ¨ä¿®å¤
        result = await auto_fixer.detect_and_fix(error_output, context)

        logger.info(f"è‡ªåŠ¨ä¿®å¤ç»“æœ: {result}")

        return {
            "success": True,
            "result": result
        }

    except Exception as e:
        logger.exception(f"è‡ªåŠ¨ä¿®å¤å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è‡ªåŠ¨ä¿®å¤å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/auto-fix/history")
async def get_auto_fix_history(projectPath: str = Query(..., description="é¡¹ç›®è·¯å¾„")):
    """è·å–è‡ªåŠ¨ä¿®å¤å†å²"""
    try:
        auto_fixer = get_auto_fixer(projectPath)
        history = auto_fixer.get_fix_history()

        return {
            "success": True,
            "history": history
        }

    except Exception as e:
        logger.exception(f"è·å–ä¿®å¤å†å²å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è·å–ä¿®å¤å†å²å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.delete("/api/auto-fix/history")
async def clear_auto_fix_history(projectPath: str = Query(..., description="é¡¹ç›®è·¯å¾„")):
    """æ¸…ç©ºè‡ªåŠ¨ä¿®å¤å†å²"""
    try:
        auto_fixer = get_auto_fixer(projectPath)
        auto_fixer.clear_history()

        return {
            "success": True,
            "message": "ä¿®å¤å†å²å·²æ¸…ç©º"
        }

    except Exception as e:
        logger.exception(f"æ¸…ç©ºä¿®å¤å†å²å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ¸…ç©ºä¿®å¤å†å²å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/context/analyze")
async def analyze_context(request: Request):
    """åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡ï¼ˆä¾èµ–å…³ç³»ã€è°ƒç”¨å…³ç³»ã€ç±»ç»§æ‰¿ï¼‰"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')
        include_dirs = data.get('includeDirs', [])

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        logger.info(f"å¼€å§‹åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡: {project_path}")

        # è·å–ä¾èµ–åˆ†æå™¨
        analyzer = get_dependency_analyzer(project_path)

        # åˆ†æé¡¹ç›®
        result = analyzer.analyze_project(include_dirs)

        logger.info(f"é¡¹ç›®ä¸Šä¸‹æ–‡åˆ†æå®Œæˆ: {len(result['modules'])} ä¸ªæ¨¡å—")

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        logger.exception(f"åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/context/module/{module_name}")
async def get_module_context(module_name: str, projectPath: str = Query(..., description="é¡¹ç›®è·¯å¾„")):
    """è·å–ç‰¹å®šæ¨¡å—çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    try:
        analyzer = get_dependency_analyzer(projectPath)

        if module_name not in analyzer.modules:
            return JSONResponse(
                content={"error": f"æ¨¡å— {module_name} ä¸å­˜åœ¨"},
                status_code=404
            )

        module = analyzer.modules[module_name]

        # è·å–ä¾èµ–çš„æ¨¡å—
        dependencies = set(module.imports)
        for from_module in module.from_imports.keys():
            dependencies.add(from_module)

        # è·å–è¢«ä¾èµ–çš„æ¨¡å—
        dependents = []
        for other_module_name, other_module in analyzer.modules.items():
            if module_name in other_module.imports or module_name in other_module.from_imports:
                dependents.append(other_module_name)

        return {
            "success": True,
            "module": {
                "name": module_name,
                "file_path": module.file_path,
                "imports": list(module.imports),
                "from_imports": {k: list(v) for k, v in module.from_imports.items()},
                "functions": {
                    func_name: {
                        "line": func_info.line_start,
                        "parameters": func_info.parameters,
                        "calls": list(func_info.calls),
                        "is_async": func_info.is_async,
                        "is_method": func_info.is_method,
                        "class_name": func_info.class_name
                    }
                    for func_name, func_info in module.functions.items()
                },
                "classes": {
                    class_name: {
                        "line": class_info.line_start,
                        "bases": class_info.bases,
                        "methods": list(class_info.methods.keys()),
                        "attributes": list(class_info.attributes)
                    }
                    for class_name, class_info in module.classes.items()
                }
            },
            "dependencies": list(dependencies),
            "dependents": dependents
        }

    except Exception as e:
        logger.exception(f"è·å–æ¨¡å—ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è·å–æ¨¡å—ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/code-style-analyze")
async def analyze_code_style(request: Request):
    """åˆ†æé¡¹ç›®ä»£ç é£æ ¼"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # è·å–ä»£ç é£æ ¼åˆ†æå™¨
        analyzer = get_code_style_analyzer(project_path)

        # åˆ†æä»£ç é£æ ¼
        style_profile = analyzer.analyze_project_style()
        style_summary = analyzer.get_style_summary()

        return {
            "success": True,
            "styleProfile": style_profile,
            "summary": style_summary
        }

    except Exception as e:
        logger.exception(f"ä»£ç é£æ ¼åˆ†æå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"ä»£ç é£æ ¼åˆ†æå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/prompt-optimize")
async def optimize_prompt(request: Request):
    """æ ¹æ®é¡¹ç›®ç‰¹å¾æ™ºèƒ½ä¼˜åŒ–ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯ï¼ˆä½¿ç”¨å¤§æ¨¡å‹ï¼‰"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')
        user_input = data.get('userInput', '')
        base_persona = data.get('persona', 'partner')

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        if not user_input:
            return JSONResponse(
                content={"error": "ç”¨æˆ·è¾“å…¥ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        logger.info(f"å¼€å§‹æ™ºèƒ½ä¼˜åŒ–æ¶ˆæ¯: project={project_path}, persona={base_persona}, input={user_input[:100]}...")

        # è·å–æç¤ºè¯ä¼˜åŒ–å™¨
        optimizer = get_prompt_optimizer(project_path)

        # å…ˆåˆ†æé¡¹ç›®ï¼ˆè¿™ä¼šæ‰«æé¡¹ç›®ä»£ç ï¼‰
        analysis = optimizer.analyze_project()

        # åˆ†æç”¨æˆ·æ„å›¾
        intent = optimizer.analyze_user_intent(user_input)

        # æŸ¥æ‰¾ç›¸å…³ä»£ç 
        relevant_code = optimizer.find_relevant_code(user_input, intent)

        # æ„å»ºé¡¹ç›®ä¸Šä¸‹æ–‡
        project_context = optimizer._build_project_context()
        style_guide = optimizer._build_style_guide()

        # æ„å»ºä¼˜åŒ–æç¤ºè¯
        optimization_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æç¤ºè¯ä¼˜åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œä¼˜åŒ–ç”¨æˆ·çš„è¾“å…¥æ¶ˆæ¯ï¼Œä½¿å…¶æ›´å…·ä½“ã€æ›´ç¬¦åˆé¡¹ç›®çš„å®é™…æƒ…å†µã€‚

## é¡¹ç›®ä¿¡æ¯
{project_context}

## ä»£ç é£æ ¼æŒ‡å—
{style_guide}

## ç”¨æˆ·æ„å›¾
- æ„å›¾ç±»å‹: {intent.get('type', 'unknown')}
- å…³é”®è¯: {', '.join(intent.get('keywords', []))}
- å®ä½“: {', '.join(intent.get('entities', []))}

## ç›¸å…³ä»£ç 
"""
        if relevant_code:
            for code in relevant_code:
                if code['type'] == 'function':
                    optimization_prompt += f"- å‡½æ•°: {code['name']} (åœ¨ {code['file']})"
                else:
                    optimization_prompt += f"- ç±»: {code['name']} (åœ¨ {code['file']})"
        else:
            optimization_prompt += "- æ— ç›¸å…³ä»£ç "

        optimization_prompt += f"""

## ç”¨æˆ·åŸå§‹è¾“å…¥
{user_input}

## ä»»åŠ¡
è¯·ä¼˜åŒ–ç”¨æˆ·çš„è¾“å…¥æ¶ˆæ¯ï¼Œä½¿å…¶ï¼š
1. åŒ…å«é¡¹ç›®èƒŒæ™¯ä¿¡æ¯
2. å¼•ç”¨ç›¸å…³çš„ä»£ç ï¼ˆå¦‚æœæœ‰ï¼‰
3. æ˜ç¡®ä»£ç é£æ ¼è¦æ±‚
4. æ ¹æ®æ„å›¾ç±»å‹æ·»åŠ å…·ä½“è¦æ±‚
5. è®© AI èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£é¡¹ç›®ä¸Šä¸‹æ–‡å¹¶æä¾›å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆ

è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„æ¶ˆæ¯ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—ã€‚"""

        logger.info("è°ƒç”¨å¤§æ¨¡å‹ä¼˜åŒ–æ¶ˆæ¯...")

        # åˆ›å»º iFlow å®¢æˆ·ç«¯
        from backend.core.iflow_client import create_iflow_client
        iflow_client = create_iflow_client(
            cwd=project_path,
            mode=global_config.get("mode", "yolo"),
            model=global_config.get("model", "GLM-4.7")
        )

        # è°ƒç”¨å¤§æ¨¡å‹
        optimized_message = ""
        async for chunk in iflow_client.chat_stream(optimization_prompt):
            optimized_message += chunk

        optimized_message = optimized_message.strip()

        logger.info(f"å¤§æ¨¡å‹ä¼˜åŒ–å®Œæˆï¼Œæ¶ˆæ¯é•¿åº¦: {len(optimized_message)}")

        return {
            "success": True,
            "analysis": analysis,
            "intent": intent,
            "relevantCode": relevant_code,
            "originalInput": user_input,
            "optimizedMessage": optimized_message,
            "projectContext": project_context,
            "codeStyleGuide": style_guide
        }

    except Exception as e:
        logger.exception(f"æ™ºèƒ½æ¶ˆæ¯ä¼˜åŒ–å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ™ºèƒ½æ¶ˆæ¯ä¼˜åŒ–å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/generate-report")
async def generate_report(req: dict):
    """ç”Ÿæˆå·¥ä½œæŠ¥å‘Š"""
    try:
        project_path = req.get("projectPath")
        report_type = req.get("type", "daily")
        
        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )
        
        expanded_path = os.path.expanduser(project_path)
        
        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(expanded_path)
        if not is_valid:
            return JSONResponse(
                content={"error": f"æ— æ•ˆçš„é¡¹ç›®è·¯å¾„: {error}"},
                status_code=400
            )
        
        analyzer = get_report_generator()
        report = analyzer.generate_report(normalized, report_type)
        
        return {"success": True, "report": report}
        
    except Exception as e:
        logger.exception(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/context-analyze")
async def analyze_context(req: dict):
    """åˆ†æä»£ç ä¸Šä¸‹æ–‡å’Œä¾èµ–å…³ç³»"""
    try:
        project_path = req.get("projectPath")
        node_id = req.get("nodeId")
        max_depth = req.get("maxDepth", 2)
        
        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )
        
        expanded_path = os.path.expanduser(project_path)
        
        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(expanded_path)
        if not is_valid:
            return JSONResponse(
                content={"error": f"æ— æ•ˆçš„é¡¹ç›®è·¯å¾„: {error}"},
                status_code=400
            )
        
        analyzer = get_dependency_analyzer()
        
        # åˆ†æé¡¹ç›®
        analyzer.analyze_project(normalized)
        
        # å¦‚æœæŒ‡å®šäº†èŠ‚ç‚¹ IDï¼Œè·å–ä¸Šä¸‹æ–‡å›¾è°±
        if node_id:
            context_graph = analyzer.get_context_graph(node_id, max_depth)
            return {"success": True, "graph": context_graph}
        
        # å¦åˆ™è¿”å›æ‰€æœ‰èŠ‚ç‚¹åˆ—è¡¨
        nodes = []
        for node_id, node in analyzer.nodes.items():
            nodes.append({
                'id': node_id,
                'name': node.name,
                'type': node.type.value,
                'file': os.path.basename(node.file_path),
                'line': node.line_number,
                'full_path': node.file_path
            })
        
        return {"success": True, "nodes": nodes[:100]}  # é™åˆ¶è¿”å›æ•°é‡
        
    except Exception as e:
        logger.exception(f"åˆ†æä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"åˆ†æä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/context-search")
async def search_context(req: dict):
    """æœç´¢ä»£ç èŠ‚ç‚¹"""
    try:
        project_path = req.get("projectPath")
        query = req.get("query")
        limit = req.get("limit", 20)
        
        if not project_path or not query:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„å’ŒæŸ¥è¯¢è¯ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )
        
        expanded_path = os.path.expanduser(project_path)
        
        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(expanded_path)
        if not is_valid:
            return JSONResponse(
                content={"error": f"æ— æ•ˆçš„é¡¹ç›®è·¯å¾„: {error}"},
                status_code=400
            )
        
        analyzer = get_dependency_analyzer()
        analyzer.analyze_project(normalized)
        
        results = analyzer.search_nodes(query, limit)
        
        return {"success": True, "results": results}

    except Exception as e:
        logger.exception(f"æœç´¢ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æœç´¢ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/query")
async def simple_query(req: dict):
    """ç®€å•çš„åŒæ­¥æŸ¥è¯¢ API - å¿«é€Ÿè·å– AI å“åº”"""
    try:
        from backend.core.iflow_client import query_sync

        prompt = req.get("prompt")
        project = req.get("project")
        model = req.get("model")
        system_prompt = req.get("system_prompt")
        timeout = req.get("timeout", 300.0)

        if not prompt:
            return JSONResponse(
                content={"error": "prompt ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # è·å–é¡¹ç›®è·¯å¾„
        cwd = None
        if project:
            cwd = get_project_path(project)

        # æ‰§è¡ŒæŸ¥è¯¢
        response = query_sync(
            prompt=prompt,
            cwd=cwd,
            model=model,
            system_prompt=system_prompt,
            timeout=timeout
        )

        return {"success": True, "response": response}

    except Exception as e:
        logger.exception(f"ç®€å•æŸ¥è¯¢å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æŸ¥è¯¢å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/mcp/config/read")
async def get_mcp_config():
    """è¯»å– MCP é…ç½®"""
    try:
        iflow_config_path = os.path.expanduser("~/.iflow/settings.json")
        
        if not os.path.exists(iflow_config_path):
            return {"success": False, "error": "iFlow é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(iflow_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        mcp_servers = config.get("mcpServers", {})
        
        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        servers = []
        for name, server_config in mcp_servers.items():
            servers.append({
                "id": name,
                "name": name,
                "type": server_config.get("type", "stdio"),
                "scope": "user",
                "config": server_config,
                "created": datetime.now().isoformat(),
                "updated": datetime.now().isoformat()
            })
        
        return {"success": True, "servers": servers}
    except Exception as e:
        logger.exception(f"è¯»å– MCP é…ç½®å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/mcp/cli/list")
async def list_mcp_cli():
    """é€šè¿‡ CLI åˆ—å‡º MCP æœåŠ¡å™¨"""
    try:
        # å°è¯•é€šè¿‡ iflow mcp list å‘½ä»¤è·å–
        result = subprocess.run(
            ["iflow", "mcp", "list"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            # è§£æ CLI è¾“å‡º
            servers = []
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¾“å‡ºæ ¼å¼è§£æ
            # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨
            return {"success": True, "servers": servers}
        else:
            return {"success": False, "error": result.stderr}
    except Exception as e:
        logger.warning(f"é€šè¿‡ CLI åˆ—å‡º MCP æœåŠ¡å™¨å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/mcp/servers")
async def get_mcp_servers(scope: str = "user"):
    """è·å– MCP æœåŠ¡å™¨åˆ—è¡¨"""
    try:
        # ä» global_config è·å–
        servers = global_config.get("mcp_servers", [])
        return {"success": True, "servers": servers}
    except Exception as e:
        logger.exception(f"è·å– MCP æœåŠ¡å™¨å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


@app.post("/api/context/analyze-dependencies")
async def analyze_code_dependencies(request: Request):
    """åˆ†æä»£ç ä¾èµ–å…³ç³»å¹¶ç”Ÿæˆå¯è§†åŒ–æ•°æ®"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(project_path)
        if not is_valid:
            return JSONResponse(
                content={"error": error},
                status_code=400
            )

        # è·å–ä¾èµ–åˆ†æå™¨
        analyzer = get_code_dependency_analyzer(normalized)

        # åˆ†æä¾èµ–å…³ç³»
        result = analyzer.analyze_project_dependencies()

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        logger.exception(f"ä»£ç ä¾èµ–åˆ†æå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"ä»£ç ä¾èµ–åˆ†æå¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/context/analyze-module")
async def analyze_module_dependencies(request: Request):
    """åˆ†æç‰¹å®šæ¨¡å—çš„ä¾èµ–å…³ç³»"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')
        module_name = data.get('moduleName', '')

        if not project_path:
            return JSONResponse(
                content={"error": "é¡¹ç›®è·¯å¾„ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        if not module_name:
            return JSONResponse(
                content={"error": "æ¨¡å—åç§°ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )

        # éªŒè¯è·¯å¾„
        is_valid, error, normalized = PathValidator.validate_project_path(project_path)
        if not is_valid:
            return JSONResponse(
                content={"error": error},
                status_code=400
            )

        # è·å–ä¾èµ–åˆ†æå™¨
        analyzer = get_code_dependency_analyzer(normalized)

        # åˆ†ææ¨¡å—ä¾èµ–
        result = analyzer.analyze_module_dependencies(module_name)

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        logger.exception(f"æ¨¡å—ä¾èµ–åˆ†æå¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ¨¡å—ä¾èµ–åˆ†æå¤±è´¥: {str(e)}"},
            status_code=500
        )


# --- TaskMaster API ç«¯ç‚¹ ---

@app.get("/api/taskmaster/installation-status")
async def get_taskmaster_installation_status():
    """è·å– TaskMaster å®‰è£…çŠ¶æ€"""
    return {
        "installation": {"isInstalled": False},
        "isReady": False
    }

@app.get("/api/taskmaster/tasks/{project_name}")
async def get_taskmaster_tasks(project_name: str):
    """è·å–é¡¹ç›®çš„ä»»åŠ¡åˆ—è¡¨"""
    try:
        project_path = get_project_path(project_name)

        # ä» task_master_service è·å–ä»»åŠ¡åˆ—è¡¨
        tasks = task_master_service.get_tasks(project_name)

        # ç»Ÿè®¡ä»»åŠ¡çŠ¶æ€
        total = len(tasks)
        completed = sum(1 for task in tasks if task.get("status") == "completed")

        return {
            "success": True,
            "tasks": tasks,
            "total": total,
            "completed": completed
        }
    except Exception as e:
        logger.exception(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "tasks": [],
            "total": 0,
            "completed": 0
        }

@app.get("/api/taskmaster/prd/{project_name}")
async def get_taskmaster_prd(project_name: str):
    """è·å–é¡¹ç›®çš„ PRD æ–‡æ¡£"""
    try:
        project_path = get_project_path(project_name)

        # å°è¯•æŸ¥æ‰¾ PRD æ–‡ä»¶ï¼ˆå¸¸è§çš„ PRD æ–‡ä»¶åï¼‰
        prd_filenames = [
            "PRD.md",
            "prd.md",
            "PRODUCT_REQUIREMENTS.md",
            "product_requirements.md",
            "REQUIREMENTS.md",
            "requirements.md"
        ]

        prd_content = None
        prd_file = None

        for filename in prd_filenames:
            prd_file_path = os.path.join(project_path, filename)
            if os.path.exists(prd_file_path) and os.path.isfile(prd_file_path):
                try:
                    with open(prd_file_path, 'r', encoding='utf-8') as f:
                        prd_content = f.read()
                    prd_file = filename
                    break
                except Exception as e:
                    logger.warning(f"è¯»å– PRD æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    continue

        return {
            "success": True,
            "prd": prd_content,
            "exists": prd_content is not None,
            "file": prd_file
        }
    except Exception as e:
        logger.exception(f"è·å– PRD æ–‡æ¡£å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "prd": None,
            "exists": False
        }

# --- Cursor Sessions API ç«¯ç‚¹ ---

@app.get("/api/cursor/sessions")
async def get_cursor_sessions(projectPath: str = Query(...)):
    """è·å– Cursor sessions åˆ—è¡¨"""
    # TODO: å®ç° Cursor sessions è¯»å–é€»è¾‘
    # Cursor sessions é€šå¸¸å­˜å‚¨åœ¨ ~/.cursor/sessions/ ç›®å½•ä¸‹
    return {
        "success": True,
        "sessions": []
    }

# --- Commands API ç«¯ç‚¹ ---

@app.post("/api/commands/list")
async def list_commands(request: Request):
    """è·å–å¯ç”¨çš„å‘½ä»¤åˆ—è¡¨"""
    # TODO: å®ç°å‘½ä»¤åˆ—è¡¨è¯»å–é€»è¾‘
    return {
        "commands": []
    }

# --- MCP Utils API ç«¯ç‚¹ ---

@app.get("/api/mcp-utils/taskmaster-server")
async def get_taskmaster_server_status():
    """è·å– TaskMaster MCP æœåŠ¡å™¨çŠ¶æ€"""
    return {
        "status": "not-implemented",
        "message": "TaskMaster MCP server is not implemented"
    }

# --- RAG API ç«¯ç‚¹ ---

@app.get("/api/rag/stats")
async def get_rag_stats(project_path: str = None, project_name: str = None):
    """è·å– RAG ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # ä¼˜å…ˆä½¿ç”¨ project_pathï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ project_name
        if project_path:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„é¡¹ç›®è·¯å¾„
            final_project_path = project_path
        elif project_name:
            # é€šè¿‡é¡¹ç›®åç§°æŸ¥æ‰¾é¡¹ç›®è·¯å¾„
            final_project_path = get_project_path(project_name)
        else:
            return JSONResponse(
                content={"error": "ç¼ºå°‘ project_path æˆ– project_name å‚æ•°"},
                status_code=400
            )

        if final_project_path not in rag_cache:
            rag_cache[final_project_path] = get_rag_service(final_project_path)

        rag_service = rag_cache[final_project_path]
        stats = rag_service.get_stats()

        return {
            "success": True,
            "stats": stats
        }
    except Exception as e:
        logger.exception(f"è·å– RAG ç»Ÿè®¡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è·å– RAG ç»Ÿè®¡å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/rag/status")
async def get_rag_status():
    """è·å– RAG ä¾èµ–çŠ¶æ€"""
    try:
        from backend.core.rag_service import CHROMADB_AVAILABLE, SKLEARN_AVAILABLE, SENTENCE_TRANSFORMERS_AVAILABLE
        
        return {
            "success": True,
            "dependencies": {
                "chromadb": CHROMADB_AVAILABLE,
                "sentence_transformers": SENTENCE_TRANSFORMERS_AVAILABLE,
                "sklearn": SKLEARN_AVAILABLE
            },
            "current_mode": global_config.get("rag_mode", "tfidf"),
            "available_retrievers": []
        }
    except Exception as e:
        logger.exception(f"è·å– RAG çŠ¶æ€å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è·å– RAG çŠ¶æ€å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/index")
async def index_project_rag(request: Request, project_path: str = None, project_name: str = None):
    """ç´¢å¼•é¡¹ç›®æ–‡æ¡£åˆ° RAGï¼ˆæ”¯æŒå¢é‡ç´¢å¼•ï¼‰"""
    try:
        # ä¼˜å…ˆä½¿ç”¨ project_pathï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ project_name
        if project_path:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„é¡¹ç›®è·¯å¾„
            final_project_path = project_path
            logger.info(f"RAG indexing request for project_path: {project_path}")
        elif project_name:
            # é€šè¿‡é¡¹ç›®åç§°æŸ¥æ‰¾é¡¹ç›®è·¯å¾„
            final_project_path = get_project_path(project_name)
            logger.info(f"RAG indexing request for project_name: {project_name}, path: {final_project_path}")
        else:
            return JSONResponse(
                content={"error": "ç¼ºå°‘ project_path æˆ– project_name å‚æ•°"},
                status_code=400
            )

        logger.info(f"RAG indexing request for project: {final_project_path}")
        
        # è§£æè¯·æ±‚å‚æ•°
        try:
            data = await request.json() if request.method == "POST" else {}
            force_reindex = data.get("force_reindex", False)
        except Exception as e:
            logger.warning(f"Failed to parse request JSON: {e}")
            data = {}
            force_reindex = False
        
        # æ£€æŸ¥ä¾èµ–
        from backend.core.rag_service import CHROMADB_AVAILABLE, SKLEARN_AVAILABLE
        
        if not CHROMADB_AVAILABLE and not SKLEARN_AVAILABLE:
            error_msg = "ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“ã€‚è¯·å®‰è£… chromadb æˆ– scikit-learn:\n" \
                        "pip install chromadb sentence-transformers\n" \
                        "æˆ–\n" \
                        "pip install scikit-learn"
            logger.error(error_msg)
            return JSONResponse(
                content={"error": error_msg},
                status_code=500
            )
        
        logger.info(f"Dependencies check: CHROMADB_AVAILABLE={CHROMADB_AVAILABLE}, SKLEARN_AVAILABLE={SKLEARN_AVAILABLE}")
        
        # ç¡®ä¿ RAG æœåŠ¡è¢«åˆ›å»ºï¼ˆä½¿ç”¨é¡¹ç›®è·¯å¾„ä½œä¸ºç¼“å­˜é”®ï¼‰
        if project_path not in rag_cache:
            # æ ¹æ®é…ç½®é€‰æ‹© RAG æ¨¡å¼
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            
            if use_chromadb and not CHROMADB_AVAILABLE:
                logger.warning("ChromaDB requested but not available, falling back to TF-IDF")
                use_chromadb = False
            
            rag_cache[project_path] = get_rag_service(project_path, use_chromadb=use_chromadb)
            logger.info(f"Created new RAG service for {project_name} at {project_path} (mode: {'ChromaDB' if use_chromadb else 'TF-IDF'})")
        
        rag_service = rag_cache[project_path]
        
        # åˆ›å»ºå¼‚æ­¥ç”Ÿæˆå™¨ç”¨äºè¿›åº¦æ›´æ–°
        async def progress_generator():
            try:
                logger.info(f"Starting progress generator for {project_name}")
                async for result in rag_service.index_project(force_reindex=force_reindex):
                    # å‘é€æ‰€æœ‰ç±»å‹çš„ç»“æœ
                    msg = f"data: {json.dumps(result)}\n\n"
                    logger.debug(f"Yielding: {msg.strip()}")
                    yield msg
                    
                    # å®Œæˆåé€€å‡º
                    if result.get("type") == "complete":
                        logger.info(f"Indexing complete for {project_name}")
                        break
                    elif result.get("type") == "error":
                        logger.error(f"Indexing error for {project_name}: {result.get('message')}")
                        break
            except Exception as e:
                logger.exception(f"Progress generator error for {project_name}: {e}")
                error_msg = f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
                yield error_msg
        
        return StreamingResponse(progress_generator(), media_type="text/event-stream")
    
    except Exception as e:
        logger.exception(f"RAG ç´¢å¼•å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"RAG ç´¢å¼•å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/retrieve/{project_name}")
async def retrieve_rag(project_name: str, request: Request):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆæ”¯æŒé«˜çº§æ£€ç´¢é€‰é¡¹ï¼‰"""
    try:
        data = await request.json()
        query = data.get("query", "")
        n_results = data.get("n_results", 5)
        
        # é«˜çº§æ£€ç´¢é€‰é¡¹
        similarity_threshold = data.get("similarity_threshold", 0.0)  # ç›¸ä¼¼åº¦é˜ˆå€¼
        file_types = data.get("file_types", [])  # æ–‡ä»¶ç±»å‹è¿‡æ»¤
        languages = data.get("languages", [])  # ç¼–ç¨‹è¯­è¨€è¿‡æ»¤
        min_chunk_size = data.get("min_chunk_size", 0)  # æœ€å°å—å¤§å°
        max_chunk_size = data.get("max_chunk_size", float('inf'))  # æœ€å¤§å—å¤§å°
        sort_by = data.get("sort_by", "similarity")  # æ’åºæ–¹å¼: similarity, date, size
        
        if not query:
            return JSONResponse(
                content={"error": "æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )
        
        project_path = get_project_path(project_name)
        
        if project_path not in rag_cache:
            rag_cache[project_path] = get_rag_service(project_path)
        
        rag_service = rag_cache[project_path]
        
        # æ‰§è¡Œæ£€ç´¢
        results = rag_service.retrieve(query, n_results)
        
        # åº”ç”¨è¿‡æ»¤å’Œæ’åº
        filtered_results = []
        for result in results:
            metadata = result.get('metadata', {})
            similarity = result.get('similarity', 0)
            
            # ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
            if similarity < similarity_threshold:
                continue
            
            # æ–‡ä»¶ç±»å‹è¿‡æ»¤
            if file_types:
                file_ext = os.path.splitext(metadata.get('file_path', ''))[1].lower()
                if file_ext not in file_types:
                    continue
            
            # ç¼–ç¨‹è¯­è¨€è¿‡æ»¤
            if languages:
                language = metadata.get('language', '')
                if language not in languages:
                    continue
            
            # å—å¤§å°è¿‡æ»¤
            content_size = len(result.get('content', ''))
            if content_size < min_chunk_size or content_size > max_chunk_size:
                continue
            
            filtered_results.append(result)
        
        # æ’åº
        if sort_by == "similarity":
            filtered_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        elif sort_by == "date":
            filtered_results.sort(key=lambda x: x.get('metadata', {}).get('timestamp', ''), reverse=True)
        elif sort_by == "size":
            filtered_results.sort(key=lambda x: len(x.get('content', '')), reverse=True)
        
        # é™åˆ¶ç»“æœæ•°é‡
        final_results = filtered_results[:n_results]
        
        return {
            "success": True,
            "query": query,
            "results": final_results,
            "count": len(final_results),
            "total_filtered": len(filtered_results),
            "filters_applied": {
                "similarity_threshold": similarity_threshold,
                "file_types": file_types,
                "languages": languages,
                "min_chunk_size": min_chunk_size,
                "max_chunk_size": max_chunk_size,
                "sort_by": sort_by
            }
        }
    except Exception as e:
        logger.exception(f"RAG æ£€ç´¢å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"RAG æ£€ç´¢å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/reset/{project_name}")
async def reset_rag(project_name: str):
    """é‡ç½® RAG ç´¢å¼•"""
    try:
        project_path = get_project_path(project_name)
        
        if project_path in rag_cache:
            rag_service = rag_cache[project_path]
            rag_service.reset()
            del rag_cache[project_path]
        
        return {
            "success": True,
            "message": "RAG ç´¢å¼•å·²é‡ç½®"
        }
    except Exception as e:
        logger.exception(f"RAG é‡ç½®å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"RAG é‡ç½®å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/clear-cache")
async def clear_rag_cache():
    """æ¸…é™¤ RAG æœåŠ¡ç¼“å­˜"""
    try:
        count = len(rag_cache)
        rag_cache.clear()
        logger.info(f"Cleared RAG cache: {count} services removed")
        
        return {
            "success": True,
            "message": f"å·²æ¸…é™¤ {count} ä¸ª RAG æœåŠ¡ç¼“å­˜"
        }
    except Exception as e:
        logger.exception(f"æ¸…é™¤ RAG ç¼“å­˜å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ¸…é™¤ RAG ç¼“å­˜å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/ask/{project_name}")
async def ask_rag_question(project_name: str, request: Request):
    """å‘ RAG çŸ¥è¯†åº“æé—®"""
    try:
        data = await request.json()
        question = data.get("question", "")
        
        if not question:
            return JSONResponse(
                content={"error": "é—®é¢˜ä¸èƒ½ä¸ºç©º"},
                status_code=400
            )
        
        project_path = get_project_path(project_name)
        
        # è·å– RAG æœåŠ¡
        if project_path not in rag_cache:
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            if use_chromadb and not CHROMADB_AVAILABLE:
                use_chromadb = False
            rag_cache[project_path] = get_rag_service(project_path, use_chromadb=use_chromadb)
        
        rag_service = rag_cache[project_path]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£
        stats = rag_service.get_stats()
        if stats.get("document_count", 0) == 0:
            return JSONResponse(
                content={"answer": "çŸ¥è¯†åº“ä¸­è¿˜æ²¡æœ‰æ–‡æ¡£ã€‚è¯·å…ˆæ·»åŠ æ–‡æ¡£æˆ–ç´¢å¼•é¡¹ç›®ã€‚", "sources": []},
                status_code=200
            )
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        results = rag_service.retrieve(question, n_results=5)
        
        if not results or len(results) == 0:
            return JSONResponse(
                content={"answer": "çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚", "sources": []},
                status_code=200
            )
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        sources = []
        max_similarity = 0
        
        for i, result in enumerate(results):
            # result æ˜¯å­—å…¸ï¼Œä¸æ˜¯å¯¹è±¡
            metadata = result.get('metadata', {})
            similarity = result.get('similarity', 0)
            
            # è®°å½•æœ€é«˜ç›¸ä¼¼åº¦
            if similarity > max_similarity:
                max_similarity = similarity
            
            # æå–æ›´è¯¦ç»†çš„æ¥æºä¿¡æ¯
            file_path = metadata.get('file_path', 'æœªçŸ¥æ–‡ä»¶')
            chunk_index = metadata.get('chunk_index', 0)
            total_chunks = metadata.get('total_chunks', 1)
            start_line = metadata.get('start_line', 1)
            end_line = metadata.get('end_line', 1)
            language = metadata.get('language', '')
            summary = metadata.get('summary', '')
            
            # æ„å»ºæ¥æºæè¿°
            source_desc = f"{file_path}"
            if language:
                source_desc += f" ({language})"
            if start_line and end_line:
                source_desc += f" [è¡Œ {start_line}-{end_line}]"
            
            context_parts.append(f"[æ–‡æ¡£ {i+1}] {source_desc}:\n{result['content']}")
            
            sources.append({
                "file_path": file_path,
                "content": result['content'][:200] + '...' if len(result['content']) > 200 else result['content'],
                "similarity": similarity,
                "chunk_index": chunk_index,
                "total_chunks": total_chunks,
                "start_line": start_line,
                "end_line": end_line,
                "language": language,
                "summary": summary,
                "source_desc": source_desc
            })
        
        logger.info(f"RAG é—®ç­”: ä¸ºé—®é¢˜ '{question}' æ‰¾åˆ° {len(sources)} ä¸ªæ¥æº")
        logger.info("=" * 80)
        logger.info("è¿”å›ç»™å‰ç«¯çš„ sources æ•°ç»„:")
        logger.info("=" * 80)
        for i, source in enumerate(sources):
            logger.info(f"\næ¥æº #{i+1}:")
            logger.info(f"  file_path: {source['file_path']}")
            logger.info(f"  chunk_index: {source['chunk_index']}")
            logger.info(f"  total_chunks: {source['total_chunks']}")
            logger.info(f"  start_line: {source['start_line']}")
            logger.info(f"  end_line: {source['end_line']}")
            logger.info(f"  similarity: {source['similarity']}")
            logger.info(f"  content_length: {len(source['content'])}")
            logger.info(f"  content_preview: {source['content'][:150]}")
        logger.info("=" * 80)
        
        context = '\n\n'.join(context_parts)
        
        # è®¡ç®—ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆåŸºäºæ£€ç´¢ç»“æœçš„ç›¸ä¼¼åº¦ï¼‰
        confidence_score = 0
        if sources:
            # ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦ä½œä¸ºç½®ä¿¡åº¦
            avg_similarity = sum(s['similarity'] for s in sources) / len(sources)
            confidence_score = avg_similarity * 100
        
        # ä½¿ç”¨ AI ç”Ÿæˆå›ç­”
        try:
            agent = get_agent(project_path, global_config.get("mode", "yolo"), global_config.get("model"))
            
            # æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤º
            rag_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰å¸®åŠ©ã€‚"""
            
            # æ”¶é›† AI å›ç­”
            answer_parts = []
            async for msg in agent.chat_stream(rag_prompt):
                if isinstance(msg, str):
                    answer_parts.append(msg)
                elif isinstance(msg, dict) and msg.get("type") == "assistant":
                    answer_parts.append(msg.get("content", ""))
            
            answer = "".join(answer_parts)
            
            # å¦‚æœæ²¡æœ‰ç”Ÿæˆå›ç­”ï¼Œä½¿ç”¨é»˜è®¤å›ç­”
            if not answer:
                answer = f"åŸºäºçŸ¥è¯†åº“æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ã€‚\n\nç›¸å…³æ–‡æ¡£ï¼š\n"
                for i, source in enumerate(sources):
                    answer += f"{i+1}. {source['file_path']}\n"
        
        except Exception as ai_error:
            logger.warning(f"AI ç”Ÿæˆå›ç­”å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å›ç­”: {ai_error}")
            answer = f"åŸºäºçŸ¥è¯†åº“æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ã€‚\n\nç›¸å…³æ–‡æ¡£ï¼š\n"
            for i, source in enumerate(sources):
                answer += f"{i+1}. {source['file_path']}\n"
        
        return JSONResponse(
            content={
                "answer": answer,
                "question": question,
                "sources": sources,
                "confidence": {
                    "score": round(confidence_score, 2),
                    "level": "high" if confidence_score > 70 else "medium" if confidence_score > 40 else "low"
                },
                "related_documents": [
                    {
                        "file_path": s['file_path'],
                        "similarity": s['similarity'],
                        "summary": s.get('summary', '')
                    }
                    for s in sources[:3]  # æ¨è top 3 ç›¸å…³æ–‡æ¡£
                ]
            },
            status_code=200
        )
        
    except Exception as e:
        logger.exception(f"RAG é—®ç­”å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"RAG é—®ç­”å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/upload/{project_name}")
async def upload_document_to_rag(project_name: str, request: Request):
    """ä¸Šä¼ æ–‡æ¡£åˆ° RAG çŸ¥è¯†åº“"""
    try:
        project_path = get_project_path(project_name)
        
        # è§£æè¡¨å•æ•°æ®
        form = await request.form()
        file = form.get("file")
        
        if not file:
            return JSONResponse(
                content={"error": "æœªæ‰¾åˆ°æ–‡ä»¶"},
                status_code=400
            )
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        text_content = content.decode('utf-8', errors='ignore')
        
        # è·å– RAG æœåŠ¡
        if project_path not in rag_cache:
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            if use_chromadb and not CHROMADB_AVAILABLE:
                use_chromadb = False
            rag_cache[project_path] = get_rag_service(project_path, use_chromadb=use_chromadb)
        
        rag_service = rag_cache[project_path]
        
        # æ·»åŠ æ–‡æ¡£
        result = await rag_service.add_document(
            file_name=file.filename,
            content=text_content,
            file_type=os.path.splitext(file.filename)[1].lower()
        )
        
        return result
        
    except Exception as e:
        logger.exception(f"ä¸Šä¼ æ–‡æ¡£åˆ° RAG å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"ä¸Šä¼ æ–‡æ¡£åˆ° RAG å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/upload-batch/{project_name}")
async def upload_documents_batch_to_rag(project_name: str, request: Request):
    """æ‰¹é‡ä¸Šä¼ æ–‡æ¡£åˆ° RAG çŸ¥è¯†åº“"""
    try:
        project_path = get_project_path(project_name)
        
        # è§£æè¡¨å•æ•°æ®
        form = await request.form()
        files = form.getlist("files")
        
        if not files:
            return JSONResponse(
                content={"error": "æœªæ‰¾åˆ°æ–‡ä»¶"},
                status_code=400
            )
        
        # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        temp_dir = os.path.join(project_path, ".rag_temp")
        os.makedirs(temp_dir, exist_ok=True)
        
        file_paths = []
        for file in files:
            file_path = os.path.join(temp_dir, file.filename)
            with open(file_path, 'wb') as f:
                f.write(await file.read())
            file_paths.append(file_path)
        
        # è·å– RAG æœåŠ¡
        if project_path not in rag_cache:
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            if use_chromadb and not CHROMADB_AVAILABLE:
                use_chromadb = False
            rag_cache[project_path] = get_rag_service(project_path, use_chromadb=use_chromadb)
        
        rag_service = rag_cache[project_path]
        
        # åˆ›å»ºæµå¼å“åº”
        async def progress_generator():
            try:
                async for result in rag_service.add_documents_from_files(file_paths):
                    yield f"data: {json.dumps(result)}\n\n"
                    
                    if result.get("type") == "complete":
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        for fp in file_paths:
                            try:
                                os.remove(fp)
                            except:
                                pass
                        break
            except Exception as e:
                logger.exception(f"æ‰¹é‡ä¸Šä¼ æ–‡æ¡£å¤±è´¥: {e}")
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
        
        return StreamingResponse(progress_generator(), media_type="text/event-stream")
        
    except Exception as e:
        logger.exception(f"æ‰¹é‡ä¸Šä¼ æ–‡æ¡£åˆ° RAG å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ‰¹é‡ä¸Šä¼ æ–‡æ¡£åˆ° RAG å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/add-files/{project_name}")
async def add_files_to_rag(project_name: str, request: Request):
    """æ·»åŠ ç³»ç»Ÿæ–‡ä»¶è·¯å¾„åˆ° RAG çŸ¥è¯†åº“ï¼ˆç›´æ¥è¯»å–ï¼Œä¸ä¸Šä¼ ï¼‰"""
    try:
        data = await request.json()
        file_paths = data.get("file_paths", [])
        
        logger.info(f"æ”¶åˆ°æ·»åŠ æ–‡ä»¶è¯·æ±‚ï¼Œé¡¹ç›®: {project_name}, æ–‡ä»¶æ•°: {len(file_paths)}")
        logger.info(f"æ–‡ä»¶è·¯å¾„åˆ—è¡¨: {file_paths}")
        
        if not file_paths:
            return JSONResponse(
                content={"error": "æœªæä¾›æ–‡ä»¶è·¯å¾„"},
                status_code=400
            )
        
        project_path = get_project_path(project_name)
        
        # éªŒè¯è·¯å¾„å®‰å…¨æ€§ï¼ˆRAG å…è®¸æ›´å®½æ¾çš„è·¯å¾„é™åˆ¶ï¼‰
        valid_paths = []
        for file_path in file_paths:
            # è§„èŒƒåŒ–è·¯å¾„
            file_path = os.path.abspath(file_path)
            logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")
            
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                logger.warning(f"è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„: {file_path}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶
            if not os.path.isfile(file_path):
                logger.warning(f"è·³è¿‡éæ–‡ä»¶è·¯å¾„: {file_path}")
                continue
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶ 500MBï¼‰
            try:
                file_size = os.path.getsize(file_path)
                if file_size > 500 * 1024 * 1024:  # 500MB
                    logger.warning(f"è·³è¿‡è¿‡å¤§çš„æ–‡ä»¶: {file_path} ({file_size} bytes)")
                    continue
            except:
                logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤§å°: {file_path}")
                continue
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            allowed_extensions = {
                '.txt', '.md', '.rst', '.py', '.js', '.ts', '.jsx', '.tsx',
                '.java', '.go', '.rs', '.json', '.yaml', '.yml', '.html', '.css',
                '.xml', '.csv', '.log', '.sql', '.sh', '.bat', '.ps1',
                '.docx', '.xlsx', '.pptx', '.pdf'
            }
            ext = os.path.splitext(file_path)[1].lower()
            if ext not in allowed_extensions:
                logger.warning(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path} ({ext})")
                continue
            
            valid_paths.append(file_path)
            logger.info(f"æ–‡ä»¶æœ‰æ•ˆ: {file_path}")
        
        logger.info(f"æœ‰æ•ˆæ–‡ä»¶æ•°: {len(valid_paths)}")
        
        if not valid_paths:
            return JSONResponse(
                content={"error": "æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„ï¼ˆæ–‡ä»¶ä¸å­˜åœ¨ã€è¿‡å¤§æˆ–ä¸æ”¯æŒçš„ç±»å‹ï¼‰ã€‚æ”¯æŒçš„æœ€å¤§æ–‡ä»¶å¤§å°: 500MB"},
                status_code=400
            )
        
        # è·å– RAG æœåŠ¡
        if project_path not in rag_cache:
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            if use_chromadb and not CHROMADB_AVAILABLE:
                use_chromadb = False
            rag_cache[project_path] = get_rag_service(project_path, use_chromadb=use_chromadb)
        
        rag_service = rag_cache[project_path]
        
        # åˆ›å»ºæµå¼å“åº”
        async def progress_generator():
            try:
                async for result in rag_service.add_documents_from_files(valid_paths):
                    yield f"data: {json.dumps(result)}\n\n"
            except Exception as e:
                logger.exception(f"æ·»åŠ æ–‡ä»¶åˆ° RAG å¤±è´¥: {e}")
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
        
        return StreamingResponse(progress_generator(), media_type="text/event-stream")
        
    except Exception as e:
        logger.exception(f"æ·»åŠ æ–‡ä»¶åˆ° RAG å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ·»åŠ æ–‡ä»¶åˆ° RAG å¤±è´¥: {str(e)}"},
            status_code=500
        )

# ==================== æ–‡æ¡£ç‰ˆæœ¬ç®¡ç† API ====================

@app.get("/api/document-versions/{project_name}/{file_path:path}")
async def get_document_versions(project_name: str, file_path: str):
    """è·å–æ–‡æ¡£çš„æ‰€æœ‰ç‰ˆæœ¬"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        full_file_path = os.path.join(project_path, file_path)
        
        if not os.path.exists(full_file_path):
            return JSONResponse(
                content={"error": "æ–‡ä»¶ä¸å­˜åœ¨"},
                status_code=404
            )
        
        versions = version_manager.get_versions(full_file_path)
        
        return {
            "success": True,
            "file_path": file_path,
            "versions": versions,
            "total": len(versions)
        }
    except Exception as e:
        logger.exception(f"è·å–æ–‡æ¡£ç‰ˆæœ¬å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è·å–æ–‡æ¡£ç‰ˆæœ¬å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/document-versions/{project_name}/{file_path:path}/{version_id}")
async def get_document_version(project_name: str, file_path: str, version_id: str):
    """è·å–ç‰¹å®šç‰ˆæœ¬çš„æ–‡æ¡£å†…å®¹"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        full_file_path = os.path.join(project_path, file_path)
        
        version = version_manager.get_version(full_file_path, version_id)
        
        if not version:
            return JSONResponse(
                content={"error": "ç‰ˆæœ¬ä¸å­˜åœ¨"},
                status_code=404
            )
        
        return {
            "success": True,
            "version": version
        }
    except Exception as e:
        logger.exception(f"è·å–æ–‡æ¡£ç‰ˆæœ¬å†…å®¹å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è·å–æ–‡æ¡£ç‰ˆæœ¬å†…å®¹å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.post("/api/document-versions/{project_name}/{file_path:path}/record")
async def record_document_version(project_name: str, file_path: str, request: Request):
    """è®°å½•æ–‡æ¡£ç‰ˆæœ¬"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        full_file_path = os.path.join(project_path, file_path)
        
        if not os.path.exists(full_file_path):
            return JSONResponse(
                content={"error": "æ–‡ä»¶ä¸å­˜åœ¨"},
                status_code=404
            )
        
        # è·å–å…ƒæ•°æ®
        try:
            data = await request.json()
            metadata = data.get("metadata", {})
        except:
            metadata = {}
        
        # è®°å½•ç‰ˆæœ¬
        version = version_manager.record_version(full_file_path, metadata=metadata)
        
        if not version:
            return JSONResponse(
                content={"error": "è®°å½•ç‰ˆæœ¬å¤±è´¥"},
                status_code=500
            )
        
        return {
            "success": True,
            "version": version
        }
    except Exception as e:
        logger.exception(f"è®°å½•æ–‡æ¡£ç‰ˆæœ¬å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è®°å½•æ–‡æ¡£ç‰ˆæœ¬å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/document-versions/{project_name}/{file_path:path}/compare/{version_id1}/{version_id2}")
async def compare_document_versions(project_name: str, file_path: str, version_id1: str, version_id2: str):
    """æ¯”è¾ƒä¸¤ä¸ªæ–‡æ¡£ç‰ˆæœ¬"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        full_file_path = os.path.join(project_path, file_path)
        
        comparison = version_manager.compare_versions(full_file_path, version_id1, version_id2)
        
        if not comparison:
            return JSONResponse(
                content={"error": "æ¯”è¾ƒç‰ˆæœ¬å¤±è´¥"},
                status_code=500
            )
        
        return {
            "success": True,
            "comparison": comparison
        }
    except Exception as e:
        logger.exception(f"æ¯”è¾ƒæ–‡æ¡£ç‰ˆæœ¬å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"æ¯”è¾ƒæ–‡æ¡£ç‰ˆæœ¬å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.delete("/api/document-versions/{project_name}/{file_path:path}/{version_id}")
async def delete_document_version(project_name: str, file_path: str, version_id: str):
    """åˆ é™¤ç‰¹å®šç‰ˆæœ¬"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        full_file_path = os.path.join(project_path, file_path)
        
        success = version_manager.delete_version(full_file_path, version_id)
        
        return {
            "success": success,
            "message": "ç‰ˆæœ¬å·²åˆ é™¤" if success else "åˆ é™¤ç‰ˆæœ¬å¤±è´¥"
        }
    except Exception as e:
        logger.exception(f"åˆ é™¤æ–‡æ¡£ç‰ˆæœ¬å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"åˆ é™¤æ–‡æ¡£ç‰ˆæœ¬å¤±è´¥: {str(e)}"},
            status_code=500
        )


@app.get("/api/document-versions/{project_name}/statistics")
async def get_version_statistics(project_name: str):
    """è·å–ç‰ˆæœ¬ç»Ÿè®¡ä¿¡æ¯"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        stats = version_manager.get_statistics()
        
        return {
            "success": True,
            "statistics": stats
        }
    except Exception as e:
        logger.exception(f"è·å–ç‰ˆæœ¬ç»Ÿè®¡å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": f"è·å–ç‰ˆæœ¬ç»Ÿè®¡å¤±è´¥: {str(e)}"},
            status_code=500
        )

# ============================================================================
# å¼€å‘è€…å·¥å…· API
# ============================================================================

import sqlite3
import json
from datetime import datetime
from typing import List, Optional, Dict, Any
from pydantic import BaseModel

# æ•°æ®åº“è·¯å¾„
DB_PATH = os.path.join(os.path.dirname(__file__), "developer_tools.db")

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # ä»£ç ç‰‡æ®µè¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS snippets (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            code TEXT NOT NULL,
            language TEXT DEFAULT 'javascript',
            category TEXT DEFAULT 'é€šç”¨',
            description TEXT,
            tags TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            is_favorite INTEGER DEFAULT 0,
            usage_count INTEGER DEFAULT 0
        )
    ''')
    
    # å‘½ä»¤å¿«æ·æ–¹å¼è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS command_shortcuts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            command TEXT NOT NULL,
            category TEXT DEFAULT 'é€šç”¨',
            description TEXT,
            tags TEXT,
            working_dir TEXT,
            timeout INTEGER DEFAULT 60,
            parameters TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            is_favorite INTEGER DEFAULT 0,
            usage_count INTEGER DEFAULT 0
        )
    ''')
    
    # æç¤ºè¯è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS prompts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            content TEXT NOT NULL,
            category TEXT DEFAULT 'è‡ªå®šä¹‰',
            description TEXT,
            tags TEXT,
            parameters TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            is_favorite INTEGER DEFAULT 0,
            usage_count INTEGER DEFAULT 0
        )
    ''')
    
    # æ–¹æ¡ˆè¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS solutions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            requirement TEXT NOT NULL,
            solution TEXT NOT NULL,
            template_type TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # æ‰§è¡Œå†å²è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS execution_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            shortcut_id INTEGER,
            command TEXT,
            working_dir TEXT,
            status TEXT,
            output TEXT,
            error TEXT,
            exit_code INTEGER,
            duration REAL,
            executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (shortcut_id) REFERENCES command_shortcuts(id)
        )
    ''')
    
    conn.commit()
    conn.close()

# åˆå§‹åŒ–æ•°æ®åº“
try:
    init_db()
    task_master_service.init_tables()
    logger.info("å¼€å‘è€…å·¥å…·æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")

# ============================================================================
# ä»£ç ç‰‡æ®µç®¡ç†å™¨ API
# ============================================================================

class SnippetCreate(BaseModel):
    title: str
    code: str
    language: str = "javascript"
    category: str = "é€šç”¨"
    description: str = ""
    tags: List[str] = []

class SnippetUpdate(BaseModel):
    title: Optional[str] = None
    code: Optional[str] = None
    language: Optional[str] = None
    category: Optional[str] = None
    description: Optional[str] = None
    tags: Optional[List[str]] = None

@app.get("/api/snippets")
async def get_snippets(
    search: Optional[str] = None,
    category: Optional[str] = None,
    language: Optional[str] = None,
    favorite_only: bool = False
):
    """è·å–ä»£ç ç‰‡æ®µåˆ—è¡¨"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = "SELECT * FROM snippets WHERE 1=1"
        params = []
        
        if search:
            query += " AND (title LIKE ? OR description LIKE ? OR code LIKE ?)"
            params.extend([f"%{search}%", f"%{search}%", f"%{search}%"])
        
        if category:
            query += " AND category = ?"
            params.append(category)
        
        if language:
            query += " AND language = ?"
            params.append(language)
        
        if favorite_only:
            query += " AND is_favorite = 1"
        
        query += " ORDER BY updated_at DESC"
        
        cursor.execute(query, params)
        rows = cursor.fetchall()
        
        snippets = []
        for row in rows:
            snippet = dict(row)
            snippet['tags'] = json.loads(snippet['tags']) if snippet['tags'] else []
            snippets.append(snippet)
        
        # è·å–åˆ†ç±»å’Œæ ‡ç­¾
        categories = [row[0] for row in cursor.execute("SELECT DISTINCT category FROM snippets ORDER BY category")]
        tags = set()
        for snippet in snippets:
            tags.update(snippet['tags'])
        
        conn.close()
        
        return JSONResponse({
            "snippets": snippets,
            "categories": categories,
            "tags": list(tags)
        })
    except Exception as e:
        logger.exception(f"è·å–ä»£ç ç‰‡æ®µå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/snippets")
async def create_snippet(snippet: SnippetCreate):
    """åˆ›å»ºä»£ç ç‰‡æ®µ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO snippets (title, code, language, category, description, tags)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            snippet.title,
            snippet.code,
            snippet.language,
            snippet.category,
            snippet.description,
            json.dumps(snippet.tags)
        ))
        
        snippet_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return JSONResponse({"id": snippet_id, "message": "ä»£ç ç‰‡æ®µåˆ›å»ºæˆåŠŸ"})
    except Exception as e:
        logger.exception(f"åˆ›å»ºä»£ç ç‰‡æ®µå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/categories")
async def get_snippet_categories():
    """è·å–ä»£ç ç‰‡æ®µåˆ†ç±»"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT DISTINCT category FROM snippets ORDER BY category")
        categories = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return JSONResponse({"categories": categories})
    except Exception as e:
        logger.exception(f"è·å–ä»£ç ç‰‡æ®µåˆ†ç±»å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/tags")
async def get_snippet_tags():
    """è·å–ä»£ç ç‰‡æ®µæ ‡ç­¾"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT tags FROM snippets")
        all_tags = set()
        for row in cursor.fetchall():
            if row[0]:
                tags = json.loads(row[0])
                all_tags.update(tags)
        
        conn.close()
        
        return JSONResponse({"tags": list(all_tags)})
    except Exception as e:
        logger.exception(f"è·å–ä»£ç ç‰‡æ®µæ ‡ç­¾å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/{snippet_id}")
async def get_snippet(snippet_id: int):
    """è·å–å•ä¸ªä»£ç ç‰‡æ®µ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM snippets WHERE id = ?", (snippet_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return JSONResponse({"error": "ä»£ç ç‰‡æ®µä¸å­˜åœ¨"}, status_code=404)
        
        snippet = dict(row)
        snippet['tags'] = json.loads(snippet['tags']) if snippet['tags'] else []
        
        # å¢åŠ ä½¿ç”¨æ¬¡æ•°
        cursor.execute("UPDATE snippets SET usage_count = usage_count + 1 WHERE id = ?", (snippet_id,))
        conn.commit()
        
        conn.close()
        
        return JSONResponse(snippet)
    except Exception as e:
        logger.exception(f"è·å–ä»£ç ç‰‡æ®µå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.put("/api/snippets/{snippet_id}")
async def update_snippet(snippet_id: int, snippet: SnippetUpdate):
    """æ›´æ–°ä»£ç ç‰‡æ®µ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
        cursor.execute("SELECT id FROM snippets WHERE id = ?", (snippet_id,))
        if not cursor.fetchone():
            conn.close()
            return JSONResponse({"error": "ä»£ç ç‰‡æ®µä¸å­˜åœ¨"}, status_code=404)
        
        # æ„å»ºæ›´æ–°è¯­å¥
        updates = []
        params = []
        
        if snippet.title is not None:
            updates.append("title = ?")
            params.append(snippet.title)
        if snippet.code is not None:
            updates.append("code = ?")
            params.append(snippet.code)
        if snippet.language is not None:
            updates.append("language = ?")
            params.append(snippet.language)
        if snippet.category is not None:
            updates.append("category = ?")
            params.append(snippet.category)
        if snippet.description is not None:
            updates.append("description = ?")
            params.append(snippet.description)
        if snippet.tags is not None:
            updates.append("tags = ?")
            params.append(json.dumps(snippet.tags))
        
        updates.append("updated_at = CURRENT_TIMESTAMP")
        params.append(snippet_id)
        
        query = f"UPDATE snippets SET {', '.join(updates)} WHERE id = ?"
        cursor.execute(query, params)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "ä»£ç ç‰‡æ®µæ›´æ–°æˆåŠŸ"})
    except Exception as e:
        logger.exception(f"æ›´æ–°ä»£ç ç‰‡æ®µå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/snippets/{snippet_id}")
async def delete_snippet(snippet_id: int):
    """åˆ é™¤ä»£ç ç‰‡æ®µ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM snippets WHERE id = ?", (snippet_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "ä»£ç ç‰‡æ®µä¸å­˜åœ¨"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "ä»£ç ç‰‡æ®µåˆ é™¤æˆåŠŸ"})
    except Exception as e:
        logger.exception(f"åˆ é™¤ä»£ç ç‰‡æ®µå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/popular")
async def get_popular_snippets(limit: int = 10):
    """è·å–çƒ­é—¨ä»£ç ç‰‡æ®µ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM snippets ORDER BY usage_count DESC, updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        snippets = []
        for row in rows:
            snippet = dict(row)
            snippet['tags'] = json.loads(snippet['tags']) if snippet['tags'] else []
            snippets.append(snippet)
        
        conn.close()
        
        return JSONResponse({"snippets": snippets})
    except Exception as e:
        logger.exception(f"è·å–çƒ­é—¨ä»£ç ç‰‡æ®µå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/recent")
async def get_recent_snippets(limit: int = 10):
    """è·å–æœ€è¿‘ä»£ç ç‰‡æ®µ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM snippets ORDER BY updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        snippets = []
        for row in rows:
            snippet = dict(row)
            snippet['tags'] = json.loads(snippet['tags']) if snippet['tags'] else []
            snippets.append(snippet)
        
        conn.close()
        
        return JSONResponse({"snippets": snippets})
    except Exception as e:
        logger.exception(f"è·å–æœ€è¿‘ä»£ç ç‰‡æ®µå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/snippets/{snippet_id}/usage")
async def increment_snippet_usage(snippet_id: int):
    """å¢åŠ ä»£ç ç‰‡æ®µä½¿ç”¨æ¬¡æ•°"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("UPDATE snippets SET usage_count = usage_count + 1 WHERE id = ?", (snippet_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "ä»£ç ç‰‡æ®µä¸å­˜åœ¨"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "ä½¿ç”¨æ¬¡æ•°å·²æ›´æ–°"})
    except Exception as e:
        logger.exception(f"æ›´æ–°ä½¿ç”¨æ¬¡æ•°å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# å‘½ä»¤å¿«æ·æ–¹å¼ API
# ============================================================================

class CommandShortcutCreate(BaseModel):
    name: str
    command: str
    category: str = "é€šç”¨"
    description: str = ""
    tags: List[str] = []
    working_dir: str = ""
    timeout: int = 60
    parameters: List[Dict[str, Any]] = []

class CommandShortcutUpdate(BaseModel):
    name: Optional[str] = None
    command: Optional[str] = None
    category: Optional[str] = None
    description: Optional[str] = None
    tags: Optional[List[str]] = None
    working_dir: Optional[str] = None
    timeout: Optional[int] = None
    parameters: Optional[List[Dict[str, Any]]] = None

@app.get("/api/command-shortcuts")
async def get_command_shortcuts(
    search: Optional[str] = None,
    category: Optional[str] = None,
    favorite_only: bool = False
):
    """è·å–å‘½ä»¤å¿«æ·æ–¹å¼åˆ—è¡¨"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = "SELECT * FROM command_shortcuts WHERE 1=1"
        params = []
        
        if search:
            query += " AND (name LIKE ? OR description LIKE ? OR command LIKE ?)"
            params.extend([f"%{search}%", f"%{search}%", f"%{search}%"])
        
        if category:
            query += " AND category = ?"
            params.append(category)
        
        if favorite_only:
            query += " AND is_favorite = 1"
        
        query += " ORDER BY usage_count DESC, updated_at DESC"
        
        cursor.execute(query, params)
        rows = cursor.fetchall()
        
        shortcuts = []
        for row in rows:
            shortcut = dict(row)
            shortcut['tags'] = json.loads(shortcut['tags']) if shortcut['tags'] else []
            shortcut['parameters'] = json.loads(shortcut['parameters']) if shortcut['parameters'] else []
            shortcuts.append(shortcut)
        
        # è·å–åˆ†ç±»å’Œæ ‡ç­¾
        categories = [row[0] for row in cursor.execute("SELECT DISTINCT category FROM command_shortcuts ORDER BY category")]
        tags = set()
        for shortcut in shortcuts:
            tags.update(shortcut['tags'])
        
        conn.close()
        
        return JSONResponse({
            "shortcuts": shortcuts,
            "categories": categories,
            "tags": list(tags)
        })
    except Exception as e:
        logger.exception(f"è·å–å‘½ä»¤å¿«æ·æ–¹å¼å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/command-shortcuts")
async def create_command_shortcut(shortcut: CommandShortcutCreate):
    """åˆ›å»ºå‘½ä»¤å¿«æ·æ–¹å¼"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO command_shortcuts (name, command, category, description, tags, working_dir, timeout, parameters)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            shortcut.name,
            shortcut.command,
            shortcut.category,
            shortcut.description,
            json.dumps(shortcut.tags),
            shortcut.working_dir,
            shortcut.timeout,
            json.dumps(shortcut.parameters)
        ))
        
        shortcut_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return JSONResponse({"id": shortcut_id, "message": "å‘½ä»¤å¿«æ·æ–¹å¼åˆ›å»ºæˆåŠŸ"})
    except Exception as e:
        logger.exception(f"åˆ›å»ºå‘½ä»¤å¿«æ·æ–¹å¼å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.put("/api/command-shortcuts/{shortcut_id}")
async def update_command_shortcut(shortcut_id: int, shortcut: CommandShortcutUpdate):
    """æ›´æ–°å‘½ä»¤å¿«æ·æ–¹å¼"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT id FROM command_shortcuts WHERE id = ?", (shortcut_id,))
        if not cursor.fetchone():
            conn.close()
            return JSONResponse({"error": "å‘½ä»¤å¿«æ·æ–¹å¼ä¸å­˜åœ¨"}, status_code=404)
        
        updates = []
        params = []
        
        if shortcut.name is not None:
            updates.append("name = ?")
            params.append(shortcut.name)
        if shortcut.command is not None:
            updates.append("command = ?")
            params.append(shortcut.command)
        if shortcut.category is not None:
            updates.append("category = ?")
            params.append(shortcut.category)
        if shortcut.description is not None:
            updates.append("description = ?")
            params.append(shortcut.description)
        if shortcut.tags is not None:
            updates.append("tags = ?")
            params.append(json.dumps(shortcut.tags))
        if shortcut.working_dir is not None:
            updates.append("working_dir = ?")
            params.append(shortcut.working_dir)
        if shortcut.timeout is not None:
            updates.append("timeout = ?")
            params.append(shortcut.timeout)
        if shortcut.parameters is not None:
            updates.append("parameters = ?")
            params.append(json.dumps(shortcut.parameters))
        
        updates.append("updated_at = CURRENT_TIMESTAMP")
        params.append(shortcut_id)
        
        query = f"UPDATE command_shortcuts SET {', '.join(updates)} WHERE id = ?"
        cursor.execute(query, params)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "å‘½ä»¤å¿«æ·æ–¹å¼æ›´æ–°æˆåŠŸ"})
    except Exception as e:
        logger.exception(f"æ›´æ–°å‘½ä»¤å¿«æ·æ–¹å¼å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/command-shortcuts/{shortcut_id}")
async def delete_command_shortcut(shortcut_id: int):
    """åˆ é™¤å‘½ä»¤å¿«æ·æ–¹å¼"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM command_shortcuts WHERE id = ?", (shortcut_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "å‘½ä»¤å¿«æ·æ–¹å¼ä¸å­˜åœ¨"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "å‘½ä»¤å¿«æ·æ–¹å¼åˆ é™¤æˆåŠŸ"})
    except Exception as e:
        logger.exception(f"åˆ é™¤å‘½ä»¤å¿«æ·æ–¹å¼å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/command-shortcuts/categories")
async def get_command_shortcut_categories():
    """è·å–å‘½ä»¤å¿«æ·æ–¹å¼åˆ†ç±»"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT DISTINCT category FROM command_shortcuts ORDER BY category")
        categories = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return JSONResponse({"categories": categories})
    except Exception as e:
        logger.exception(f"è·å–å‘½ä»¤å¿«æ·æ–¹å¼åˆ†ç±»å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/command-shortcuts/tags")
async def get_command_shortcut_tags():
    """è·å–å‘½ä»¤å¿«æ·æ–¹å¼æ ‡ç­¾"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT tags FROM command_shortcuts")
        all_tags = set()
        for row in cursor.fetchall():
            if row[0]:
                tags = json.loads(row[0])
                all_tags.update(tags)
        
        conn.close()
        
        return JSONResponse({"tags": list(all_tags)})
    except Exception as e:
        logger.exception(f"è·å–å‘½ä»¤å¿«æ·æ–¹å¼æ ‡ç­¾å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/command-shortcuts/history")
async def get_execution_history(limit: int = 50):
    """è·å–æ‰§è¡Œå†å²"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT eh.*, cs.name as shortcut_name
            FROM execution_history eh
            LEFT JOIN command_shortcuts cs ON eh.shortcut_id = cs.id
            ORDER BY eh.executed_at DESC
            LIMIT ?
        ''', (limit,))
        
        history = [dict(row) for row in cursor.fetchall()]
        conn.close()
        
        return JSONResponse({"history": history})
    except Exception as e:
        logger.exception(f"è·å–æ‰§è¡Œå†å²å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/command-shortcuts/{shortcut_id}/execute")
async def execute_command_shortcut(shortcut_id: int, params: Optional[Dict[str, Any]] = None):
    """æ‰§è¡Œå‘½ä»¤å¿«æ·æ–¹å¼"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM command_shortcuts WHERE id = ?", (shortcut_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return JSONResponse({"error": "å‘½ä»¤å¿«æ·æ–¹å¼ä¸å­˜åœ¨"}, status_code=404)
        
        shortcut = dict(row)
        command = shortcut['command']
        working_dir = shortcut['working_dir'] or os.getcwd()
        timeout = shortcut['timeout']
        
        # æ›¿æ¢å‚æ•°
        if params:
            for key, value in params.items():
                command = command.replace(f"${{{key}}}", str(value))
        
        # å¢åŠ ä½¿ç”¨æ¬¡æ•°
        cursor.execute("UPDATE command_shortcuts SET usage_count = usage_count + 1 WHERE id = ?", (shortcut_id,))
        conn.commit()
        
        conn.close()
        
        # æ‰§è¡Œå‘½ä»¤
        import subprocess
        start_time = time.time()
        
        try:
            result = subprocess.run(
                command,
                shell=True,
                cwd=working_dir,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            duration = time.time() - start_time
            
            # ä¿å­˜æ‰§è¡Œå†å²
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO execution_history (shortcut_id, command, working_dir, status, output, error, exit_code, duration)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                shortcut_id,
                command,
                working_dir,
                "success" if result.returncode == 0 else "failed",
                result.stdout,
                result.stderr,
                result.returncode,
                duration
            ))
            conn.commit()
            conn.close()
            
            return JSONResponse({
                "status": "success" if result.returncode == 0 else "failed",
                "output": result.stdout,
                "error": result.stderr,
                "exit_code": result.returncode,
                "duration": duration
            })
        except subprocess.TimeoutExpired:
            return JSONResponse({
                "status": "timeout",
                "error": f"å‘½ä»¤æ‰§è¡Œè¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰"
            }, status_code=408)
        except Exception as e:
            return JSONResponse({
                "status": "error",
                "error": str(e)
            }, status_code=500)
            
    except Exception as e:
        logger.exception(f"æ‰§è¡Œå‘½ä»¤å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# æç¤ºè¯ç®¡ç†å™¨ API
# ============================================================================

class PromptCreate(BaseModel):
    title: str
    content: str
    category: str = "è‡ªå®šä¹‰"
    description: str = ""
    tags: List[str] = []
    parameters: List[Dict[str, Any]] = []

class PromptUpdate(BaseModel):
    title: Optional[str] = None
    content: Optional[str] = None
    category: Optional[str] = None
    description: Optional[str] = None
    tags: Optional[List[str]] = None
    parameters: Optional[List[Dict[str, Any]]] = None

@app.get("/api/prompts")
async def get_prompts(
    search: Optional[str] = None,
    category: Optional[str] = None,
    favorite_only: bool = False
):
    """è·å–æç¤ºè¯åˆ—è¡¨"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = "SELECT * FROM prompts WHERE 1=1"
        params = []
        
        if search:
            query += " AND (title LIKE ? OR description LIKE ? OR content LIKE ?)"
            params.extend([f"%{search}%", f"%{search}%", f"%{search}%"])
        
        if category:
            query += " AND category = ?"
            params.append(category)
        
        if favorite_only:
            query += " AND is_favorite = 1"
        
        query += " ORDER BY usage_count DESC, updated_at DESC"
        
        cursor.execute(query, params)
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        # è·å–åˆ†ç±»å’Œæ ‡ç­¾
        categories = [row[0] for row in cursor.execute("SELECT DISTINCT category FROM prompts ORDER BY category")]
        tags = set()
        for prompt in prompts:
            tags.update(prompt['tags'])
        
        conn.close()
        
        return JSONResponse({
            "prompts": prompts,
            "categories": categories,
            "tags": list(tags)
        })
    except Exception as e:
        logger.exception(f"è·å–æç¤ºè¯å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/prompts")
async def create_prompt(prompt: PromptCreate):
    """åˆ›å»ºæç¤ºè¯"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO prompts (title, content, category, description, tags, parameters)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            prompt.title,
            prompt.content,
            prompt.category,
            prompt.description,
            json.dumps(prompt.tags),
            json.dumps(prompt.parameters)
        ))
        
        prompt_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return JSONResponse({"id": prompt_id, "message": "æç¤ºè¯åˆ›å»ºæˆåŠŸ"})
    except Exception as e:
        logger.exception(f"åˆ›å»ºæç¤ºè¯å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/categories")
async def get_prompt_categories():
    """è·å–æç¤ºè¯åˆ†ç±»"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT DISTINCT category FROM prompts ORDER BY category")
        categories = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return JSONResponse({"categories": categories})
    except Exception as e:
        logger.exception(f"è·å–æç¤ºè¯åˆ†ç±»å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/tags")
async def get_prompt_tags():
    """è·å–æç¤ºè¯æ ‡ç­¾"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT tags FROM prompts")
        all_tags = set()
        for row in cursor.fetchall():
            if row[0]:
                tags = json.loads(row[0])
                all_tags.update(tags)
        
        conn.close()
        
        return JSONResponse({"tags": list(all_tags)})
    except Exception as e:
        logger.exception(f"è·å–æç¤ºè¯æ ‡ç­¾å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/popular")
async def get_popular_prompts(limit: int = 10):
    """è·å–çƒ­é—¨æç¤ºè¯"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts ORDER BY usage_count DESC, updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        conn.close()
        
        return JSONResponse({"prompts": prompts})
    except Exception as e:
        logger.exception(f"è·å–çƒ­é—¨æç¤ºè¯å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/recent")
async def get_recent_prompts(limit: int = 10):
    """è·å–æœ€è¿‘æç¤ºè¯"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts ORDER BY updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        conn.close()
        
        return JSONResponse({"prompts": prompts})
    except Exception as e:
        logger.exception(f"è·å–æœ€è¿‘æç¤ºè¯å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/favorite")
async def get_favorite_prompts(limit: int = 10):
    """è·å–æ”¶è—çš„æç¤ºè¯"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts WHERE is_favorite = 1 ORDER BY updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        conn.close()
        
        return JSONResponse({"prompts": prompts})
    except Exception as e:
        logger.exception(f"è·å–æ”¶è—æç¤ºè¯å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/{prompt_id}")
async def get_prompt(prompt_id: int):
    """è·å–å•ä¸ªæç¤ºè¯"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts WHERE id = ?", (prompt_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return JSONResponse({"error": "æç¤ºè¯ä¸å­˜åœ¨"}, status_code=404)
        
        prompt = dict(row)
        prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
        prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
        
        # å¢åŠ ä½¿ç”¨æ¬¡æ•°
        cursor.execute("UPDATE prompts SET usage_count = usage_count + 1 WHERE id = ?", (prompt_id,))
        conn.commit()
        
        conn.close()
        
        return JSONResponse(prompt)
    except Exception as e:
        logger.exception(f"è·å–æç¤ºè¯å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.put("/api/prompts/{prompt_id}")
async def update_prompt(prompt_id: int, prompt: PromptUpdate):
    """æ›´æ–°æç¤ºè¯"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT id FROM prompts WHERE id = ?", (prompt_id,))
        if not cursor.fetchone():
            conn.close()
            return JSONResponse({"error": "æç¤ºè¯ä¸å­˜åœ¨"}, status_code=404)
        
        updates = []
        params = []
        
        if prompt.title is not None:
            updates.append("title = ?")
            params.append(prompt.title)
        if prompt.content is not None:
            updates.append("content = ?")
            params.append(prompt.content)
        if prompt.category is not None:
            updates.append("category = ?")
            params.append(prompt.category)
        if prompt.description is not None:
            updates.append("description = ?")
            params.append(prompt.description)
        if prompt.tags is not None:
            updates.append("tags = ?")
            params.append(json.dumps(prompt.tags))
        if prompt.parameters is not None:
            updates.append("parameters = ?")
            params.append(json.dumps(prompt.parameters))
        
        updates.append("updated_at = CURRENT_TIMESTAMP")
        params.append(prompt_id)
        
        query = f"UPDATE prompts SET {', '.join(updates)} WHERE id = ?"
        cursor.execute(query, params)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "æç¤ºè¯æ›´æ–°æˆåŠŸ"})
    except Exception as e:
        logger.exception(f"æ›´æ–°æç¤ºè¯å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/prompts/{prompt_id}")
async def delete_prompt(prompt_id: int):
    """åˆ é™¤æç¤ºè¯"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM prompts WHERE id = ?", (prompt_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "æç¤ºè¯ä¸å­˜åœ¨"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "æç¤ºè¯åˆ é™¤æˆåŠŸ"})
    except Exception as e:
        logger.exception(f"åˆ é™¤æç¤ºè¯å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/popular")
async def get_popular_prompts(limit: int = 10):
    """è·å–çƒ­é—¨æç¤ºè¯"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts ORDER BY usage_count DESC, updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        conn.close()
        
        return JSONResponse({"prompts": prompts})
    except Exception as e:
        logger.exception(f"è·å–çƒ­é—¨æç¤ºè¯å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/prompts/{prompt_id}/usage")
async def increment_prompt_usage(prompt_id: int):
    """å¢åŠ æç¤ºè¯ä½¿ç”¨æ¬¡æ•°"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("UPDATE prompts SET usage_count = usage_count + 1 WHERE id = ?", (prompt_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "æç¤ºè¯ä¸å­˜åœ¨"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "ä½¿ç”¨æ¬¡æ•°å·²æ›´æ–°"})
    except Exception as e:
        logger.exception(f"æ›´æ–°ä½¿ç”¨æ¬¡æ•°å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# æ–¹æ¡ˆç”Ÿæˆå™¨ API
# ============================================================================

class SolutionGenerate(BaseModel):
    requirement: str
    template_type: Optional[str] = None

@app.post("/api/solutions/generate")
async def generate_solution(request: Request, req: SolutionGenerate):
    """ç”Ÿæˆæ–¹æ¡ˆ"""
    try:
        project_name = request.query_params.get("project")
        if not project_name:
            return JSONResponse({"error": "ç¼ºå°‘é¡¹ç›®åç§°"}, status_code=400)
        
        project_path = get_project_path(project_name)
        logger.info(f"[generate_solution] é¡¹ç›®è·¯å¾„: {project_path}")
        
        # ä½¿ç”¨ iFlow Agent ç”Ÿæˆæ–¹æ¡ˆ
        agent = get_agent(project_path)
        logger.info(f"[generate_solution] Agent åˆ›å»ºæˆåŠŸ")
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æŠ€æœ¯æ–¹æ¡ˆï¼š

éœ€æ±‚ï¼š{req.requirement}
{f'æ¨¡æ¿ç±»å‹ï¼š{req.template_type}' if req.template_type else ''}

è¯·æä¾›ï¼š
1. æŠ€æœ¯æ ˆé€‰æ‹©
2. æ¶æ„è®¾è®¡
3. å®ç°æ­¥éª¤
4. å…³é”®ä»£ç ç¤ºä¾‹
5. æ³¨æ„äº‹é¡¹

è¯·ç”¨ Markdown æ ¼å¼è¾“å‡ºã€‚"""
        
        logger.info(f"[generate_solution] å¼€å§‹ç”Ÿæˆæ–¹æ¡ˆï¼Œéœ€æ±‚: {req.requirement}")
        
        solution_content = ""
        message_count = 0
        async for msg in agent.chat_stream(prompt):
            message_count += 1
            msg_type = msg.get("type")
            logger.debug(f"[generate_solution] æ”¶åˆ°æ¶ˆæ¯ {message_count}: {msg_type}, å®Œæ•´æ¶ˆæ¯: {msg}")
            
            # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
            if msg_type == "content":
                content = msg.get("content", "")
                solution_content += content
                logger.debug(f"[generate_solution] ç´¯è®¡å†…å®¹é•¿åº¦: {len(solution_content)}")
            elif msg_type == "text":
                content = msg.get("text", "")
                solution_content += content
                logger.debug(f"[generate_solution] ç´¯è®¡å†…å®¹é•¿åº¦: {len(solution_content)}")
            elif msg_type == "assistant":
                # assistant æ¶ˆæ¯å¯èƒ½åŒ…å«å†…å®¹
                if "content" in msg:
                    content = msg["content"]
                    if isinstance(content, str):
                        solution_content += content
                    elif isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict) and "text" in item:
                                solution_content += item["text"]
                    logger.debug(f"[generate_solution] ç´¯è®¡å†…å®¹é•¿åº¦: {len(solution_content)}")
            elif msg_type == "message":
                # message ç±»å‹
                content = msg.get("message", "")
                solution_content += content
                logger.debug(f"[generate_solution] ç´¯è®¡å†…å®¹é•¿åº¦: {len(solution_content)}")
        
        logger.info(f"[generate_solution] ç”Ÿæˆå®Œæˆï¼Œå…± {message_count} æ¡æ¶ˆæ¯ï¼Œå†…å®¹é•¿åº¦: {len(solution_content)}")
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO solutions (requirement, solution, template_type)
            VALUES (?, ?, ?)
        ''', (req.requirement, solution_content, req.template_type))
        solution_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        logger.info(f"[generate_solution] æ–¹æ¡ˆå·²ä¿å­˜ï¼ŒID: {solution_id}")
        
        return JSONResponse({
            "id": solution_id,
            "requirement": req.requirement,
            "solution": solution_content,
            "template_type": req.template_type
        })
    except Exception as e:
        logger.exception(f"ç”Ÿæˆæ–¹æ¡ˆå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/solutions/generate-stream")
async def generate_solution_stream(request: Request, req: SolutionGenerate):
    """æµå¼ç”Ÿæˆæ–¹æ¡ˆ"""
    async def event_generator():
        try:
            project_name = request.query_params.get("project")
            if not project_name:
                yield f"data: {json.dumps({'error': 'ç¼ºå°‘é¡¹ç›®åç§°'})}\n\n"
                return
            
            project_path = get_project_path(project_name)
            logger.info(f"[generate_solution_stream] é¡¹ç›®è·¯å¾„: {project_path}")
            
            agent = get_agent(project_path)
            logger.info(f"[generate_solution_stream] Agent åˆ›å»ºæˆåŠŸ")
            
            prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æŠ€æœ¯æ–¹æ¡ˆï¼š

éœ€æ±‚ï¼š{req.requirement}
{f'æ¨¡æ¿ç±»å‹ï¼š{req.template_type}' if req.template_type else ''}

è¯·æä¾›ï¼š
1. æŠ€æœ¯æ ˆé€‰æ‹©
2. æ¶æ„è®¾è®¡
3. å®ç°æ­¥éª¤
4. å…³é”®ä»£ç ç¤ºä¾‹
5. æ³¨æ„äº‹é¡¹

è¯·ç”¨ Markdown æ ¼å¼è¾“å‡ºã€‚"""
            
            logger.info(f"[generate_solution_stream] å¼€å§‹ç”Ÿæˆæ–¹æ¡ˆï¼Œéœ€æ±‚: {req.requirement}")
            
            solution_content = ""
            message_count = 0
            async for msg in agent.chat_stream(prompt):
                message_count += 1
                msg_type = msg.get("type")
                logger.debug(f"[generate_solution_stream] æ”¶åˆ°æ¶ˆæ¯ {message_count}: {msg_type}")
                
                # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
                if msg_type == "content":
                    content = msg.get("content", "")
                    solution_content += content
                    # æµå¼å‘é€å†…å®¹
                    yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                elif msg_type == "text":
                    content = msg.get("text", "")
                    solution_content += content
                    yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                elif msg_type == "assistant":
                    if "content" in msg:
                        content = msg["content"]
                        if isinstance(content, str):
                            solution_content += content
                            yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                        elif isinstance(content, list):
                            for item in content:
                                if isinstance(item, dict) and "text" in item:
                                    solution_content += item["text"]
                                    yield f"data: {json.dumps({'type': 'content', 'content': item['text']})}\n\n"
            
            logger.info(f"[generate_solution_stream] ç”Ÿæˆå®Œæˆï¼Œå…± {message_count} æ¡æ¶ˆæ¯ï¼Œå†…å®¹é•¿åº¦: {len(solution_content)}")
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO solutions (requirement, solution, template_type)
                VALUES (?, ?, ?)
            ''', (req.requirement, solution_content, req.template_type))
            solution_id = cursor.lastrowid
            conn.commit()
            conn.close()
            
            logger.info(f"[generate_solution_stream] æ–¹æ¡ˆå·²ä¿å­˜ï¼ŒID: {solution_id}")
            
            # å‘é€å®Œæˆäº‹ä»¶
            yield f"data: {json.dumps({'type': 'done', 'solution_id': solution_id, 'solution': solution_content})}\n\n"
            
        except Exception as e:
            logger.exception(f"[generate_solution_stream] ç”Ÿæˆæ–¹æ¡ˆå¤±è´¥: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

@app.get("/api/solutions")
async def get_solutions(limit: int = 10):
    """è·å–å·²ä¿å­˜çš„æ–¹æ¡ˆ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM solutions ORDER BY created_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        solutions = [dict(row) for row in rows]
        conn.close()
        
        return JSONResponse({"solutions": solutions})
    except Exception as e:
        logger.exception(f"è·å–æ–¹æ¡ˆå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/solutions/templates")
async def get_solution_templates():
    """è·å–æ–¹æ¡ˆæ¨¡æ¿åˆ—è¡¨"""
    try:
        templates = [
            {
                "id": "web-app",
                "name": "Web åº”ç”¨å¼€å‘",
                "description": "é€‚ç”¨äº Web åº”ç”¨å¼€å‘çš„æŠ€æœ¯æ–¹æ¡ˆæ¨¡æ¿",
                "icon": "ğŸŒ"
            },
            {
                "id": "mobile-app",
                "name": "ç§»åŠ¨åº”ç”¨å¼€å‘",
                "description": "é€‚ç”¨äºç§»åŠ¨åº”ç”¨å¼€å‘çš„æŠ€æœ¯æ–¹æ¡ˆæ¨¡æ¿",
                "icon": "ğŸ“±"
            },
            {
                "id": "api-service",
                "name": "API æœåŠ¡å¼€å‘",
                "description": "é€‚ç”¨äº API æœåŠ¡å¼€å‘çš„æŠ€æœ¯æ–¹æ¡ˆæ¨¡æ¿",
                "icon": "ğŸ”Œ"
            },
            {
                "id": "data-analysis",
                "name": "æ•°æ®åˆ†æå¹³å°",
                "description": "é€‚ç”¨äºæ•°æ®åˆ†æå¹³å°çš„æŠ€æœ¯æ–¹æ¡ˆæ¨¡æ¿",
                "icon": "ğŸ“Š"
            },
            {
                "id": "microservices",
                "name": "å¾®æœåŠ¡æ¶æ„",
                "description": "é€‚ç”¨äºå¾®æœåŠ¡æ¶æ„çš„æŠ€æœ¯æ–¹æ¡ˆæ¨¡æ¿",
                "icon": "ğŸ”—"
            }
        ]
        
        return JSONResponse({"templates": templates})
    except Exception as e:
        logger.exception(f"è·å–æ–¹æ¡ˆæ¨¡æ¿å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/solutions/{solution_id}")
async def get_solution(solution_id: int):
    """è·å–å•ä¸ªæ–¹æ¡ˆ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM solutions WHERE id = ?", (solution_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return JSONResponse({"error": "æ–¹æ¡ˆä¸å­˜åœ¨"}, status_code=404)
        
        solution = dict(row)
        conn.close()
        
        return JSONResponse(solution)
    except Exception as e:
        logger.exception(f"è·å–æ–¹æ¡ˆå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# ä¸šåŠ¡æµç¨‹æ€»ç»“ API
# ============================================================================

@app.get("/api/business-flow/summary")
async def get_business_flow_summary(request: Request, limit: int = 50):
    """è·å–ä¸šåŠ¡æµç¨‹æ€»ç»“"""
    try:
        project_name = request.query_params.get("project")
        if not project_name:
            return JSONResponse({"error": "ç¼ºå°‘é¡¹ç›®åç§°"}, status_code=400)
        
        project_path = get_project_path(project_name)
        
        # è·å– Git å†å²
        import subprocess
        result = subprocess.run(
            ["git", "log", "--pretty=format:%H|%an|%ae|%ad|%s", f"-{limit}", "--date=iso"],
            cwd=project_path,
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            return JSONResponse({"error": "æ— æ³•è·å– Git å†å²"}, status_code=500)
        
        commits = []
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split('|', 4)
                if len(parts) == 5:
                    commits.append({
                        "hash": parts[0],
                        "author": parts[1],
                        "email": parts[2],
                        "date": parts[3],
                        "message": parts[4]
                    })
        
        # ä½¿ç”¨ AI æ€»ç»“ä¸šåŠ¡æµç¨‹
        agent = get_agent(project_path)
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹ Git æäº¤å†å²ï¼Œæ€»ç»“é¡¹ç›®çš„ä¸šåŠ¡æµç¨‹å’ŒåŠŸèƒ½æ¼”è¿›ï¼š

{json.dumps(commits[:20], ensure_ascii=False, indent=2)}

è¯·æä¾›ï¼š
1. ä¸»è¦åŠŸèƒ½æ¨¡å—
2. ä¸šåŠ¡æµç¨‹å›¾
3. å…³é”®é‡Œç¨‹ç¢‘
4. æŠ€æœ¯æ¼”è¿›

è¯·ç”¨ Markdown æ ¼å¼è¾“å‡ºã€‚"""
        
        summary_content = ""
        async for msg in agent.chat_stream(prompt):
            if msg.get("type") == "content":
                summary_content += msg.get("content", "")
        
        return JSONResponse({
            "business_flow": summary_content,
            "commits": commits
        })
    except Exception as e:
        logger.exception(f"è·å–ä¸šåŠ¡æµç¨‹æ€»ç»“å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/business-flow/timeline")
async def get_business_flow_timeline(request: Request, limit: int = 100):
    """è·å–ä¸šåŠ¡æµç¨‹æ—¶é—´çº¿"""
    try:
        project_name = request.query_params.get("project")
        if not project_name:
            return JSONResponse({"error": "ç¼ºå°‘é¡¹ç›®åç§°"}, status_code=400)
        
        project_path = get_project_path(project_name)
        
        # è·å– Git å†å²
        import subprocess
        result = subprocess.run(
            ["git", "log", "--pretty=format:%H|%an|%ad|%s", f"-{limit}", "--date=iso"],
            cwd=project_path,
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            return JSONResponse({"error": "æ— æ³•è·å– Git å†å²"}, status_code=500)
        
        timeline = []
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split('|', 3)
                if len(parts) == 4:
                    timeline.append({
                        "hash": parts[0],
                        "author": parts[1],
                        "date": parts[2],
                        "message": parts[3]
                    })
        
        return JSONResponse({"timeline": timeline})
    except Exception as e:
        logger.exception(f"è·å–ä¸šåŠ¡æµç¨‹æ—¶é—´çº¿å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# ä»£ç å®¡æŸ¥ API
# ============================================================================

class CodeReviewRequest(BaseModel):
    project_name: str
    file_path: str
    check_types: List[str] = ["quality", "style", "security", "performance"]

@app.post("/api/review/code")
async def review_code(req: CodeReviewRequest):
    """å®¡æŸ¥ä»£ç """
    try:
        project_path = get_project_path(req.project_name)
        file_path = os.path.join(project_path, req.file_path)
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        if not os.path.exists(file_path):
            return JSONResponse({"error": "æ–‡ä»¶ä¸å­˜åœ¨"}, status_code=404)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä½¿ç”¨ AI å®¡æŸ¥ä»£ç 
        agent = get_agent(project_path)
        
        check_types_str = ", ".join(req.check_types)
        prompt = f"""è¯·å¯¹ä»¥ä¸‹ä»£ç è¿›è¡Œä»£ç å®¡æŸ¥ï¼Œæ£€æŸ¥ä»¥ä¸‹æ–¹é¢ï¼š{check_types_str}

æ–‡ä»¶è·¯å¾„ï¼š{req.file_path}

ä»£ç å†…å®¹ï¼š
```
{content}
```

è¯·æä¾›ï¼š
1. å‘ç°çš„é—®é¢˜ï¼ˆæŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»ï¼‰
2. æ”¹è¿›å»ºè®®
3. æœ€ä½³å®è·µå»ºè®®

è¯·ç”¨ JSON æ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "summary": {{"total_issues": 0, "by_severity": {{"critical": 0, "high": 0, "medium": 0, "low": 0}}}},
  "issues": [
    {{
      "id": "1",
      "severity": "high",
      "category": "quality",
      "message": "é—®é¢˜æè¿°",
      "line": 10,
      "suggestion": "æ”¹è¿›å»ºè®®"
    }}
  ]
}}"""
        
        review_result = ""
        async for msg in agent.chat_stream(prompt):
            if msg.get("type") == "content":
                review_result += msg.get("content", "")
        
        # å°è¯•è§£æ JSON
        try:
            # æå– JSON éƒ¨åˆ†
            json_start = review_result.find('{')
            json_end = review_result.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = review_result[json_start:json_end]
                review_data = json.loads(json_str)
            else:
                # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›åŸå§‹æ–‡æœ¬
                review_data = {
                    "summary": {"total_issues": 0, "by_severity": {}},
                    "issues": [],
                    "raw_output": review_result
                }
        except:
            review_data = {
                "summary": {"total_issues": 0, "by_severity": {}},
                "issues": [],
                "raw_output": review_result
            }
        
        return JSONResponse(review_data)
    except Exception as e:
        logger.exception(f"ä»£ç å®¡æŸ¥å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

from backend.core.business_flow_summarizer import business_flow_summarizer

# ============================================================================
# ä¸šåŠ¡æµç¨‹æ€»ç»“ API
# ============================================================================

@app.get("/api/business-flow/summary")
async def get_business_flow_summary(limit: int = 50):
    """è·å–ä¸šåŠ¡æµç¨‹æ€»ç»“"""
    try:
        # ç¡®ä¿ limit æ˜¯åˆç†çš„æ•´æ•°
        if limit < 1:
            limit = 50
        if limit > 500:
            limit = 500
            
        result = business_flow_summarizer.generate_business_flow(limit)
        return {"success": True, "business_flow": result}
    except Exception as e:
        logger.exception(f"è·å–ä¸šåŠ¡æµç¨‹æ€»ç»“å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/business-flow/stats")
async def get_business_flow_stats():
    """è·å–ä¸šåŠ¡æµç¨‹ç»Ÿè®¡"""
    try:
        # å¤ç”¨ generate_business_flow çš„ç»“æœï¼Œæˆ–è€…å®ç°ä¸“é—¨çš„ stats æ–¹æ³•
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œç›´æ¥å¤ç”¨ summary çš„ summary éƒ¨åˆ†
        result = business_flow_summarizer.generate_business_flow(limit=1)
        return {"success": True, "stats": result.get("summary", {})}
    except Exception as e:
        logger.exception(f"è·å–ä¸šåŠ¡æµç¨‹ç»Ÿè®¡å¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# æ™ºèƒ½éœ€æ±‚åˆ†æ API
# ============================================================================

class RequirementAnalysisRequest(BaseModel):
    text: str
    image_path: Optional[str] = None
    project_name: Optional[str] = None

class MatchModulesRequest(BaseModel):
    keywords: List[str]
    project_name: Optional[str] = None

class GenerateSolutionRequest(BaseModel):
    analysis: Dict[str, Any]
    matched_modules: List[Dict[str, Any]]
    project_root: str = "."

class OptimizeRequirementRequest(BaseModel):
    text: str
    project_name: str = ""

class ProjectOptimizationRequest(BaseModel):
    focus: str = ""
    project_name: str

@app.post("/api/smart-requirement/optimize-project")
async def optimize_project(req: ProjectOptimizationRequest):
    try:
        project_path = get_project_path(req.project_name)
        result = await smart_requirement_service.analyze_project_optimization(req.focus, project_path)
        return {"success": True, "result": result}
    except Exception as e:
        logger.exception(f"Project Optimization failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/optimize")
async def optimize_requirement(req: OptimizeRequirementRequest):
    try:
        result = await smart_requirement_service.optimize_requirement(req.text, req.project_name)
        return {"success": True, "result": result}
    except Exception as e:
        logger.exception(f"Optimization failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/step1-analyze")
async def analyze_requirement_step1(req: RequirementAnalysisRequest):
    try:
        project_path = get_project_path(req.project_name)
        analysis = await smart_requirement_service.analyze_requirement(req.text, req.image_path, project_path)
        return {"success": True, "analysis": analysis}
    except Exception as e:
        logger.exception(f"Step 1 failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/step2-match")
async def match_modules_step2(req: MatchModulesRequest):
    try:
        project_path = get_project_path(req.project_name)
        matched_modules = await smart_requirement_service.match_modules(req.keywords, project_path)
        return {"success": True, "matched_modules": matched_modules}
    except Exception as e:
        logger.exception(f"Step 2 failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

class RefineSolutionRequest(BaseModel):
    previous_solution: Dict[str, Any]
    feedback: str

@app.post("/api/smart-requirement/refine")
async def refine_solution(req: RefineSolutionRequest):
    try:
        updated_solution = await smart_requirement_service.refine_solution(req.previous_solution, req.feedback)
        return {"success": True, "updated_solution": updated_solution}
    except Exception as e:
        logger.exception(f"Refinement failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/step2-5-context")
async def generate_business_context(req: GenerateSolutionRequest):
    try:
        # We reuse GenerateSolutionRequest but only need matched_modules
        # Assuming project_root is accessible or passed. Here we use "."
        project_root = "." 
        context_data = await smart_requirement_service.generate_business_context(req.matched_modules, project_root)
        return {
            "success": True, 
            "context": context_data
        }
    except Exception as e:
        logger.exception(f"Step 2.5 failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/step3-solution")
async def generate_solution_step3(req: GenerateSolutionRequest):
    try:
        solution_data = await smart_requirement_service.generate_solution(req.analysis, req.matched_modules)
        return {
            "success": True, 
            "solution_doc": solution_data.get("solution_doc", ""),
            "execution_plan": solution_data.get("execution_plan", {}),
            "api_design": solution_data.get("api_design", [])
        }
    except Exception as e:
        logger.exception(f"Step 3 failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/project/file-content")
async def get_project_file_content(path: str, project_name: Optional[str] = None):
    """Securely read a file from the project directory"""
    try:
        # Determine root
        if project_name:
            root = get_project_path(project_name)
        else:
            root = os.getcwd() # Fallback to current working dir if no project specified
        
        # Sanitize path to prevent directory traversal
        safe_path = os.path.normpath(os.path.join(root, path))
        if not safe_path.startswith(os.path.abspath(root)):
             return JSONResponse({"error": "Access denied: Path outside project root"}, status_code=403)
             
        if not os.path.exists(safe_path) or not os.path.isfile(safe_path):
             return JSONResponse({"error": "File not found"}, status_code=404)
             
        # Read content (limit size)
        file_size = os.path.getsize(safe_path)
        if file_size > 1024 * 1024: # 1MB limit
             return JSONResponse({"error": "File too large to preview"}, status_code=400)
             
        with open(safe_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            
        return {"content": content, "path": path, "size": file_size}
    except Exception as e:
        logger.error(f"Error reading file {path}: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

class SaveAnalysisRequest(BaseModel):
    project_name: str
    title: str
    content: str
    folder: str = "docs/requirements"

@app.post("/api/smart-requirement/save")
async def save_analysis_doc(req: SaveAnalysisRequest):
    """Save analysis result as a markdown file"""
    try:
        root = get_project_path(req.project_name)
        target_dir = os.path.join(root, req.folder)
        os.makedirs(target_dir, exist_ok=True)
        
        # Sanitize filename
        safe_title = "".join([c for c in req.title if c.isalnum() or c in (' ', '-', '_')]).strip()
        safe_title = safe_title.replace(' ', '_')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{safe_title}_{timestamp}.md"
        
        file_path = os.path.join(target_dir, filename)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(req.content)
            
        return {"success": True, "path": f"{req.folder}/{filename}"}
    except Exception as e:
        logger.exception(f"Failed to save analysis: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/analyze")
async def analyze_smart_requirement(req: RequirementAnalysisRequest):
    """æ™ºèƒ½åˆ†æéœ€æ±‚"""
    try:
        project_path = get_project_path(req.project_name)
        
        # 1. Analyze Requirement
        analysis = await smart_requirement_service.analyze_requirement(req.text, req.image_path, project_path)
        
        # 2. Match Modules
        keywords = analysis.get("keywords", [])
        matched_modules = await smart_requirement_service.match_modules(keywords, project_path)
        
        # 3. Generate Solution
        solution_data = await smart_requirement_service.generate_solution(analysis, matched_modules)
        
        return {
            "success": True,
            "analysis": analysis,
            "matched_modules": matched_modules,
            "solution_doc": solution_data.get("solution_doc", ""),
            "execution_plan": solution_data.get("execution_plan", {})
        }
    except Exception as e:
        logger.exception(f"æ™ºèƒ½éœ€æ±‚åˆ†æå¤±è´¥: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# CI/CD Generator API
# ============================================================================

@app.get("/api/cicd/platforms")
async def get_cicd_platforms():
    """Get supported CI/CD platforms"""
    return {
        "success": True,
        "platforms": [
            {"id": "github", "name": "GitHub Actions", "icon": "github"},
            {"id": "gitlab", "name": "GitLab CI", "icon": "gitlab"},
            {"id": "jenkins", "name": "Jenkins", "icon": "jenkins"}
        ]
    }

class CICDGenerateRequest(BaseModel):
    platform: str
    projectType: str
    projectName: str
    config: Optional[Dict[str, Any]] = None

@app.post("/api/cicd/generate")
async def generate_cicd_config(req: CICDGenerateRequest):
    """Generate CI/CD configuration files"""
    try:
        result = cicd_generator.generate(
            req.platform, 
            req.projectType, 
            req.projectName, 
            req.config
        )
        return result
    except Exception as e:
        logger.exception(f"CI/CD generation failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


# ============================================================================
# Project Template API
# ============================================================================

@app.get("/api/templates")
async def get_project_templates():
    """Get available project templates"""
    service = get_project_template_service()
    return service.get_templates()

class TemplateGenerateRequest(BaseModel):
    templateId: str
    projectName: str
    path: str
    config: Optional[Dict[str, Any]] = None

@app.post("/api/templates/generate")
async def generate_project_from_template(req: TemplateGenerateRequest):
    """Generate a new project from a template"""
    try:
        service = get_project_template_service()
        # Verify path exists
        if not os.path.exists(req.path):
            return JSONResponse({"error": "Target path does not exist"}, status_code=400)
            
        result = service.generate_project(
            req.templateId,
            req.projectName,
            req.path,
            req.config
        )
        return result
    except Exception as e:
        logger.exception(f"Template generation failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


# ============================================================================
# TaskMaster API (Real Implementation)
# ============================================================================

@app.get("/api/taskmaster/tasks/{project_name}")
async def get_project_tasks(project_name: str):
    """Get tasks for a project"""
    tasks = task_master_service.get_tasks(project_name)
    return {"tasks": tasks}

@app.post("/api/taskmaster/tasks/{project_name}")
async def create_project_task(project_name: str, task: TaskModel):
    """Create a new task"""
    try:
        # Ensure project name matches
        task.project_name = project_name 
        created_task = task_master_service.create_task(task)
        return {"success": True, "task": created_task}
    except Exception as e:
        logger.error(f"Error creating task: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.put("/api/taskmaster/tasks/{project_name}/{task_id}")
async def update_project_task(project_name: str, task_id: str, updates: Dict[str, Any]):
    """Update a task"""
    try:
        updated_task = task_master_service.update_task(task_id, updates)
        if not updated_task:
             return JSONResponse({"error": "Task not found"}, status_code=404)
        return {"success": True, "task": updated_task}
    except Exception as e:
        logger.error(f"Error updating task: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/taskmaster/tasks/{project_name}/{task_id}")
async def delete_project_task(project_name: str, task_id: str):
    """Delete a task"""
    success = task_master_service.delete_task(task_id)
    return {"success": success}

@app.get("/api/taskmaster/prd/{project_name}")
async def get_project_prds(project_name: str):
    """Get list of PRDs for a project"""
    prds = task_master_service.get_prds(project_name)
    return prds

@app.get("/api/taskmaster/prd/{project_name}/{prd_name}")
async def get_prd_details(project_name: str, prd_name: str):
    """Get PRD content"""
    prd = task_master_service.get_prd_content(project_name, prd_name)
    if not prd:
        return JSONResponse({"error": "PRD not found"}, status_code=404)
    return prd

class PRDSaveRequest(BaseModel):
    title: str
    content: str

@app.post("/api/taskmaster/prd/{project_name}")
async def save_project_prd(project_name: str, req: PRDSaveRequest):
    """Save/Update a PRD"""
    try:
        saved_prd = task_master_service.save_prd(project_name, req.title, req.content)
        return {"success": True, "prd": saved_prd}
    except Exception as e:
         logger.error(f"Error saving PRD: {e}")
         return JSONResponse({"error": str(e)}, status_code=500)

# --- Database Query API ---
  
class DatabaseConnectRequest(BaseModel):
    db_type: str = "sqlite"  # sqlite, mysql, postgresql, sqlserver, oracle
    db_path: Optional[str] = None  # SQLite ä¸“ç”¨
    host: Optional[str] = None
    port: Optional[int] = None
    database: Optional[str] = None
    username: Optional[str] = None
    password: Optional[str] = None
    connection_name: Optional[str] = None

@app.post("/api/database/connect")
async def connect_database(req: DatabaseConnectRequest):
    """è¿æ¥æ•°æ®åº“ï¼ˆæ”¯æŒå¤šç§ç±»å‹ï¼‰"""
    try:
        success = False
        
        if req.db_type == "sqlite":
            if not req.db_path:
                return JSONResponse({"error": "db_path is required for SQLite"}, status_code=400)
            success = database_query_service.connect_sqlite(req.db_path, req.connection_name)
        elif req.db_type == "mysql":
            if not all([req.host, req.port, req.database, req.username, req.password]):
                return JSONResponse({"error": "host, port, database, username, password are required for MySQL"}, status_code=400)
            success = database_query_service.connect_mysql(
                req.host, req.port, req.database, req.username, req.password, req.connection_name
            )
        elif req.db_type == "postgresql":
            if not all([req.host, req.port, req.database, req.username, req.password]):
                return JSONResponse({"error": "host, port, database, username, password are required for PostgreSQL"}, status_code=400)
            success = database_query_service.connect_postgresql(
                req.host, req.port, req.database, req.username, req.password, req.connection_name
            )
        elif req.db_type == "sqlserver":
            if not all([req.host, req.port, req.database, req.username, req.password]):
                return JSONResponse({"error": "host, port, database, username, password are required for SQL Server"}, status_code=400)
            success = database_query_service.connect_sqlserver(
                req.host, req.port, req.database, req.username, req.password, req.connection_name
            )
        elif req.db_type == "oracle":
            if not all([req.host, req.port, req.database, req.username, req.password]):
                return JSONResponse({"error": "host, port, database, username, password are required for Oracle"}, status_code=400)
            success = database_query_service.connect_oracle(
                req.host, req.port, req.database, req.username, req.password, req.connection_name
            )
        else:
            return JSONResponse({"error": f"Unsupported database type: {req.db_type}"}, status_code=400)
        
        if success:
            return {"success": True, "message": "Database connected successfully"}
        else:
            return JSONResponse({"error": "Failed to connect to database"}, status_code=400)
    except Exception as e:
        logger.error(f"Error connecting to database: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)
@app.post("/api/database/disconnect/{connection_name}")
async def disconnect_database(connection_name: str):
    """æ–­å¼€æ•°æ®åº“è¿æ¥"""
    try:
        success = database_query_service.disconnect(connection_name)
        if success:
            return {"success": True, "message": "Database disconnected successfully"}
        else:
            return JSONResponse({"error": "Connection not found"}, status_code=404)
    except Exception as e:
        logger.error(f"Error disconnecting database: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/connections")
async def get_database_connections():
    """è·å–æ‰€æœ‰æ•°æ®åº“è¿æ¥"""
    try:
        connections = database_query_service.get_connections()
        return {"connections": connections}
    except Exception as e:
        logger.error(f"Error getting connections: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/tables/{connection_name}")
async def get_database_tables(connection_name: str):
    """è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨"""
    try:
        tables = database_query_service.get_tables(connection_name)
        return {"tables": tables}
    except Exception as e:
        logger.error(f"Error getting tables: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/table/{connection_name}/{table_name}")
async def get_table_info(connection_name: str, table_name: str):
      """è·å–è¡¨çš„è¯¦ç»†ä¿¡æ¯"""
      try:
          logger.info(f"Getting table info - connection: {connection_name}, table: {table_name}")
          table_info = database_query_service.get_table_info(connection_name, table_name)
          if table_info:
              return {
                  "name": table_info.name,
                  "type": table_info.type,
                  "row_count": table_info.row_count,
                  "columns": table_info.columns,
                  "indexes": table_info.indexes
              }
          else:
              logger.warning(f"Table not found: {table_name}")
              return JSONResponse({"error": "Table not found"}, status_code=404)
      except Exception as e:
          logger.error(f"Error getting table info: {e}")
          return JSONResponse({"error": str(e)}, status_code=500)
class DatabaseQueryRequest(BaseModel):
    connection_name: str
    sql: str
    params: Optional[Dict[str, Any]] = None

@app.post("/api/database/query")
async def execute_database_query(req: DatabaseQueryRequest):
    """æ‰§è¡Œ SQL æŸ¥è¯¢"""
    try:
        result = database_query_service.execute_query(req.connection_name, req.sql, req.params)
        if result.success:
            return {
                "success": True,
                "columns": result.columns,
                "rows": result.rows,
                "row_count": result.row_count,
                "execution_time": result.execution_time
            }
        else:
            return JSONResponse({"error": result.error}, status_code=400)
    except Exception as e:
        logger.error(f"Error executing query: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/export/{connection_name}/{format}")
async def export_query_result(connection_name: str, format: str, sql: str, params: Optional[str] = None):
    """å¯¼å‡ºæŸ¥è¯¢ç»“æœ"""
    try:
        params_dict = json.loads(params) if params else None
        
        if format == "csv":
            data = database_query_service.export_to_csv(connection_name, sql, params_dict)
            return Response(content=data, media_type="text/csv", headers={
                "Content-Disposition": f"attachment; filename=query_result.csv"
            })
        elif format == "json":
            data = database_query_service.export_to_json(connection_name, sql, params_dict)
            return Response(content=data, media_type="application/json", headers={
                "Content-Disposition": f"attachment; filename=query_result.json"
            })
        elif format == "excel":
            data = database_query_service.export_to_excel(connection_name, sql, params_dict)
            return Response(content=data, media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", headers={
                "Content-Disposition": f"attachment; filename=query_result.xlsx"
            })
        else:
            return JSONResponse({"error": "Unsupported format. Use csv, json, or excel"}, status_code=400)
    except Exception as e:
        logger.error(f"Error exporting query result: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/templates")
async def get_query_templates():
    """è·å–æŸ¥è¯¢æ¨¡æ¿"""
    try:
        templates = database_query_service.get_query_templates()
        return {"templates": templates}
    except Exception as e:
        logger.error(f"Error getting templates: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

class AddTemplateRequest(BaseModel):
    name: str
    sql: str
    description: Optional[str] = ""
    category: Optional[str] = "è‡ªå®šä¹‰"
    params: Optional[List[str]] = None

@app.post("/api/database/templates")
async def add_query_template(req: AddTemplateRequest):
    """æ·»åŠ æŸ¥è¯¢æ¨¡æ¿"""
    try:
        template = database_query_service.add_query_template(
            req.name, req.sql, req.description, req.category, req.params
        )
        return {"success": True, "template": template}
    except Exception as e:
        logger.error(f"Error adding template: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/history")
async def get_query_history(limit: int = 50):
    """è·å–æŸ¥è¯¢å†å²"""
    try:
        history = database_query_service.get_query_history(limit)
        return {"history": history}
    except Exception as e:
        logger.error(f"Error getting query history: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

class DatabaseConfigRequest(BaseModel):
    project_name: str
    config_name: str
    db_type: str
    config: Dict[str, Any]

@app.post("/api/database/save-config")
async def save_database_config(req: DatabaseConfigRequest):
    """ä¿å­˜æ•°æ®åº“é…ç½®åˆ°é¡¹ç›®"""
    try:
        import os
        import json
        
        project_path = get_project_path(req.project_name)
        if not project_path:
            return JSONResponse({"error": "Project not found"}, status_code=404)
        
        # åˆ›å»ºæ•°æ®åº“é…ç½®ç›®å½•
        config_dir = os.path.join(project_path, ".database")
        os.makedirs(config_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config_file = os.path.join(config_dir, f"{req.config_name}.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump({
                "name": req.config_name,
                "db_type": req.db_type,
                "config": req.config,
                "created_at": datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved database config: {req.config_name} for project: {req.project_name}")
        return {"success": True, "message": "Database config saved successfully"}
    except Exception as e:
        logger.error(f"Error saving database config: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/configs/{project_name}")
async def get_database_configs(project_name: str):
    """è·å–é¡¹ç›®çš„æ•°æ®åº“é…ç½®åˆ—è¡¨"""
    try:
        import os
        import json
        
        project_path = get_project_path(project_name)
        if not project_path:
            return JSONResponse({"error": "Project not found"}, status_code=404)
        
        config_dir = os.path.join(project_path, ".database")
        configs = []
        
        if os.path.exists(config_dir):
            for filename in os.listdir(config_dir):
                if filename.endswith('.json'):
                    config_file = os.path.join(config_dir, filename)
                    try:
                        with open(config_file, 'r', encoding='utf-8') as f:
                            config = json.load(f)
                            configs.append(config)
                    except Exception as e:
                        logger.error(f"Error loading config {filename}: {e}")
        
        return {"configs": configs}
    except Exception as e:
        logger.error(f"Error getting database configs: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/database/config/{project_name}/{config_name}")
async def delete_database_config(project_name: str, config_name: str):
    """åˆ é™¤æ•°æ®åº“é…ç½®"""
    try:
        import os
        
        project_path = get_project_path(project_name)
        if not project_path:
            return JSONResponse({"error": "Project not found"}, status_code=404)
        
        config_file = os.path.join(project_path, ".database", f"{config_name}.json")
        
        if os.path.exists(config_file):
            os.remove(config_file)
            logger.info(f"Deleted database config: {config_name} for project: {project_name}")
            return {"success": True, "message": "Database config deleted successfully"}
        else:
            return JSONResponse({"error": "Config not found"}, status_code=404)
    except Exception as e:
        logger.error(f"Error deleting database config: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

def parse_gorm_dsn(dsn: str) -> dict:
    """è§£æ GORM DSN æ ¼å¼çš„è¿æ¥å­—ç¬¦ä¸²
    
    æ”¯æŒæ ¼å¼:
    - mysql:user:password@tcp(host:port)/database
    - postgresql://user:password@host:port/database
    """
    import re
    
    result = {
        'type': 'unknown',
        'host': '',
        'port': '',
        'user': '',
        'password': '',
        'database': ''
    }
    
    try:
        # MySQL æ ¼å¼: mysql:user:password@tcp(host:port)/database
        if dsn.startswith('mysql:'):
            result['type'] = 'mysql'
            # ç§»é™¤ mysql: å‰ç¼€
            dsn = dsn[6:]
            
            # è§£æ user:password@tcp(host:port)/database
            match = re.match(r'([^:]*):([^@]*)@tcp\(([^:]+):(\d+)\)/(.+)', dsn)
            if match:
                result['user'] = match.group(1)
                result['password'] = match.group(2)
                result['host'] = match.group(3)
                result['port'] = int(match.group(4))
                result['database'] = match.group(5)
                logger.info(f"è§£æ MySQL DSN æˆåŠŸ: {result}")
        
        # PostgreSQL æ ¼å¼: postgresql://user:password@host:port/database
        elif dsn.startswith('postgresql://'):
            result['type'] = 'postgresql'
            # ç§»é™¤ postgresql:// å‰ç¼€
            dsn = dsn[11:]
            
            # è§£æ user:password@host:port/database
            match = re.match(r'([^:]*):([^@]*)@([^:]+):(\d+)/(.+)', dsn)
            if match:
                result['user'] = match.group(1)
                result['password'] = match.group(2)
                result['host'] = match.group(3)
                result['port'] = int(match.group(4))
                result['database'] = match.group(5)
                logger.info(f"è§£æ PostgreSQL DSN æˆåŠŸ: {result}")
        
        # ç®€å•æ ¼å¼: user:password@host:port/database
        elif '@' in dsn and '/' in dsn:
            match = re.match(r'([^:]*):([^@]*)@([^:]+):(\d+)/(.+)', dsn)
            if match:
                result['user'] = match.group(1)
                result['password'] = match.group(2)
                result['host'] = match.group(3)
                result['port'] = int(match.group(4))
                result['database'] = match.group(5)
                logger.info(f"è§£æç®€å• DSN æˆåŠŸ: {result}")
        
        return result
    except Exception as e:
        logger.error(f"è§£æ DSN å¤±è´¥: {e}")
        return result


def parse_database_config(config_data: dict, config_type: str) -> list:
    """ä»é…ç½®æ•°æ®ä¸­è§£ææ•°æ®åº“è¿æ¥ä¿¡æ¯"""
    db_connections = []
    
    try:
        logger.info(f"å¼€å§‹è§£æé…ç½®æ•°æ®ï¼Œç±»å‹: {config_type}")
        logger.info(f"é…ç½®æ•°æ®é”®: {list(config_data.keys())}")
        
        # å¸¸è§çš„æ•°æ®åº“é…ç½®é”®å
        db_keys = ['database', 'db', 'sql', 'mysql', 'postgres', 'postgresql', 'mongodb', 'redis']
        
        def extract_db_info(data, prefix=''):
            """é€’å½’æå–æ•°æ®åº“ä¿¡æ¯"""
            if isinstance(data, dict):
                for key, value in data.items():
                    key_lower = key.lower()
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°æ®åº“é…ç½®
                    if any(db_key in key_lower for db_key in db_keys):
                        if isinstance(value, dict):
                            db_info = {
                                'name': key,
                                'type': 'unknown',
                                'config': {}
                            }
                            
                            logger.info(f"æ‰¾åˆ°æ•°æ®åº“é…ç½®: {key}, å€¼: {value}")
                            
                            # å°è¯•è¯†åˆ«æ•°æ®åº“ç±»å‹
                            for db_type in ['mysql', 'postgres', 'postgresql', 'mongodb', 'redis', 'sqlite']:
                                if db_type in key_lower:
                                    db_info['type'] = db_type
                                    break
                            
                            # æå–è¿æ¥å‚æ•°
                            for param in ['host', 'port', 'user', 'username', 'password', 'database', 'dbname', 'name', 'path', 'dsn', 'url', 'address']:
                                if param in value:
                                    db_info['config'][param] = value[param]
                                    logger.info(f"æå–å‚æ•° {param}: {value[param]}")
                            
                            # å¦‚æœé…ç½®ä¸ºç©ºä½†æœ‰æ•°æ®ï¼Œå°è¯•ä»æ•´ä¸ªå¯¹è±¡ä¸­æå–
                            if not db_info['config']:
                                db_info['config'] = value
                                logger.info(f"ä½¿ç”¨å®Œæ•´é…ç½®: {value}")
                            
                            if db_info['config']:
                                db_connections.append(db_info)
                                logger.info(f"æ·»åŠ æ•°æ®åº“è¿æ¥: {db_info}")
                    else:
                        extract_db_info(value, f'{prefix}.{key}' if prefix else key)
            elif isinstance(data, list):
                for item in data:
                    extract_db_info(item, prefix)
        
        # å…ˆè¿›è¡Œç‰¹æ®Šå¤„ç†ï¼ˆé¿å…é‡å¤ï¼‰
        if config_type in ['yaml', 'toml']:
            # æŸ¥æ‰¾ datasource é…ç½®
            if 'datasource' in config_data:
                datasource = config_data['datasource']
                if isinstance(datasource, dict):
                    for ds_name, ds_config in datasource.items():
                        if isinstance(ds_config, dict):
                            db_info = {
                                'name': ds_name,
                                'type': 'unknown',
                                'config': {}
                            }
                            for param in ['host', 'port', 'user', 'password', 'database', 'dbname', 'driver']:
                                if param in ds_config:
                                    db_info['config'][param] = ds_config[param]
                            
                            # å°è¯•è§£æ GORM DSN æ ¼å¼çš„ link å­—æ®µ
                            if 'link' in ds_config:
                                dsn_info = parse_gorm_dsn(ds_config['link'])
                                if dsn_info:
                                    db_info['config'].update(dsn_info)
                                    db_info['type'] = dsn_info.get('type', 'unknown')
                            
                            if db_info['config']:
                                db_connections.append(db_info)
                                logger.info(f"æ·»åŠ  datasource é…ç½®: {db_info}")
            
            # æŸ¥æ‰¾ database é…ç½®ï¼ˆGo é¡¹ç›®å¸¸è§ç»“æ„ï¼‰
            if 'database' in config_data:
                database = config_data['database']
                if isinstance(database, dict):
                    # æ£€æŸ¥æ˜¯å¦æœ‰åµŒå¥—çš„æ•°æ®åº“é…ç½®ï¼ˆå¦‚ defaultRead, backup, sysolinï¼‰
                    for db_name, db_config in database.items():
                        if db_name in ['logger', 'cacheKey']:
                            continue  # è·³è¿‡éæ•°æ®åº“é…ç½®
                        
                        if isinstance(db_config, dict) and 'link' in db_config:
                            # è¿™æ˜¯ä¸€ä¸ªæ•°æ®åº“é…ç½®
                            db_info = {
                                'name': db_name,
                                'type': 'unknown',
                                'config': {}
                            }
                            
                            # è§£æ GORM DSN æ ¼å¼çš„ link å­—æ®µ
                            dsn_info = parse_gorm_dsn(db_config['link'])
                            if dsn_info:
                                db_info['config'].update(dsn_info)
                                db_info['type'] = dsn_info.get('type', 'unknown')
                            
                            if db_info['config']:
                                db_connections.append(db_info)
                                logger.info(f"æ·»åŠ æ•°æ®åº“é…ç½® {db_name}: {db_info}")
                        
                        elif isinstance(db_config, list):
                            # å¤„ç†æ•°ç»„ç±»å‹çš„é…ç½®ï¼ˆå¦‚ default: [{role: 'master', link: '...'}, {role: 'slave', link: '...'}]ï¼‰
                            for idx, item in enumerate(db_config):
                                if isinstance(item, dict) and 'link' in item:
                                    db_info = {
                                        'name': f"{db_name}_{item.get('role', idx)}",
                                        'type': 'unknown',
                                        'config': {}
                                    }
                                    
                                    # è§£æ GORM DSN æ ¼å¼çš„ link å­—æ®µ
                                    dsn_info = parse_gorm_dsn(item['link'])
                                    if dsn_info:
                                        db_info['config'].update(dsn_info)
                                        db_info['type'] = dsn_info.get('type', 'unknown')
                                    
                                    if db_info['config']:
                                        db_connections.append(db_info)
                                        logger.info(f"æ·»åŠ æ•°æ®åº“é…ç½® {db_info['name']}: {db_info}")
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•åµŒå¥—é…ç½®ï¼Œå°è¯•ç›´æ¥è§£æ database å¯¹è±¡
                    if not db_connections:
                        db_info = {
                            'name': 'database',
                            'type': database.get('type', 'unknown'),
                            'config': {}
                        }
                        
                        # æå–æ‰€æœ‰å¯èƒ½çš„é…ç½®å‚æ•°
                        for param in ['host', 'port', 'user', 'username', 'password', 'database', 'dbname', 'name', 'address', 'dsn', 'url', 'charset', 'link']:
                            if param in database:
                                if param == 'link':
                                    # å°è¯•è§£æ GORM DSN æ ¼å¼
                                    dsn_info = parse_gorm_dsn(database[param])
                                    if dsn_info:
                                        db_info['config'].update(dsn_info)
                                        db_info['type'] = dsn_info.get('type', 'unknown')
                                else:
                                    db_info['config'][param] = database[param]
                        
                        # å¦‚æœé…ç½®ä¸ºç©ºä½†æœ‰æ•°æ®ï¼Œä½¿ç”¨æ•´ä¸ªå¯¹è±¡
                        if not db_info['config']:
                            db_info['config'] = database
                        
                        if db_info['config']:
                            db_connections.append(db_info)
                            logger.info(f"æ·»åŠ  database é…ç½®: {db_info}")
            
            # æŸ¥æ‰¾ mysql, postgres ç­‰ç›´æ¥é…ç½®
            for db_type in ['mysql', 'postgres', 'postgresql', 'mongodb', 'redis']:
                if db_type in config_data:
                    db_config = config_data[db_type]
                    if isinstance(db_config, dict):
                        db_info = {
                            'name': db_type,
                            'type': db_type,
                            'config': {}
                        }
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰ link å­—æ®µï¼ˆGORM DSN æ ¼å¼ï¼‰
                        if 'link' in db_config:
                            dsn_info = parse_gorm_dsn(db_config['link'])
                            if dsn_info:
                                db_info['config'].update(dsn_info)
                        
                        # æå–å…¶ä»–å‚æ•°
                        for param in ['host', 'port', 'user', 'username', 'password', 'database', 'dbname', 'name', 'address']:
                            if param in db_config:
                                db_info['config'][param] = db_config[param]
                        
                        if db_info['config']:
                            db_connections.append(db_info)
                            logger.info(f"æ·»åŠ  {db_type} é…ç½®: {db_info}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•é…ç½®ï¼Œå†è¿›è¡Œé€’å½’æå–
        if not db_connections:
            logger.info("æœªæ‰¾åˆ°ç‰¹æ®Šé…ç½®ï¼Œè¿›è¡Œé€’å½’æå–")
            extract_db_info(config_data)
    
    except Exception as e:
        logger.error(f"è§£ææ•°æ®åº“é…ç½®æ—¶å‡ºé”™: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    except Exception as e:
        logger.warning(f"Failed to parse database config: {e}")
    
    return db_connections

@app.get("/api/database/project-databases/{project_name}")
async def get_project_databases(project_name: str):
    """è·å–é¡¹ç›®ä¸­çš„æ‰€æœ‰æ•°æ®åº“æ–‡ä»¶å’Œé…ç½®"""
    try:
        import glob
        import yaml
        import toml
        
        project_path = get_project_path(project_name)
        
        if not project_path or not os.path.exists(project_path):
            return JSONResponse({"error": "Project not found"}, status_code=404)
        
        db_files = []
        db_configs = []
        
        # é€’å½’æœç´¢æ•°æ®åº“æ–‡ä»¶å’Œé…ç½®æ–‡ä»¶
        for root, dirs, files in os.walk(project_path):
            # è·³è¿‡å¸¸è§çš„éæ•°æ®åº“ç›®å½•
            dirs[:] = [d for d in dirs if d not in ['node_modules', '.git', '__pycache__', 'dist', 'build', 'vendor']]
            
            for file in files:
                full_path = os.path.join(root, file)
                relative_path = os.path.relpath(full_path, project_path)
                
                # æœç´¢ SQLite æ•°æ®åº“æ–‡ä»¶
                if file.endswith('.db') or file.endswith('.sqlite') or file.endswith('.sqlite3'):
                    file_size = os.path.getsize(full_path) if os.path.exists(full_path) else 0
                    
                    # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ SQLite æ•°æ®åº“
                    is_valid = False
                    try:
                        conn = sqlite3.connect(full_path)
                        conn.execute("SELECT name FROM sqlite_master WHERE type='table' LIMIT 1;")
                        conn.close()
                        is_valid = True
                    except Exception:
                        pass
                    
                    db_files.append({
                        "name": file,
                        "path": relative_path,
                        "full_path": full_path,
                        "size": file_size,
                        "is_valid": is_valid,
                        "type": "sqlite"
                    })
                
                # æœç´¢ Go é¡¹ç›®çš„é…ç½®æ–‡ä»¶
                elif file.endswith(('.yaml', '.yml', '.toml')) or file in ['.env', 'go.mod']:
                    # æ”¯æŒå¸¦ç¯å¢ƒåç¼€çš„é…ç½®æ–‡ä»¶ï¼ˆå¦‚ config.dev.toml, config.pro.tomlï¼‰
                    is_config_file = (
                        file in ['config.yaml', 'config.yml', 'config.toml', '.env', 'go.mod'] or
                        file.startswith('config.') and file.endswith(('.yaml', '.yml', '.toml'))
                    )
                    
                    # åªå¤„ç†æ ¹ç›®å½•çš„é…ç½®æ–‡ä»¶
                    if is_config_file and (root == project_path or relative_path.count('/') <= 1):
                        try:
                            config_data = None
                            config_type = None
                            
                            if file.endswith(('.yaml', '.yml')):
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    config_data = yaml.safe_load(f)
                                config_type = 'yaml'
                            elif file.endswith('.toml'):
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    config_data = toml.load(f)
                                config_type = 'toml'
                            elif file == '.env':
                                from dotenv import load_dotenv
                                config_data = {}
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    for line in f:
                                        line = line.strip()
                                        if line and not line.startswith('#') and '=' in line:
                                            key, value = line.split('=', 1)
                                            config_data[key.strip()] = value.strip()
                                config_type = 'env'
                            elif file == 'go.mod':
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    config_data = {'module': '', 'go_version': ''}
                                    for line in f:
                                        if line.startswith('module '):
                                            config_data['module'] = line.split()[1]
                                        elif line.startswith('go '):
                                            config_data['go_version'] = line.split()[1]
                                config_type = 'go'
                            
                            if config_data:
                                # è§£ææ•°æ®åº“é…ç½®
                                db_connections = parse_database_config(config_data, config_type)
                                
                                # æå–ç¯å¢ƒä¿¡æ¯
                                env_info = None
                                if file.startswith('config.') and '.' in file[:-5]:
                                    # æå–ç¯å¢ƒåç§°ï¼ˆå¦‚ config.dev.toml -> devï¼‰
                                    parts = file.split('.')
                                    if len(parts) >= 3:
                                        env_info = parts[1]
                                
                                db_configs.append({
                                    "name": file,
                                    "path": relative_path,
                                    "full_path": full_path,
                                    "type": config_type,
                                    "data": config_data,
                                    "db_connections": db_connections,
                                    "environment": env_info
                                })
                        except Exception as e:
                            logger.warning(f"Failed to read config file {file}: {e}")
        
        # æŒ‰æ–‡ä»¶åæ’åº
        db_files.sort(key=lambda x: x["name"])
        db_configs.sort(key=lambda x: x["name"])
        
        return {
            "project_name": project_name,
            "project_path": project_path,
            "databases": db_files,
            "configs": db_configs,
            "database_count": len(db_files),
            "config_count": len(db_configs)
        }
    except Exception as e:
        logger.error(f"Error getting project databases: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# --- Catch-all è·¯ç”± ---

@app.api_route("/api/{path_name:path}", methods=["GET", "POST", "PUT", "DELETE"])
async def catch_all(path_name: str, request: Request):
    """Catch-all è·¯ç”± - å¤„ç†æœªå®ç°çš„ API ç«¯ç‚¹"""
    logger.warning(f"æœªå¤„ç†çš„ API è¯·æ±‚: {request.method} /api/{path_name}")

    # MCP ç›¸å…³çš„ API
    if path_name.startswith("mcp-utils/"):
        return JSONResponse(content={
            "status": "not-implemented",
            "message": f"MCP endpoint '{path_name}' is not implemented"
        }, status_code=200)

    # é»˜è®¤å“åº”
    return JSONResponse(content={"status": "mocked", "sessions": [], "hasMore": False}, status_code=200)


# --- Workflow API ---

class WorkflowSaveRequest(BaseModel):
    project_name: str
    workflow_name: str
    nodes: List[Dict[str, Any]]
    edges: List[Dict[str, Any]]

class WorkflowGenerateRequest(BaseModel):
    prompt: str

@app.post("/api/workflows/save")
async def save_workflow(req: WorkflowSaveRequest):
    """ä¿å­˜å·¥ä½œæµ"""
    try:
        from backend.core.workflow_service import Workflow
        
        workflow = Workflow(
            id=None,
            name=req.workflow_name,
            nodes=req.nodes,
            edges=req.edges,
            project_name=req.project_name,
            created_at=datetime.now().isoformat(),
            updated_at=datetime.now().isoformat()
        )
        
        workflow_id = workflow_service.save_workflow(workflow)
        
        return {
            "success": True,
            "workflow_id": workflow_id,
            "message": "å·¥ä½œæµä¿å­˜æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"Error saving workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/workflows/{project_name}")
async def get_workflows(project_name: str):
    """è·å–é¡¹ç›®çš„æ‰€æœ‰å·¥ä½œæµ"""
    try:
        workflows = workflow_service.get_workflows_by_project(project_name)
        return {
            "workflows": [
                {
                    "id": w.id,
                    "name": w.name,
                    "created_at": w.created_at,
                    "updated_at": w.updated_at,
                    "nodes_count": len(w.nodes),
                    "edges_count": len(w.edges)
                }
                for w in workflows
            ]
        }
    except Exception as e:
        logger.error(f"Error getting workflows: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/workflows/{project_name}/{workflow_id}")
async def get_workflow(project_name: str, workflow_id: str):
    """è·å–å·¥ä½œæµè¯¦æƒ…"""
    try:
        workflow = workflow_service.get_workflow(workflow_id)
        if not workflow:
            return JSONResponse({"error": "Workflow not found"}, status_code=404)
        
        return {
            "id": workflow.id,
            "name": workflow.name,
            "nodes": workflow.nodes,
            "edges": workflow.edges,
            "created_at": workflow.created_at,
            "updated_at": workflow.updated_at
        }
    except Exception as e:
        logger.error(f"Error getting workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/workflows/{project_name}/{workflow_id}")
async def delete_workflow(project_name: str, workflow_id: str):
    """åˆ é™¤å·¥ä½œæµ"""
    try:
        success = workflow_service.delete_workflow(workflow_id)
        if success:
            return {"success": True, "message": "å·¥ä½œæµåˆ é™¤æˆåŠŸ"}
        else:
            return JSONResponse({"error": "Workflow not found"}, status_code=404)
    except Exception as e:
        logger.error(f"Error deleting workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/workflows/generate")
async def generate_workflow(req: WorkflowGenerateRequest):
    """AI ç”Ÿæˆå·¥ä½œæµ"""
    try:
        result = workflow_service.generate_workflow_from_prompt(req.prompt)
        return result
    except Exception as e:
        logger.error(f"Error generating workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/workflows/{workflow_id}/execute")
async def execute_workflow(workflow_id: str, context: Dict[str, Any] = None):
    """æ‰§è¡Œå·¥ä½œæµ"""
    try:
        workflow = workflow_service.get_workflow(workflow_id)
        if not workflow:
            return JSONResponse({"error": "Workflow not found"}, status_code=404)

        # è·å–é¡¹ç›®è·¯å¾„
        project_path = project_registry.get_project_path(workflow.project_name)
        if not project_path:
            return JSONResponse({"error": "Project path not found"}, status_code=404)

        # æ‰§è¡Œå·¥ä½œæµ
        result = await workflow_executor.execute_workflow(
            workflow_id,
            {
                'nodes': workflow.nodes,
                'edges': workflow.edges
            },
            project_path,
            context
        )

        return {
            "success": result.success,
            "steps_completed": result.steps_completed,
            "steps_total": result.steps_total,
            "logs": result.logs,
            "output": result.output,
            "error": result.error
        }
    except Exception as e:
        logger.error(f"Error executing workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


if __name__ == "__main__":
    import uvicorn
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    uvicorn.run(app, host="0.0.0.0", port=8000)
