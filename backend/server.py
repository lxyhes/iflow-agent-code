import sys
import os
import mimetypes
import asyncio
import json
import platform
import subprocess
import logging
from datetime import datetime

# Windows ‰∫ã‰ª∂Âæ™ÁéØÁ≠ñÁï•ËÆæÁΩÆ - ÂøÖÈ°ªÂú®‰ªª‰ΩïÂºÇÊ≠•Êìç‰Ωú‰πãÂâçËÆæÁΩÆ
if platform.system() == 'Windows':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

from fastapi import FastAPI, HTTPException, Request, Query, Body, WebSocket
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse, FileResponse

# ÈÖçÁΩÆÊó•Âøó - ÊîØÊåÅÁéØÂ¢ÉÂèòÈáè
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
if log_level not in valid_levels:
    logger.warning(f"Invalid LOG_LEVEL: {log_level}, using INFO")
    log_level = "INFO"

logging.basicConfig(
    level=getattr(logging, log_level),
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("Server")
logger.info(f"Êó•ÂøóÁ∫ßÂà´ËÆæÁΩÆ‰∏∫: {log_level}")

# Add backend to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/..")

from backend.impl.reviewer import create_code_review_agent
from backend.core.project_manager import project_manager
from backend.core.agent import Agent
from backend.core.smart_requirement_service import smart_requirement_service
from backend.core.cicd_generator import cicd_generator
from backend.core.project_template_service import get_project_template_service
from backend.core.task_master_service import task_master_service, Task as TaskModel
from backend.core.file_service import file_service
from backend.core.git_service import git_service
from backend.core.shell_service import ShellSession
from backend.core.path_validator import PathValidator, project_registry
from backend.core.error_analyzer import ErrorAnalyzer, get_error_analyzer
from backend.core.code_style_analyzer import CodeStyleAnalyzer, get_code_style_analyzer
from backend.core.report_generator import ReportGenerator, get_report_generator
from backend.core.dependency_analyzer import DependencyAnalyzer, get_dependency_analyzer
from backend.core.auto_fixer import AutoFixer, get_auto_fixer
from backend.core.code_dependency_analyzer import CodeDependencyAnalyzer, get_dependency_analyzer as get_code_dependency_analyzer
from backend.core.prompt_optimizer import PromptOptimizer, get_prompt_optimizer
from backend.core.rag_service import RAGService, get_rag_service
from backend.core.document_version_manager import get_version_manager
from backend.core.database_query_service import database_query_service
from backend.core.workflow_service import workflow_service
from backend.core.workflow_executor import workflow_executor
from backend.core.workflow_execution_store import workflow_execution_store

app = FastAPI(title="IFlow Agent API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- CACHE MANAGER ---
class CacheManager:
    """ÁºìÂ≠òÁÆ°ÁêÜÂô®ÔºåÊîØÊåÅËá™Âä®Ê∏ÖÁêÜÂíåÂ§ßÂ∞èÈôêÂà∂"""

    def __init__(self, max_size=100, name="Cache"):
        self.cache = {}
        self.max_size = max_size
        self.name = name
        self.access_times = {}
        self._total_accesses = 0
        self._hits = 0

    def get(self, key):
        """Ëé∑ÂèñÁºìÂ≠òÈ°π"""
        if key in self.cache:
            self.access_times[key] = datetime.now().timestamp()
            self._total_accesses += 1
            self._hits += 1
            return self.cache[key]
        self._total_accesses += 1
        return None

    def set(self, key, value):
        """ËÆæÁΩÆÁºìÂ≠òÈ°πÔºåÂ¶ÇÊûúË∂ÖÂá∫Â§ßÂ∞èÈôêÂà∂ÂàôÊ∏ÖÁêÜÊúÄÊóßÁöÑÈ°π"""
        if len(self.cache) >= self.max_size and key not in self.cache:
            self._cleanup_oldest()
        self.cache[key] = value
        self.access_times[key] = datetime.now().timestamp()

    def _cleanup_oldest(self):
        """Ê∏ÖÁêÜÊúÄÊóßÁöÑÁºìÂ≠òÈ°π"""
        if not self.access_times:
            return

        oldest_key = min(self.access_times, key=self.access_times.get)
        del self.cache[oldest_key]
        del self.access_times[oldest_key]
        logger.debug(f"[{self.name}] Cleaned up oldest cache entry: {oldest_key}")

    def clear(self):
        """Ê∏ÖÁ©∫ÊâÄÊúâÁºìÂ≠ò"""
        count = len(self.cache)
        self.cache.clear()
        self.access_times.clear()
        logger.info(f"[{self.name}] Cleared {count} cache entries")

    def get_stats(self):
        """Ëé∑ÂèñÁºìÂ≠òÁªüËÆ°‰ø°ÊÅØ"""
        hit_rate = (self._hits / self._total_accesses * 100) if self._total_accesses > 0 else 0
        return {
            "name": self.name,
            "size": len(self.cache),
            "max_size": self.max_size,
            "total_accesses": self._total_accesses,
            "hits": self._hits,
            "hit_rate": f"{hit_rate:.2f}%"
        }

    def __contains__(self, key):
        return key in self.cache

    def __len__(self):
        return len(self.cache)

    def __getitem__(self, key):
        if key in self.cache:
            self.access_times[key] = datetime.now().timestamp()
            self._total_accesses += 1
            self._hits += 1
            return self.cache[key]
        self._total_accesses += 1
        raise KeyError(key)

    def __setitem__(self, key, value):
        self.set(key, value)

    def __delitem__(self, key):
        if key not in self.cache:
            raise KeyError(key)
        del self.cache[key]
        if key in self.access_times:
            del self.access_times[key]

# --- MODELS ---
class CreateProjectRequest(BaseModel): path: str

class CreateWorkspaceRequest(BaseModel):
    workspaceType: str
    path: str
    githubUrl: str = None
    githubTokenId: int = None
    newGithubToken: str = None
class SaveFileRequest(BaseModel): filePath: str; content: str
class CheckoutRequest(BaseModel): project: str; branch: str
class CommitRequest(BaseModel): project: str; message: str; files: list

# --- GLOBAL CONFIG ---
global_config = {
    "mode": "yolo",
    "model": "GLM-4.7", # Set to recommended model
    "mcp_servers": [],
    "iflow_path": "iflow", # Default command
    "rag_mode": "tfidf", # RAG Ê®°Âºè: "chromadb" (ÈúÄË¶Å‰∏ãËΩΩÊ®°Âûã) Êàñ "tfidf" (ËΩªÈáèÁ∫ß)
    "chat_only_mode": False # ‰ªÖËÅäÂ§©Ê®°ÂºèÔºöAI Âè™ËÉΩËÅäÂ§©Ôºå‰∏çËÉΩ‰øÆÊîπÊñá‰ª∂
}

# --- HELPERS ---
def get_iflow_version():
    try:
        cmd = global_config.get("iflow_path", "iflow")
        result = subprocess.run([cmd, "--version"], capture_output=True, text=True, timeout=2)
        if result.returncode == 0:
            return result.stdout.strip()
    except:
        return None
    return None

def get_agent(cwd: str, mode: str = "yolo", model: str = None, mcp_servers: list = None, persona: str = "partner", auth_method_id: str = None, auth_method_info: dict = None):
    key = (cwd, mode, model, json.dumps(mcp_servers or []), persona, auth_method_id)
    if key not in agent_cache:
        system_prompt = PERSONA_PROMPTS.get(persona, PERSONA_PROMPTS["partner"])
        agent_cache[key] = Agent(name="IFlowAgent", cwd=cwd, mode=mode, model=model, mcp_servers=mcp_servers, persona=persona, system_prompt=system_prompt, auth_method_id=auth_method_id, auth_method_info=auth_method_info)
    return agent_cache[key]

def get_project_path(project_name: str) -> str:
    """ÂÆâÂÖ®Âú∞Ëé∑ÂèñÈ°πÁõÆË∑ØÂæÑÔºåÈò≤Ê≠¢Ë∑ØÂæÑÈÅçÂéÜÊîªÂáª"""
    logger.info(f"[get_project_path] Looking for project: '{project_name}'")

    if not project_name:
        logger.warning(f"[get_project_path] No project name provided, returning cwd: {os.getcwd()}")
        return os.getcwd()

    # Ê£ÄÊü• project_name ÊòØÂê¶Êú¨Ë∫´Â∞±ÊòØ‰∏Ä‰∏™ÊúâÊïàÁöÑÈ°πÁõÆË∑ØÂæÑ
    # Â¶ÇÊûúÂåÖÂê´Ë∑ØÂæÑÂàÜÈöîÁ¨¶ÔºàWindows: \ Êàñ /ÔºâÔºåÂàôËÆ§‰∏∫ÂÆÉÊòØ‰∏Ä‰∏™Ë∑ØÂæÑ
    if '\\' in project_name or '/' in project_name:
        # È™åËØÅË∑ØÂæÑÂÆâÂÖ®ÊÄß
        is_valid, error, normalized = PathValidator.validate_project_path(project_name)
        if is_valid and os.path.exists(normalized):
            logger.info(f"[get_project_path] project_name is a valid path: {normalized}")
            # Ê≥®ÂÜåÂà∞È°πÁõÆÊ≥®ÂÜåË°®
            project_registry.register_project(os.path.basename(normalized), normalized)
            return normalized

    # È¶ñÂÖàÂ∞ùËØï‰ªéÊ≥®ÂÜåË°®Ëé∑Âèñ
    registered_path = project_registry.get_project_path(project_name)
    if registered_path:
        logger.info(f"[get_project_path] Found in registry: {registered_path}")
        return registered_path
    
    logger.info(f"[get_project_path] Not in registry, checking project_manager...")
    
    # ÁÑ∂Âêé‰ªé project_manager Ëé∑Âèñ
    projects = project_manager.get_projects()
    logger.info(f"[get_project_path] Found {len(projects)} projects in manager")
    for p in projects:
        logger.info(f"[get_project_path]   - {p.get('name')}: {p.get('fullPath')}")
        if p["name"] == project_name:
            # È™åËØÅË∑ØÂæÑÂÆâÂÖ®ÊÄß
            is_valid, error, normalized = PathValidator.validate_project_path(p["fullPath"])
            if is_valid:
                project_registry.register_project(p["name"], normalized)
                logger.info(f"[get_project_path] Found in project_manager: {normalized}")
                return normalized

    # Â¶ÇÊûúËøòÊòØÊâæ‰∏çÂà∞ÔºåÂ∞ùËØïÂú®Áà∂ÁõÆÂΩï‰∏ãÂØªÊâæÂåπÈÖçÁöÑÈ°πÁõÆÊñá‰ª∂Â§πÂêç
    # Ëé∑Âèñ backend ÁöÑÁà∂ÁõÆÂΩïÂç≥ agent_project
    current_base = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    PathValidator.add_allowed_root(current_base)
    PathValidator.add_allowed_root(os.path.dirname(current_base))
    logger.info(f"[get_project_path] Checking if project_name matches current_base: {project_name} == {os.path.basename(current_base)}")
    # Ê£ÄÊü•ÊòØÂê¶ÂåπÈÖçÂΩìÂâçÈ°πÁõÆÊñá‰ª∂Â§πÂêç
    if project_name == os.path.basename(current_base):
        logger.info(f"[get_project_path] Matched current_base: {current_base}")
        return current_base

    potential_in_base = os.path.join(current_base, project_name)
    logger.info(f"[get_project_path] Checking potential_in_base: {potential_in_base}")
    if os.path.isdir(potential_in_base):
        is_valid, _, normalized = PathValidator.validate_project_path(potential_in_base)
        if is_valid:
            project_registry.register_project(project_name, normalized)
            logger.info(f"[get_project_path] Found in current_base: {normalized}")
            return normalized
        
    # Ê£ÄÊü•ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩïÁöÑÁà∂ÁõÆÂΩï
    parent_dir = os.path.dirname(os.getcwd())
    potential_path = os.path.join(parent_dir, project_name)
    logger.info(f"[get_project_path] Checking potential_path: {potential_path}")
    if os.path.isdir(potential_path):
        is_valid, _, normalized = PathValidator.validate_project_path(potential_path)
        if is_valid:
            project_registry.register_project(project_name, normalized)
            logger.info(f"[get_project_path] Found in parent_dir: {normalized}")
            return normalized
    
    # ‰∏çÂÜçÁõ¥Êé•ËøîÂõûÁî®Êà∑ËæìÂÖ•ÁöÑË∑ØÂæÑÔºåËÄåÊòØËøîÂõûÂÆâÂÖ®ÁöÑÈªòËÆ§ÂÄº
    logger.warning(f"[get_project_path] Êú™ÊâæÂà∞È°πÁõÆ: {project_name}, ËøîÂõûÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï: {os.getcwd()}")
    return os.getcwd()

# ‰ªéÁéØÂ¢ÉÂèòÈáèËØªÂèñÁºìÂ≠òÈÖçÁΩÆ
agent_cache_max_size = int(os.getenv("AGENT_CACHE_MAX_SIZE", "100"))
rag_cache_max_size = int(os.getenv("RAG_CACHE_MAX_SIZE", "50"))

logger.info(f"Agent ÁºìÂ≠òÊúÄÂ§ßÂ§ßÂ∞è: {agent_cache_max_size}")
logger.info(f"RAG ÁºìÂ≠òÊúÄÂ§ßÂ§ßÂ∞è: {rag_cache_max_size}")

# ‰ΩøÁî®ÁºìÂ≠òÁÆ°ÁêÜÂô®
agent_cache = CacheManager(max_size=agent_cache_max_size, name="AgentCache")
rag_cache = CacheManager(max_size=rag_cache_max_size, name="RAGCache")

# AI Persona System Prompts
PERSONA_PROMPTS = {
    "senior": """You are a senior software architect with 15+ years of experience. Your role is to ensure code excellence.

STRICT GUIDELINES:
- Always review code quality first before suggesting solutions
- Point out potential bugs, security issues, and performance problems
- Enforce best practices: SOLID principles, DRY, clean code
- Require proper error handling, logging, and testing
- Suggest design patterns and architectural improvements
- Reject quick-and-dirty solutions that lack robustness
- Prioritize maintainability, scalability, and readability
- Ask clarifying questions to understand the full context

RESPONSE STYLE:
- Professional and authoritative but constructive
- Provide detailed explanations for your recommendations
- Reference industry standards and common pitfalls
- Suggest refactoring when code is messy
- Emphasize long-term code health over quick wins

Example: "This approach has a critical flaw: it doesn't handle edge cases. Let me show you a more robust solution that follows the Repository pattern..."
""",
    "hacker": """You are a pragmatic hacker who values shipping over perfection. Your role is to get things working fast.

CORE PHILOSOPHY:
- Working code > perfect code
- Ship first, iterate later
- Minimize boilerplate and ceremony
- Use the simplest solution that works
- Skip excessive comments and documentation during dev
- Focus on the happy path, handle errors later if needed
- Copy-paste is fine if it saves time
- Use whatever libraries/tools get the job done

RESPONSE STYLE:
- Direct and action-oriented
- Minimal explanations, maximum code
- "Here's the solution" not "Let's discuss the approach"
- Skip lengthy justifications
- Assume the user knows what they're doing
- Provide shortcuts and quick fixes

Example: "Here's the code. Copy-paste it and you're done. If it breaks, we'll fix it then."
""",
    "partner": """You are an empathetic pair programming partner. Your role is to be supportive and encouraging.

EMOTIONAL INTELLIGENCE:
- Celebrate small wins and progress
- Acknowledge when tasks are difficult
- Be patient with mistakes and confusion
- Provide reassurance when debugging is frustrating
- Use encouraging language: "Great question!", "Nice work!", "We'll get this!"
- Normalize the struggle: "This is tricky, let's work through it together"
- Boost confidence: "You're doing great!", "Almost there!"

COLLABORATIVE STYLE:
- Ask questions to understand their thinking
- Suggest alternatives without being pushy
- Explain concepts in simple, friendly terms
- Share enthusiasm for solving problems together
- Make coding feel like a team effort
- Use "we" language: "Let's try this", "We can fix that"

Example: "That's a great idea! Let's implement it together. I know this part can be tricky, but we'll figure it out. Nice work getting this far!"
"""
}

# --- API ENDPOINTS ---

def check_iflow_auth():
    try:
        # Â∞ùËØïËøêË°å auth status Êü•ÁúãÊòØÂê¶ÁôªÂΩï
        result = subprocess.run(["iflow", "auth", "status"], capture_output=True, text=True, timeout=2)
        return "Logged in" in result.stdout or result.returncode == 0
    except:
        return False

@app.get("/api/auth/status")
async def auth_status():
    version = get_iflow_version()
    # FORCE CONNECTED if we suspect it's running but command fails
    is_connected = True if version else True 
    
    return {
        "authenticated": True, 
        "is_iflow_installed": is_connected,
        "is_iflow_authenticated": True,
        "iflow_version": version or "Active Session",
        "install_command": "npm install -g iflow-cli",
        "user": {"username": "iflow-dev"},
        "provider": "iflow"
    }

@app.get("/api/config")
async def get_config(): return global_config

@app.post("/api/config")
async def update_config(config: dict = Body(...)):
    global_config.update(config)
    agent_cache.clear()
    return global_config


@app.get("/api/iflow/mcp-servers")
async def get_iflow_mcp_servers():
    """‰ªé iFlow ÈÖçÁΩÆÊñá‰ª∂ËØªÂèñÂ∑≤ÈÖçÁΩÆÁöÑ MCP ÊúçÂä°Âô®"""
    try:
        # iFlow ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
        iflow_config_path = os.path.expanduser("~/.iflow/settings.json")

        if not os.path.exists(iflow_config_path):
            return {"success": True, "servers": [], "message": "iFlow ÈÖçÁΩÆÊñá‰ª∂‰∏çÂ≠òÂú®"}

        # ËØªÂèñÈÖçÁΩÆÊñá‰ª∂
        with open(iflow_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # ÊèêÂèñ MCP ÊúçÂä°Âô®ÈÖçÁΩÆ
        mcp_servers = config.get("mcpServers", {})

        # ËΩ¨Êç¢‰∏∫Ê†áÂáÜÊ†ºÂºè
        servers_list = []
        for server_name, server_config in mcp_servers.items():
            if isinstance(server_config, dict):
                servers_list.append({
                    "name": server_name,
                    "type": server_config.get("type", "stdio"),
                    "config": {
                        "command": server_config.get("command", ""),
                        "args": server_config.get("args", []),
                        "env": server_config.get("env", {}),
                        "url": server_config.get("url", ""),
                        "headers": server_config.get("headers", {}),
                        "timeout": server_config.get("timeout", 30000)
                    }
                })

        logger.info(f"‰ªé iFlow ÈÖçÁΩÆËØªÂèñÂà∞ {len(servers_list)} ‰∏™ MCP ÊúçÂä°Âô®")
        return {
            "success": True,
            "servers": servers_list,
            "config_path": iflow_config_path
        }

    except FileNotFoundError:
        return {"success": True, "servers": [], "message": "iFlow ÈÖçÁΩÆÊñá‰ª∂‰∏çÂ≠òÂú®"}
    except json.JSONDecodeError as e:
        logger.error(f"Ëß£Êûê iFlow ÈÖçÁΩÆÊñá‰ª∂Â§±Ë¥•: {e}")
        return {"success": False, "error": f"ÈÖçÁΩÆÊñá‰ª∂Ëß£ÊûêÂ§±Ë¥•: {str(e)}"}
    except Exception as e:
        logger.error(f"ËØªÂèñ iFlow MCP ÈÖçÁΩÆÂ§±Ë¥•: {e}")
        return {"success": False, "error": f"ËØªÂèñÂ§±Ë¥•: {str(e)}"}


@app.post("/api/iflow/sync-mcp-servers")
async def sync_iflow_mcp_servers():
    """‰ªé iFlow ÈÖçÁΩÆÂêåÊ≠• MCP ÊúçÂä°Âô®Âà∞ÂêéÁ´Ø global_config"""
    try:
        # ËØªÂèñ iFlow MCP ÈÖçÁΩÆ
        result = await get_iflow_mcp_servers()

        if not result.get("success"):
            return {"success": False, "error": result.get("error")}

        servers = result.get("servers", [])

        # Êõ¥Êñ∞ global_config
        global_config["mcp_servers"] = servers

        # Ê∏ÖÈô§ agent ÁºìÂ≠ò‰ª•‰ΩøÁî®Êñ∞ÈÖçÁΩÆ
        agent_cache.clear()

        logger.info(f"Â∑≤‰ªé iFlow ÂêåÊ≠• {len(servers)} ‰∏™ MCP ÊúçÂä°Âô®Âà∞ÂêéÁ´Ø")
        return {
            "success": True,
            "servers_count": len(servers),
            "servers": servers
        }

    except Exception as e:
        logger.error(f"ÂêåÊ≠• iFlow MCP ÊúçÂä°Âô®Â§±Ë¥•: {e}")
        return {"success": False, "error": f"ÂêåÊ≠•Â§±Ë¥•: {str(e)}"}


@app.get("/api/projects")
async def get_projects():
    """Ëé∑ÂèñÈ°πÁõÆÂàóË°® - Â¢ûÂº∫ÂÆâÂÖ®ÁâàÊú¨"""
    projects = project_manager.get_projects()
    
    # È™åËØÅÂπ∂ËøáÊª§ÊØè‰∏™È°πÁõÆÁöÑË∑ØÂæÑ
    safe_projects = []
    for p in projects:
        is_valid, error, normalized = PathValidator.validate_project_path(p.get("fullPath", ""), must_exist=False)
        if is_valid:
            # Ê≥®ÂÜåÂà∞ÂÖ®Â±ÄÊ≥®ÂÜåË°®
            project_registry.register_project(p["name"], normalized)
            safe_projects.append(p)
        else:
            logger.warning(f"Ë∑ØÂæÑ‰∏çÂÆâÂÖ®ÁöÑÈ°πÁõÆ: {p.get('name')} - {error}")
            # Ëá™Âä®Â∞ÜËØ•Ë∑ØÂæÑÊ∑ªÂä†Âà∞ÂÖÅËÆ∏ÁöÑÊ†πÁõÆÂΩïÂàóË°®
            project_path = p.get("fullPath", "")
            if project_path:
                PathValidator.add_allowed_root(project_path)
                PathValidator.add_allowed_root(os.path.dirname(project_path))
                logger.info(f"Â∑≤Â∞ÜË∑ØÂæÑÊ∑ªÂä†Âà∞ÂÖÅËÆ∏ÂàóË°®: {project_path}")
                # ÈáçÊñ∞È™åËØÅ
                is_valid, error, normalized = PathValidator.validate_project_path(p.get("fullPath", ""), must_exist=False)
                if is_valid:
                    project_registry.register_project(p["name"], normalized)
                    safe_projects.append(p)
    
    # Ëá™Âä®Êâ´ÊèèÈ°πÁõÆÊ†πÁõÆÂΩï‰∏ãÁöÑÂÖ∂‰ªñÈ°πÁõÆÔºà‰ΩÜÈúÄË¶ÅÈ™åËØÅÔºâ
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    PathValidator.add_allowed_root(root_dir)  # ÂÖÅËÆ∏È°πÁõÆÊ†πÁõÆÂΩï
    
    try:
        for item in os.listdir(root_dir):
            # Ë∑≥ËøáÈöêËóèÊñá‰ª∂ÂíåÂ∑≤Â§ÑÁêÜÁöÑÈ°πÁõÆ
            if item.startswith('.') or item in ['agent_project', 'node_modules', '__pycache__', 'storage']:
                continue
            
            full_path = os.path.join(root_dir, item)
            
            # È™åËØÅË∑ØÂæÑÊòØÂê¶ÂÆâÂÖ®
            is_valid, error, normalized = PathValidator.validate_project_path(full_path)
            if not is_valid:
                continue
            
            # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®
            if not any(p["name"] == item for p in safe_projects):
                safe_projects.append({
                    "name": item,
                    "displayName": item,
                    "path": full_path,
                    "fullPath": full_path,
                    "sessions": [],
                    "sessionMeta": {"total": 0}
                })
    except Exception as e:
        logger.error(f"Êâ´ÊèèÈ°πÁõÆÊ†πÁõÆÂΩïÂ§±Ë¥•: {e}")
    
    return safe_projects

@app.get("/stream")
async def stream_endpoint(message: str, cwd: str = None, sessionId: str = None, project: str = None, model: str = None, persona: str = "partner", auth_method_id: str = None, auth_method_info: str = None, mode: str = None):
    logger.info(f"=== /stream request ===")
    logger.info(f"  message: {message[:100]}...")
    logger.info(f"  model: {model}")
    logger.info(f"  persona: {persona}")
    logger.info(f"  mode: {mode}")
    logger.info(f"  auth_method_id: {auth_method_id}")

    target_cwd = cwd or os.getcwd()
    project_name = project or os.path.basename(target_cwd)
    if sessionId: project_manager.save_message(project_name, sessionId, "user", message)

    # Ëß£ÊûêËÆ§ËØÅÊñπÊ≥ï‰ø°ÊÅØÔºàÂ¶ÇÊûúÊúâÔºâ
    auth_info = None
    if auth_method_info:
        try:
            auth_info = json.loads(auth_method_info)
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse auth_method_info: {auth_method_info}")

    # Use provided model or fallback to global config
    target_model = model or global_config.get("model")
    
    # Use provided mode or fallback to global config
    target_mode = mode or global_config.get("mode", "default")

    agent = get_agent(
        target_cwd,
        target_mode,
        target_model,
        global_config.get("mcp_servers"),
        persona=persona,
        auth_method_id=auth_method_id,
        auth_method_info=auth_info
    )
    
    async def event_generator():
        yield f"data: {json.dumps({'type': 'status', 'content': 'IFlow is thinking...'})}\n\n"
        full_reply = ""
        try:
            async for msg in agent.chat_stream(message):
                # Ê£ÄÊü• msg ÊòØÂ≠óÁ¨¶‰∏≤ËøòÊòØÂ≠óÂÖ∏
                if isinstance(msg, str):
                    # Â¶ÇÊûúÊòØÂ≠óÁ¨¶‰∏≤ÔºåÁõ¥Êé•‰Ωú‰∏∫ÂÜÖÂÆπËøîÂõûÔºàÊóßÂÆ¢Êà∑Á´ØÂÖºÂÆπÔºâ
                    content = msg
                    full_reply += content
                    yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                else:
                    # Â¶ÇÊûúÊòØÂ≠óÂÖ∏ÔºåÂ§ÑÁêÜ SDK ÂÆ¢Êà∑Á´ØËøîÂõûÁöÑÊ∂àÊÅØÁ±ªÂûã
                    msg_type = msg.get("type", "text")
                    logger.debug(f">>> Stream msg: type={msg_type}, keys={list(msg.keys())}")
                    
                    if msg_type == "assistant":
                        # AI ÂõûÂ§çÔºàSDK ÂÆ¢Êà∑Á´ØÔºâ
                        content = msg.get("content", "")
                        full_reply += content
                        agent_info = msg.get("metadata", {}).get("agent_info")
                        yield f"data: {json.dumps({'type': 'content', 'content': content, 'agent_info': agent_info})}\n\n"
                        
                    elif msg_type == "tool_call":
                        # Â∑•ÂÖ∑Ë∞ÉÁî®ÔºàSDK ÂÆ¢Êà∑Á´ØÔºâ
                        metadata = msg.get("metadata", {})
                        tool_name = metadata.get("tool_name", "unknown")
                        tool_type = metadata.get("tool_type") or "generic"
                        status = metadata.get("status", "running")
                        agent_info = metadata.get("agent_info")
                        tool_call_id = metadata.get("tool_call_id")
                        tool_params = metadata.get("tool_params")
                        result = metadata.get("result")
                        old_content = metadata.get("old_content")
                        new_content = metadata.get("new_content")
                        
                        if status == "running":
                            # Â∑•ÂÖ∑ÂºÄÂßãÊâßË°å
                            event_data = {
                                'type': 'tool_start',
                                'tool_type': tool_type,
                                'tool_name': tool_name,
                                'tool_call_id': tool_call_id,
                                'label': msg.get("content", "") or metadata.get('label', ''),
                                'agent_info': agent_info,
                                'tool_params': tool_params,
                            }
                            logger.info(f">>> TOOL_START: {event_data}")
                            yield f"data: {json.dumps(event_data)}\n\n"
                        else:
                            # Â∑•ÂÖ∑ÊâßË°åÂÆåÊàê
                            event_data = {
                                'type': 'tool_end',
                                'tool_type': tool_type,
                                'tool_name': tool_name,
                                'tool_call_id': tool_call_id,
                                'status': status,
                                'agent_info': agent_info,
                                'tool_params': tool_params,
                                'result': result,
                                'old_content': old_content,
                                'new_content': new_content,
                            }
                            logger.info(f">>> TOOL_END: {event_data}")
                            yield f"data: {json.dumps(event_data)}\n\n"
                            
                    elif msg_type == "plan":
                        # ÊâßË°åËÆ°ÂàíÔºàSDK ÂÆ¢Êà∑Á´ØÔºâ
                        entries = msg.get("metadata", {}).get("entries", [])
                        event_data = {'type': 'plan', 'entries': entries}
                        logger.info(f">>> PLAN: {len(entries)} entries")
                        yield f"data: {json.dumps(event_data)}\n\n"
                        
                    elif msg_type == "finish":
                        # ‰ªªÂä°ÂÆåÊàêÔºàSDK ÂÆ¢Êà∑Á´ØÔºâ
                        metadata = msg.get("metadata", {})
                        logger.info(f">>> FINISH: {metadata}")
                        break
                        
                    elif msg_type == "error":
                        # ÈîôËØØÔºàSDK ÂÆ¢Êà∑Á´ØÔºâ
                        error_content = msg.get("content", "Unknown error")
                        logger.error(f">>> ERROR: {error_content}")
                        yield f"data: {json.dumps({'type': 'error', 'content': error_content})}\n\n"
                        
                    elif msg_type == "text":
                        # ÊñáÊú¨Ê∂àÊÅØÔºàÊóßÂÆ¢Êà∑Á´ØÂÖºÂÆπÔºâ
                        content = msg.get("content", "")
                        full_reply += content
                        yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                        
                    elif msg_type == "tool_start":
                        # Â∑•ÂÖ∑ÂºÄÂßãÊâßË°åÔºàÊóßÂÆ¢Êà∑Á´ØÂÖºÂÆπÔºâ
                        event_data = {'type': 'tool_start', 'tool_type': msg.get('tool_type'), 'tool_name': msg.get('tool_name'), 'label': msg.get('label', ''), 'agent_info': msg.get('agent_info')}
                        logger.info(f">>> TOOL_START: {event_data}")
                        yield f"data: {json.dumps(event_data)}\n\n"
                        
                    elif msg_type == "tool_end":
                        # Â∑•ÂÖ∑ÊâßË°åÂÆåÊàêÔºàÊóßÂÆ¢Êà∑Á´ØÂÖºÂÆπÔºâ
                        event_data = {'type': 'tool_end', 'tool_type': msg.get('tool_type'), 'tool_name': msg.get('tool_name'), 'status': msg.get('status', 'success'), 'agent_info': msg.get('agent_info')}
                        logger.info(f">>> TOOL_END: {event_data}")
                        yield f"data: {json.dumps(event_data)}\n\n"
                        
                    elif msg_type == "done":
                        # ÂÆåÊàêÔºàÊóßÂÆ¢Êà∑Á´ØÂÖºÂÆπÔºâ
                        break
                    
            logger.info(f"Stream completed, reply length: {len(full_reply)}")
        except Exception as e:
            logger.exception(f"Error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"
        if sessionId: project_manager.save_message(project_name, sessionId, "assistant", full_reply)
        yield f"data: {json.dumps({'type': 'done'})}\n\n"
    return StreamingResponse(event_generator(), media_type="text/event-stream")

@app.get("/api/projects/{project_name}/files")
async def get_project_files(project_name: str):
    return file_service.get_tree(get_project_path(project_name))

@app.get("/api/projects/{project_name}/file")
async def read_project_file(project_name: str, filePath: str):
    try: return {"content": file_service.read_file(get_project_path(project_name), filePath)}
    except: raise HTTPException(status_code=404)

@app.get("/api/projects/{project_name}/files/content")
async def read_project_file_content(project_name: str, filePath: str):
    try:
        root_path = get_project_path(project_name)

        if '..' in filePath.replace('\\', '/').split('/'):
            raise HTTPException(status_code=403, detail="Access denied: path traversal detected")

        full_path = os.path.normpath(os.path.join(root_path, filePath))
        real_root = os.path.realpath(root_path)
        real_full = os.path.realpath(full_path) if os.path.exists(full_path) else full_path

        if not real_full.startswith(real_root + os.sep) and real_full != real_root:
            raise HTTPException(status_code=403, detail="Access denied: path outside project directory")

        if not os.path.exists(full_path) or not os.path.isfile(full_path):
            raise HTTPException(status_code=404, detail="File not found")

        file_size = os.path.getsize(full_path)
        if file_size > 20 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="File too large to preview")

        media_type = mimetypes.guess_type(full_path)[0] or "application/octet-stream"
        return FileResponse(full_path, media_type=media_type)
    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"Error serving file content: {e}")
        raise HTTPException(status_code=500, detail="Error reading file")

@app.put("/api/projects/{project_name}/file")
async def save_project_file(project_name: str, req: SaveFileRequest):
    try:
        file_service.write_file(get_project_path(project_name), req.filePath, req.content)
        return {"status": "success"}
    except Exception as e: return JSONResponse(content={"error": str(e)}, status_code=500)

@app.get("/api/git/status")
async def get_git_status(project: str = Query(None)):
    path = get_project_path(project)
    logger.info(f"Getting git status for project '{project}' at path: '{path}'")
    return await git_service.get_status(path)

@app.get("/api/git/branches")
async def get_branches(project: str = Query(None)):
    branches = await git_service.get_branches(get_project_path(project))
    return {"branches": branches}

@app.get("/api/git/remote-status")
async def get_remote_status(project: str = Query(None)):
    return await git_service.get_remote_status(get_project_path(project))

@app.get("/api/git/diff")
async def get_diff(project: str = Query(None), file: str = Query(None)):
    diff = await git_service.get_diff(get_project_path(project), file)
    return {"diff": diff}

@app.get("/api/git/commits")
async def get_commits(project: str = Query(None), limit: int = 10):
    commits = await git_service.get_commits(get_project_path(project), limit)
    return {"commits": commits}

@app.get("/api/git/commit-diff")
async def get_commit_diff(project: str = Query(None), commit: str = Query(None)):
    diff = await git_service.get_commit_diff(get_project_path(project), commit)
    return {"diff": diff}

@app.post("/api/git/checkout")
async def checkout_branch(req: CheckoutRequest):
    await git_service.checkout(get_project_path(req.project), req.branch)
    return {"success": True}

@app.post("/api/git/create-branch")
async def create_new_branch(req: CheckoutRequest):
    await git_service.create_branch(get_project_path(req.project), req.branch)
    return {"success": True}

@app.post("/api/git/commit")
async def commit_changes(req: CommitRequest):
    output = await git_service.commit(get_project_path(req.project), req.message, req.files)
    return {"success": True, "output": output}

@app.get("/api/git/file-with-diff")
async def get_file_with_diff(project: str = Query(None), file: str = Query(None)):
    logger.info(f"[GitDiff] project={project}, file={file}")
    path = get_project_path(project)
    current_content = ""
    try:
        current_content = file_service.read_file(path, file)
    except Exception as e:
        logger.warning(f"[GitDiff] Failed to read current file: {e}")
        pass # File might be deleted

    old_content = await git_service.get_file_at_head(path, file)

    return {
        "currentContent": current_content,
        "oldContent": old_content
    }

@app.websocket("/shell")
async def websocket_shell(websocket: WebSocket, project: str = None):
    """WebSocket Shell Á´ØÁÇπ"""
    try:
        # Ëé∑ÂèñÈ°πÁõÆË∑ØÂæÑ
        project_path = None
        if project:
            project_path = get_project_path(project)
            logger.info(f"[Shell] È°πÁõÆË∑ØÂæÑ: {project_path}")
        else:
            project_path = os.getcwd()
            logger.info(f"[Shell] ‰ΩøÁî®ÂΩìÂâçÁõÆÂΩï: {project_path}")

        # ÂàõÂª∫ Shell ‰ºöËØù
        session = ShellSession(cwd=project_path)
        await session.start(websocket)
    except Exception as e:
        logger.exception(f"[Shell] WebSocket Á´ØÁÇπÈîôËØØ: {e}")
        try:
            await websocket.close(code=1011, reason=str(e))
        except:
            pass

@app.get("/api/user/onboarding-status")
async def onboarding_status(): return {"hasCompletedOnboarding": True}

@app.post("/api/user/complete-onboarding")
async def complete_onboarding(): return {"success": True}

@app.get("/api/projects/{project_name}/sessions")
async def get_sessions(project_name: str, limit: int = 5, offset: int = 0):
    """Ëé∑ÂèñÈ°πÁõÆÁöÑ‰ºöËØùÂàóË°®"""
    sessions = project_manager.get_sessions(project_name, limit, offset)
    return {
        "sessions": sessions,
        "hasMore": len(sessions) >= limit,
        "total": len(sessions)
    }

@app.put("/api/projects/{project_name}/sessions/{session_id}")
async def update_session_summary(project_name: str, session_id: str, request: Request):
    """Êõ¥Êñ∞ session ÁöÑËá™ÂÆö‰πâÂêçÁß∞/ÊëòË¶Å"""
    try:
        data = await request.json()
        summary = data.get("summary")

        if not summary:
            return JSONResponse(content={"error": "Summary is required"}, status_code=400)

        project_manager.update_session_summary(project_name, session_id, summary)

        return {"success": True, "summary": summary}
    except Exception as e:
        logger.error(f"Error updating session summary: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)

@app.get("/api/projects/{project_name}/sessions/{session_id}/messages")
async def get_session_messages(project_name: str, session_id: str, limit: int = None, offset: int = 0):
    """Ëé∑Âèñ session ÁöÑÊ∂àÊÅØÂàóË°®"""
    messages = project_manager.get_messages(project_name, session_id)

    # Â¶ÇÊûúÊåáÂÆö‰∫Ü limitÔºåÂàôÂàÜÈ°µ
    if limit is not None:
        messages = messages[offset:offset + limit]
    elif offset > 0:
        messages = messages[offset:]

    return {
        "messages": messages,
        "total": len(messages),
        "hasMore": limit is not None and len(messages) >= limit
    }

@app.get("/api/projects/{project_name}/sessions/{session_id}/token-usage")
async def get_token_usage(project_name: str, session_id: str):
    """Ëé∑Âèñ session ÁöÑ token ‰ΩøÁî®ÊÉÖÂÜµÔºàÁÆÄÂåñÁâàÊú¨Ôºâ"""
    try:
        messages = project_manager.get_messages(project_name, session_id)

        # ÁÆÄÂçï‰º∞ÁÆóÔºöÂÅáËÆæÊØèÊù°Ê∂àÊÅØÂ§ßÁ∫¶‰ΩøÁî®‰∏ÄÂÆöÊï∞ÈáèÁöÑ token
        # ÂÆûÈôÖÂ∫îÁî®‰∏≠Â∫îËØ•‰ªé AI ÂìçÂ∫î‰∏≠Ëé∑ÂèñÂáÜÁ°ÆÁöÑ token ËÆ°Êï∞
        total_messages = len(messages)
        estimated_tokens = total_messages * 100  # Á≤óÁï•‰º∞ÁÆó

        return {
            "totalMessages": total_messages,
            "estimatedTokens": estimated_tokens,
            "session_id": session_id
        }
    except Exception as e:
        logger.error(f"Error getting token usage: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)

# --- È°πÁõÆÂàõÂª∫Â∑•‰ΩúÊµÅ API ---

@app.get("/api/validate-path")
async def validate_path(path: str = Query(...)):
    """È™åËØÅË∑ØÂæÑÁä∂ÊÄÅÔºàÁî®‰∫éÂâçÁ´ØÂÆûÊó∂ÂèçÈ¶àÔºâ"""
    try:
        path = os.path.expanduser(path.strip())
        path = os.path.abspath(path)
        
        exists = os.path.exists(path)
        is_dir = os.path.isdir(path) if exists else False
        is_empty = False
        is_git = False
        
        if is_dir:
            try:
                is_empty = not any(os.scandir(path))
                is_git = os.path.isdir(os.path.join(path, ".git"))
            except PermissionError:
                pass
                
        parent_exists = os.path.exists(os.path.dirname(path))
        
        return {
            "exists": exists,
            "isDirectory": is_dir,
            "isEmpty": is_empty,
            "isGit": is_git,
            "parentExists": parent_exists,
            "path": path
        }
    except Exception as e:
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/api/create-workspace")
async def create_workspace(req: CreateWorkspaceRequest):
    """ÂàõÂª∫ÊàñÊ∑ªÂä†Â∑•‰ΩúÁ©∫Èó¥"""
    logger.info(f"=== ÂàõÂª∫Â∑•‰ΩúÁ©∫Èó¥ËØ∑Ê±Ç ===")
    logger.info(f"  Á±ªÂûã: {req.workspaceType}")
    logger.info(f"  Ë∑ØÂæÑ: {req.path}")
    logger.info(f"  GitHub URL: {req.githubUrl}")
    
    try:
        # ËßÑËåÉÂåñË∑ØÂæÑ
        workspace_path = os.path.expanduser(req.path.strip())
        workspace_path = os.path.abspath(workspace_path)

        # È™åËØÅË∑ØÂæÑÂÆâÂÖ®ÊÄß
        is_valid, error, normalized_path = PathValidator.validate_project_path(
            workspace_path,
            must_exist=(req.workspaceType == 'existing')
        )

        # Â¶ÇÊûúÈ™åËØÅÂ§±Ë¥•ÊòØÂõ†‰∏∫‰∏çÂú®ÂÖÅËÆ∏ÁöÑÊ†πÁõÆÂΩïËåÉÂõ¥ÂÜÖÔºåÂä®ÊÄÅÊ∑ªÂä†ËØ•Ë∑ØÂæÑ
        if not is_valid and "‰∏çÂú®ÂÖÅËÆ∏ÁöÑÊ†πÁõÆÂΩïËåÉÂõ¥ÂÜÖ" in error:
            logger.info(f"Ë∑ØÂæÑ‰∏çÂú®ÂÖÅËÆ∏ÂàóË°®‰∏≠ÔºåÂä®ÊÄÅÊ∑ªÂä†: {workspace_path}")
            # Ê∑ªÂä†ËØ•Ë∑ØÂæÑÂèäÂÖ∂Áà∂ÁõÆÂΩïÂà∞ÂÖÅËÆ∏ÂàóË°®
            PathValidator.add_allowed_root(workspace_path)
            PathValidator.add_allowed_root(os.path.dirname(workspace_path))

            # ÈáçÊñ∞È™åËØÅ
            is_valid, error, normalized_path = PathValidator.validate_project_path(
                workspace_path,
                must_exist=(req.workspaceType == 'existing')
            )

        if req.workspaceType == 'existing':
            # Â∑≤ÊúâÂ∑•‰ΩúÁ©∫Èó¥ - ÈúÄË¶ÅË∑ØÂæÑÂ≠òÂú®
            if not is_valid:
                logger.error(f"Ë∑ØÂæÑÈ™åËØÅÂ§±Ë¥•: {error}")
                return JSONResponse(
                    content={"error": f"Êó†ÊïàÁöÑÂ∑•‰ΩúÁ©∫Èó¥Ë∑ØÂæÑ: {error}"},
                    status_code=400
                )

            if not os.path.isdir(normalized_path):
                return JSONResponse(
                    content={"error": "ÊåáÂÆöÁöÑË∑ØÂæÑ‰∏çÊòØ‰∏Ä‰∏™ÁõÆÂΩï"},
                    status_code=400
                )
        else:
            # Êñ∞Âª∫Â∑•‰ΩúÁ©∫Èó¥
            parent_dir = os.path.dirname(workspace_path)
            
            # Ê£ÄÊü•Áà∂ÁõÆÂΩïÊòØÂê¶Â≠òÂú®
            if not os.path.exists(parent_dir):
                try:
                    os.makedirs(parent_dir, exist_ok=True)
                    logger.info(f"ÂàõÂª∫Áà∂ÁõÆÂΩï: {parent_dir}")
                except Exception as e:
                    return JSONResponse(
                        content={"error": f"Êó†Ê≥ïÂàõÂª∫Áà∂ÁõÆÂΩï: {e}"},
                        status_code=400
                    )
            
            if req.githubUrl:
                # ‰ªé GitHub ÂÖãÈöÜ
                logger.info(f"‰ªé GitHub ÂÖãÈöÜ: {req.githubUrl}")
                
                # ÊûÑÂª∫ git clone ÂëΩ‰ª§
                clone_url = req.githubUrl.strip()
                
                # Â¶ÇÊûúÊèê‰æõ‰∫Ü tokenÔºå‰øÆÊîπ URL ‰ª•ÂåÖÂê´ËÆ§ËØÅ‰ø°ÊÅØ
                if req.newGithubToken:
                    # Ëß£Êûê GitHub URL Âπ∂Ê≥®ÂÖ• token
                    if clone_url.startswith("https://github.com/"):
                        clone_url = clone_url.replace(
                            "https://github.com/",
                            f"https://{req.newGithubToken}@github.com/"
                        )
                    elif clone_url.startswith("https://"):
                        # ÈÄöÁî® HTTPS URL
                        clone_url = clone_url.replace(
                            "https://",
                            f"https://{req.newGithubToken}@"
                        )
                
                try:
                    # ÊâßË°å git clone
                    result = subprocess.run(
                        ["git", "clone", clone_url, workspace_path],
                        capture_output=True,
                        text=True,
                        timeout=300,  # 5ÂàÜÈíüË∂ÖÊó∂
                        cwd=parent_dir
                    )
                    
                    if result.returncode != 0:
                        error_msg = result.stderr or result.stdout or "ÂÖãÈöÜÂ§±Ë¥•"
                        # Ê∏ÖÁêÜÈîôËØØ‰ø°ÊÅØ‰∏≠ÂèØËÉΩÂåÖÂê´ÁöÑ token
                        if req.newGithubToken:
                            error_msg = error_msg.replace(req.newGithubToken, "***")
                        logger.error(f"Git clone Â§±Ë¥•: {error_msg}")
                        return JSONResponse(
                            content={"error": f"Git clone Â§±Ë¥•: {error_msg}"},
                            status_code=400
                        )
                    
                    logger.info(f"ÊàêÂäüÂÖãÈöÜ‰ªìÂ∫ìÂà∞: {workspace_path}")
                    
                except subprocess.TimeoutExpired:
                    return JSONResponse(
                        content={"error": "ÂÖãÈöÜË∂ÖÊó∂ÔºåËØ∑Ê£ÄÊü•ÁΩëÁªúËøûÊé•Êàñ‰ªìÂ∫ìÂ§ßÂ∞è"},
                        status_code=408
                    )
                except FileNotFoundError:
                    return JSONResponse(
                        content={"error": "Git Êú™ÂÆâË£ÖÊàñ‰∏çÂú®Á≥ªÁªü PATH ‰∏≠"},
                        status_code=500
                    )
                except Exception as e:
                    logger.exception(f"Git clone ÂºÇÂ∏∏: {e}")
                    return JSONResponse(
                        content={"error": f"ÂÖãÈöÜËøáÁ®ã‰∏≠ÂèëÁîüÈîôËØØ: {str(e)}"},
                        status_code=500
                    )
            else:
                # ÂàõÂª∫Á©∫ÁõÆÂΩï
                if not os.path.exists(workspace_path):
                    os.makedirs(workspace_path, exist_ok=True)
                    logger.info(f"ÂàõÂª∫Á©∫Â∑•‰ΩúÁ©∫Èó¥: {workspace_path}")
                elif os.listdir(workspace_path):
                    return JSONResponse(
                        content={"error": "ÁõÆÂΩïÂ∑≤Â≠òÂú®‰∏î‰∏ç‰∏∫Á©∫"},
                        status_code=400
                    )
            
            normalized_path = workspace_path
        
        # Â∞ÜÈ°πÁõÆÊ∑ªÂä†Âà∞È°πÁõÆÁÆ°ÁêÜÂô®
        project = project_manager.add_project(normalized_path)
        
        # Ê≥®ÂÜåÂà∞Ë∑ØÂæÑÈ™åËØÅÂô®
        project_registry.register_project(project["name"], normalized_path)
        
        logger.info(f"Â∑•‰ΩúÁ©∫Èó¥ÂàõÂª∫ÊàêÂäü: {project}")
        
        return {
            "success": True,
            "project": project,
            "message": f"{'Â∑≤Ê∑ªÂä†' if req.workspaceType == 'existing' else 'Â∑≤ÂàõÂª∫'}Â∑•‰ΩúÁ©∫Èó¥: {project['displayName']}"
        }
        
    except Exception as e:
        logger.exception(f"ÂàõÂª∫Â∑•‰ΩúÁ©∫Èó¥Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÂàõÂª∫Â∑•‰ΩúÁ©∫Èó¥Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.get("/api/browse-filesystem")
async def browse_filesystem(path: str = Query(None), include_files: bool = Query(False), limit: int = Query(100)):
    """ÊµèËßàÊñá‰ª∂Á≥ªÁªüÔºåÊèê‰æõË∑ØÂæÑËá™Âä®Ë°•ÂÖ®Âª∫ËÆÆ"""
    try:
        suggestions = []
        
        # Â§ÑÁêÜËôöÊãüÊ†πË∑ØÂæÑËØ∑Ê±Ç
        if path == "__ROOT__":
            base_dirs = []
            
            # 1. ÂßãÁªàÊ∑ªÂä†Áî®Êà∑‰∏ªÁõÆÂΩï
            home_dir = os.path.expanduser("~")
            base_dirs.append({
                "name": "Home üè†", 
                "path": home_dir, 
                "type": "directory"
            })
            
            # 2. Ê†πÊçÆÁ≥ªÁªüÊ∑ªÂä†Ê†πËäÇÁÇπ
            if platform.system() == "Windows":
                # Windows: Ê∑ªÂä†ÊâÄÊúâÈÄªËæëÈ©±Âä®Âô®
                import string
                try:
                    # Â∞ùËØï‰ΩøÁî® ctypes Ëé∑ÂèñÈ©±Âä®Âô®ÔºàÊõ¥ÂáÜÁ°ÆÔºâ
                    from ctypes import windll
                    drives = []
                    bitmask = windll.kernel32.GetLogicalDrives()
                    for letter in string.ascii_uppercase:
                        if bitmask & 1:
                            drives.append(f"{letter}:\\")
                        bitmask >>= 1
                except:
                    # ÂõûÈÄÄÂà∞ÁÆÄÂçïÁöÑÂ≠òÂú®ÊÄßÊ£ÄÊü•
                    drives = [f"{d}:\\" for d in string.ascii_uppercase if os.path.exists(f"{d}:\\")]
                
                for drive in drives:
                    base_dirs.append({
                        "name": f"Local Disk ({drive})",
                        "path": drive,
                        "type": "directory"
                    })
            else:
                # Unix/Mac: Ê∑ªÂä†Á≥ªÁªüÊ†πÁõÆÂΩï
                base_dirs.append({
                    "name": "System Root (/)",
                    "path": "/",
                    "type": "directory"
                })
                
                # Mac ÁâπÊúâ: /Volumes
                if platform.system() == "Darwin" and os.path.exists("/Volumes"):
                     base_dirs.append({
                        "name": "Volumes",
                        "path": "/Volumes",
                        "type": "directory"
                    })

            return {"suggestions": base_dirs, "currentPath": "__ROOT__"}

        # Â¶ÇÊûúÊ≤°ÊúâÊèê‰æõË∑ØÂæÑÔºåÈªòËÆ§ËøîÂõûÁî®Êà∑‰∏ªÁõÆÂΩï‰ø°ÊÅØÔºà‰øùÊåÅÂÖºÂÆπÊÄßÔºâ
        if not path or path == "~":
            home_dir = os.path.expanduser("~")
            return {"suggestions": [], "currentPath": home_dir} # ÁÆÄÂåñÔºå‰∏çÂÜçÂú®Ê≠§Â§ÑËøîÂõûÈ©±Âä®Âô®ÂàóË°®ÔºåÁî± __ROOT__ Êé•ÁÆ°
        
        # Â±ïÂºÄ ~ Á¨¶Âè∑
        expanded_path = os.path.expanduser(path)
        
        # Á°ÆÂÆöË¶ÅÊµèËßàÁöÑÁõÆÂΩï
        if os.path.isdir(expanded_path):
            browse_dir = expanded_path
            prefix = ""
        else:
            browse_dir = os.path.dirname(expanded_path)
            prefix = os.path.basename(expanded_path).lower()
        
        if not os.path.isdir(browse_dir):
            return {"suggestions": [], "currentPath": path, "error": "ÁõÆÂΩï‰∏çÂ≠òÂú®"}
        
        # ÂàóÂá∫ÁõÆÂΩïÂÜÖÂÆπ
        try:
            entries = os.listdir(browse_dir)
        except PermissionError:
            return {"suggestions": [], "currentPath": path, "error": "ÊùÉÈôê‰∏çË∂≥"}
        
        for entry in entries:
            # Ë∑≥ËøáÈöêËóèÊñá‰ª∂ÔºàÈô§ÈùûÁî®Êà∑ÊòéÁ°ÆËæìÂÖ•‰∫ÜÁÇπÂè∑ÂºÄÂ§¥Ôºâ
            if entry.startswith('.') and not prefix.startswith('.'):
                continue
            
            # ÂâçÁºÄËøáÊª§
            if prefix and not entry.lower().startswith(prefix):
                continue
            
            full_path = os.path.join(browse_dir, entry)
            is_dir = os.path.isdir(full_path)
            
            # Filter based on include_files
            if not include_files and not is_dir:
                continue
            
            suggestions.append({
                "name": entry,
                "path": full_path,
                "type": "directory" if is_dir else "file"
            })
        
        # Sort: directories first, then alphabetical
        suggestions.sort(key=lambda x: (0 if x["type"] == "directory" else 1, x["name"].lower()))
        
        # Limit results
        if limit > 0:
            suggestions = suggestions[:limit]
        
        return {
            "suggestions": suggestions,
            "currentPath": expanded_path
        }
        
    except Exception as e:
        logger.error(f"ÊµèËßàÊñá‰ª∂Á≥ªÁªüÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÊµèËßàÊñá‰ª∂Á≥ªÁªüÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.get("/api/search-filesystem")
async def search_filesystem(q: str = Query(...), path: str = Query(None), limit: int = Query(50)):
    """Search for directories matching query"""
    start_dir = os.path.expanduser(path or "~")
    
    def _search():
        results = []
        if not os.path.exists(start_dir):
            return results
        
        try:
            count = 0
            # Limit depth effectively by not following hidden/large dirs
            exclude_dirs = {'node_modules', 'Library', 'venv', '__pycache__', '.git', '.idea', '.vscode'}
            
            for root, dirs, files in os.walk(start_dir):
                # Filter in-place to prevent recursion
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in exclude_dirs]
                
                for d in dirs:
                    if q.lower() in d.lower():
                        full_path = os.path.join(root, d)
                        results.append({
                            "name": d,
                            "path": full_path,
                            "type": "directory"
                        })
                        count += 1
                        if count >= limit:
                            return results
                
                if len(results) >= limit:
                    break
        except Exception as e:
            logger.error(f"Search error: {e}")
            pass
        return results

    results = await asyncio.to_thread(_search)
    return {"results": results}


@app.post("/api/projects/create")
async def create_project(req: CreateProjectRequest):
    """ÂàõÂª∫È°πÁõÆÔºàÁÆÄÂçïÁâàÊú¨ - ‰ªÖÊ∑ªÂä†Áé∞ÊúâË∑ØÂæÑÔºâ"""
    try:
        workspace_path = os.path.expanduser(req.path.strip())
        workspace_path = os.path.abspath(workspace_path)

        if not os.path.isdir(workspace_path):
            return JSONResponse(
                content={"error": "ÊåáÂÆöÁöÑË∑ØÂæÑ‰∏çÂ≠òÂú®Êàñ‰∏çÊòØÁõÆÂΩï"},
                status_code=400
            )

        # Â∞ùËØïÊ≥®ÂÜåÈ°πÁõÆÔºåÂ¶ÇÊûúÂ§±Ë¥•ÂàôÂä®ÊÄÅÊ∑ªÂä†Ë∑ØÂæÑ
        project = project_manager.add_project(workspace_path)

        # Ê≥®ÂÜåÂà∞È°πÁõÆÊ≥®ÂÜåË°®
        is_registered, error = project_registry.register_project(project["name"], workspace_path)
        if not is_registered:
            # Â¶ÇÊûúÊ≥®ÂÜåÂ§±Ë¥•ÊòØÂõ†‰∏∫Ë∑ØÂæÑ‰∏çÂú®ÂÖÅËÆ∏ÂàóË°®‰∏≠ÔºåÂä®ÊÄÅÊ∑ªÂä†
            if "‰∏çÂú®ÂÖÅËÆ∏ÁöÑÊ†πÁõÆÂΩïËåÉÂõ¥ÂÜÖ" in error:
                logger.info(f"Ë∑ØÂæÑ‰∏çÂú®ÂÖÅËÆ∏ÂàóË°®‰∏≠ÔºåÂä®ÊÄÅÊ∑ªÂä†: {workspace_path}")
                PathValidator.add_allowed_root(workspace_path)
                PathValidator.add_allowed_root(os.path.dirname(workspace_path))

                # ÈáçÊñ∞Ê≥®ÂÜå
                is_registered, error = project_registry.register_project(project["name"], workspace_path)

            if not is_registered:
                logger.error(f"Ê≥®ÂÜåÈ°πÁõÆÂ§±Ë¥•: {error}")
                return JSONResponse(
                    content={"error": f"Ê≥®ÂÜåÈ°πÁõÆÂ§±Ë¥•: {error}"},
                    status_code=400
                )

        return {"success": True, "project": project}

    except Exception as e:
        logger.exception(f"ÂàõÂª∫È°πÁõÆÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÂàõÂª∫È°πÁõÆÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/projects/create-workspace")
async def create_workspace(req: CreateWorkspaceRequest):
    """ÂàõÂª∫Â∑•‰ΩúÁ©∫Èó¥ÔºàÂÆåÊï¥ÊµÅÁ®ãÔºâ"""
    try:
        workspace_path = os.path.expanduser(req.path.strip())
        workspace_path = os.path.abspath(workspace_path)
        
        # 1. È™åËØÅÊàñÂàõÂª∫ÁõÆÂΩï
        if req.workspaceType == 'new':
            if os.path.exists(workspace_path):
                if not os.path.isdir(workspace_path):
                    return JSONResponse(content={"error": "Ë∑ØÂæÑÂ≠òÂú®‰∏î‰∏çÊòØÁõÆÂΩï"}, status_code=400)
                # ÂÖÅËÆ∏Âú®Á©∫ÁõÆÂΩï‰∏≠ÂàõÂª∫ÔºàÊàñÈùûÁ©∫ÁõÆÂΩï‰ΩÜÁî®Êà∑Â∑≤Á°ÆËÆ§Ôºâ
            else:
                try:
                    os.makedirs(workspace_path, exist_ok=True)
                except Exception as e:
                    return JSONResponse(content={"error": f"Êó†Ê≥ïÂàõÂª∫ÁõÆÂΩï: {str(e)}"}, status_code=500)
                
            # 2. Â§ÑÁêÜ GitHub ÂÖãÈöÜ
            if req.githubUrl:
                repo_url = req.githubUrl
                if req.newGithubToken:
                    # ÊèíÂÖ• token: https://TOKEN@github.com/...
                    if repo_url.startswith("https://"):
                        repo_url = repo_url.replace("https://", f"https://{req.newGithubToken}@")
                
                try:
                    # Ê£ÄÊü•ÁõÆÂΩïÊòØÂê¶‰∏∫Á©∫
                    if os.path.exists(workspace_path) and any(os.scandir(workspace_path)):
                         return JSONResponse(content={"error": "ÁõÆÊ†áÁõÆÂΩïÈùûÁ©∫ÔºåÊó†Ê≥ïÂÖãÈöÜ‰ªìÂ∫ì"}, status_code=400)

                    process = await asyncio.create_subprocess_exec(
                        "git", "clone", repo_url, ".",
                        cwd=workspace_path,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    stdout, stderr = await process.communicate()
                    if process.returncode != 0:
                        err_msg = stderr.decode()
                        # ÁÆÄÂçïÈöêËóè token
                        if req.newGithubToken:
                            err_msg = err_msg.replace(req.newGithubToken, "***")
                        return JSONResponse(content={"error": f"ÂÖãÈöÜÂ§±Ë¥•: {err_msg}"}, status_code=400)
                except Exception as e:
                    return JSONResponse(content={"error": f"GitÊìç‰ΩúÂ§±Ë¥•: {str(e)}"}, status_code=500)

        elif req.workspaceType == 'existing':
            if not os.path.isdir(workspace_path):
                return JSONResponse(content={"error": "Ë∑ØÂæÑ‰∏çÂ≠òÂú®"}, status_code=400)

        # 3. Ê≥®ÂÜåÈ°πÁõÆ
        # ‰ΩøÁî® project_manager Ê∑ªÂä†
        project = project_manager.add_project(workspace_path)
        
        # Ê≥®ÂÜåÂà∞ registry (‰∏∫‰∫ÜÂÖÅËÆ∏ËÆøÈóÆ)
        is_registered, error = project_registry.register_project(project["name"], workspace_path)
        
        if not is_registered:
             if "‰∏çÂú®ÂÖÅËÆ∏ÁöÑÊ†πÁõÆÂΩïËåÉÂõ¥ÂÜÖ" in error:
                logger.info(f"Ë∑ØÂæÑ‰∏çÂú®ÂÖÅËÆ∏ÂàóË°®‰∏≠ÔºåÂä®ÊÄÅÊ∑ªÂä†: {workspace_path}")
                PathValidator.add_allowed_root(workspace_path)
                PathValidator.add_allowed_root(os.path.dirname(workspace_path))
                is_registered, error = project_registry.register_project(project["name"], workspace_path)
        
        if not is_registered:
             return JSONResponse(content={"error": f"Ê≥®ÂÜåÈ°πÁõÆÂ§±Ë¥•: {error}"}, status_code=400)

        return {"success": True, "project": project}

    except Exception as e:
        logger.exception(f"ÂàõÂª∫Â∑•‰ΩúÁ©∫Èó¥Â§±Ë¥•: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)


@app.post("/api/error-analyze")
async def analyze_error(request: Request):
    """ÂàÜÊûêÈîôËØØÂπ∂Êèê‰æõ‰øÆÂ§çÂª∫ËÆÆ"""
    try:
        data = await request.json()
        error_output = data.get('error', '')
        project_path = data.get('projectPath', '')

        if not error_output:
            return JSONResponse(
                content={"error": "ÈîôËØØËæìÂá∫‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        # Ëé∑ÂèñÈîôËØØÂàÜÊûêÂô®
        analyzer = get_error_analyzer(project_path) if project_path else ErrorAnalyzer('.')

        # ÂàÜÊûêÈîôËØØ
        analysis = analyzer.analyze_error(error_output, project_path)

        # Ëé∑Âèñ‰ª£Á†Å‰∏ä‰∏ãÊñá
        if analysis['error_info']['file'] and analysis['error_info']['line']:
            context = analyzer.get_error_context(
                analysis['error_info']['file'],
                analysis['error_info']['line']
            )
            analysis['code_context'] = context

        # ÁîüÊàêËá™Âä®‰øÆÂ§çÊñπÊ°à
        if analysis['can_auto_fix']:
            auto_fix = analyzer.generate_auto_fix(error_output, project_path)
            analysis['auto_fix'] = auto_fix

        return {
            "success": True,
            "analysis": analysis
        }

    except Exception as e:
        logger.exception(f"ÈîôËØØÂàÜÊûêÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÈîôËØØÂàÜÊûêÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/auto-fix")
async def auto_fix_error(request: Request):
    """Ëá™Âä®Ê£ÄÊµãÂπ∂‰øÆÂ§çÈîôËØØ"""
    try:
        data = await request.json()
        error_output = data.get('error', '')
        project_path = data.get('projectPath', '')
        context = data.get('context', {})

        if not error_output:
            return JSONResponse(
                content={"error": "ÈîôËØØËæìÂá∫‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        if not project_path:
            return JSONResponse(
                content={"error": "È°πÁõÆË∑ØÂæÑ‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        logger.info(f"ÂºÄÂßãËá™Âä®‰øÆÂ§ç: {error_output[:100]}...")

        # Ëé∑Âèñ Agent ÂÆû‰æã
        agent = get_agent(
            project_path,
            global_config["mode"],
            global_config.get("model"),
            global_config.get("mcp_servers")
        )

        # Ëé∑ÂèñËá™Âä®‰øÆÂ§çÂô®
        auto_fixer = get_auto_fixer(project_path, agent)

        # ÊâßË°åËá™Âä®‰øÆÂ§ç
        result = await auto_fixer.detect_and_fix(error_output, context)

        logger.info(f"Ëá™Âä®‰øÆÂ§çÁªìÊûú: {result}")

        return {
            "success": True,
            "result": result
        }

    except Exception as e:
        logger.exception(f"Ëá™Âä®‰øÆÂ§çÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ëá™Âä®‰øÆÂ§çÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.get("/api/auto-fix/history")
async def get_auto_fix_history(projectPath: str = Query(..., description="È°πÁõÆË∑ØÂæÑ")):
    """Ëé∑ÂèñËá™Âä®‰øÆÂ§çÂéÜÂè≤"""
    try:
        auto_fixer = get_auto_fixer(projectPath)
        history = auto_fixer.get_fix_history()

        return {
            "success": True,
            "history": history
        }

    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰øÆÂ§çÂéÜÂè≤Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ëé∑Âèñ‰øÆÂ§çÂéÜÂè≤Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.delete("/api/auto-fix/history")
async def clear_auto_fix_history(projectPath: str = Query(..., description="È°πÁõÆË∑ØÂæÑ")):
    """Ê∏ÖÁ©∫Ëá™Âä®‰øÆÂ§çÂéÜÂè≤"""
    try:
        auto_fixer = get_auto_fixer(projectPath)
        auto_fixer.clear_history()

        return {
            "success": True,
            "message": "‰øÆÂ§çÂéÜÂè≤Â∑≤Ê∏ÖÁ©∫"
        }

    except Exception as e:
        logger.exception(f"Ê∏ÖÁ©∫‰øÆÂ§çÂéÜÂè≤Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ê∏ÖÁ©∫‰øÆÂ§çÂéÜÂè≤Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/context/analyze")
async def analyze_context(request: Request):
    """ÂàÜÊûêÈ°πÁõÆ‰∏ä‰∏ãÊñáÔºà‰æùËµñÂÖ≥Á≥ª„ÄÅË∞ÉÁî®ÂÖ≥Á≥ª„ÄÅÁ±ªÁªßÊâøÔºâ"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')
        include_dirs = data.get('includeDirs', [])

        if not project_path:
            return JSONResponse(
                content={"error": "È°πÁõÆË∑ØÂæÑ‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        logger.info(f"ÂºÄÂßãÂàÜÊûêÈ°πÁõÆ‰∏ä‰∏ãÊñá: {project_path}")

        # Ëé∑Âèñ‰æùËµñÂàÜÊûêÂô®
        analyzer = get_dependency_analyzer(project_path)

        # ÂàÜÊûêÈ°πÁõÆ
        result = analyzer.analyze_project(include_dirs)

        logger.info(f"È°πÁõÆ‰∏ä‰∏ãÊñáÂàÜÊûêÂÆåÊàê: {len(result['modules'])} ‰∏™Ê®°Âùó")

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        logger.exception(f"ÂàÜÊûêÈ°πÁõÆ‰∏ä‰∏ãÊñáÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÂàÜÊûêÈ°πÁõÆ‰∏ä‰∏ãÊñáÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.get("/api/context/module/{module_name}")
async def get_module_context(module_name: str, projectPath: str = Query(..., description="È°πÁõÆË∑ØÂæÑ")):
    """Ëé∑ÂèñÁâπÂÆöÊ®°ÂùóÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ"""
    try:
        analyzer = get_dependency_analyzer(projectPath)

        if module_name not in analyzer.modules:
            return JSONResponse(
                content={"error": f"Ê®°Âùó {module_name} ‰∏çÂ≠òÂú®"},
                status_code=404
            )

        module = analyzer.modules[module_name]

        # Ëé∑Âèñ‰æùËµñÁöÑÊ®°Âùó
        dependencies = set(module.imports)
        for from_module in module.from_imports.keys():
            dependencies.add(from_module)

        # Ëé∑ÂèñË¢´‰æùËµñÁöÑÊ®°Âùó
        dependents = []
        for other_module_name, other_module in analyzer.modules.items():
            if module_name in other_module.imports or module_name in other_module.from_imports:
                dependents.append(other_module_name)

        return {
            "success": True,
            "module": {
                "name": module_name,
                "file_path": module.file_path,
                "imports": list(module.imports),
                "from_imports": {k: list(v) for k, v in module.from_imports.items()},
                "functions": {
                    func_name: {
                        "line": func_info.line_start,
                        "parameters": func_info.parameters,
                        "calls": list(func_info.calls),
                        "is_async": func_info.is_async,
                        "is_method": func_info.is_method,
                        "class_name": func_info.class_name
                    }
                    for func_name, func_info in module.functions.items()
                },
                "classes": {
                    class_name: {
                        "line": class_info.line_start,
                        "bases": class_info.bases,
                        "methods": list(class_info.methods.keys()),
                        "attributes": list(class_info.attributes)
                    }
                    for class_name, class_info in module.classes.items()
                }
            },
            "dependencies": list(dependencies),
            "dependents": dependents
        }

    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊ®°Âùó‰∏ä‰∏ãÊñáÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ëé∑ÂèñÊ®°Âùó‰∏ä‰∏ãÊñáÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/code-style-analyze")
async def analyze_code_style(request: Request):
    """ÂàÜÊûêÈ°πÁõÆ‰ª£Á†ÅÈ£éÊ†º"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')

        if not project_path:
            return JSONResponse(
                content={"error": "È°πÁõÆË∑ØÂæÑ‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        # Ëé∑Âèñ‰ª£Á†ÅÈ£éÊ†ºÂàÜÊûêÂô®
        analyzer = get_code_style_analyzer(project_path)

        # ÂàÜÊûê‰ª£Á†ÅÈ£éÊ†º
        style_profile = analyzer.analyze_project_style()
        style_summary = analyzer.get_style_summary()

        return {
            "success": True,
            "styleProfile": style_profile,
            "summary": style_summary
        }

    except Exception as e:
        logger.exception(f"‰ª£Á†ÅÈ£éÊ†ºÂàÜÊûêÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"‰ª£Á†ÅÈ£éÊ†ºÂàÜÊûêÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/prompt-optimize")
async def optimize_prompt(request: Request):
    """Ê†πÊçÆÈ°πÁõÆÁâπÂæÅÊô∫ËÉΩ‰ºòÂåñÁî®Êà∑ËæìÂÖ•ÁöÑÊ∂àÊÅØÔºà‰ΩøÁî®Â§ßÊ®°ÂûãÔºâ"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')
        user_input = data.get('userInput', '')
        base_persona = data.get('persona', 'partner')

        if not project_path:
            return JSONResponse(
                content={"error": "È°πÁõÆË∑ØÂæÑ‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        if not user_input:
            return JSONResponse(
                content={"error": "Áî®Êà∑ËæìÂÖ•‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        logger.info(f"ÂºÄÂßãÊô∫ËÉΩ‰ºòÂåñÊ∂àÊÅØ: project={project_path}, persona={base_persona}, input={user_input[:100]}...")

        # Ëé∑ÂèñÊèêÁ§∫ËØç‰ºòÂåñÂô®
        optimizer = get_prompt_optimizer(project_path)

        # ÂÖàÂàÜÊûêÈ°πÁõÆÔºàËøô‰ºöÊâ´ÊèèÈ°πÁõÆ‰ª£Á†ÅÔºâ
        analysis = optimizer.analyze_project()

        # ÂàÜÊûêÁî®Êà∑ÊÑèÂõæ
        intent = optimizer.analyze_user_intent(user_input)

        # Êü•ÊâæÁõ∏ÂÖ≥‰ª£Á†Å
        relevant_code = optimizer.find_relevant_code(user_input, intent)

        # ÊûÑÂª∫È°πÁõÆ‰∏ä‰∏ãÊñá
        project_context = optimizer._build_project_context()
        style_guide = optimizer._build_style_guide()

        # ÊûÑÂª∫‰ºòÂåñÊèêÁ§∫ËØç
        optimization_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑÊèêÁ§∫ËØç‰ºòÂåñ‰∏ìÂÆ∂„ÄÇËØ∑Ê†πÊçÆ‰ª•‰∏ã‰ø°ÊÅØÔºå‰ºòÂåñÁî®Êà∑ÁöÑËæìÂÖ•Ê∂àÊÅØÔºå‰ΩøÂÖ∂Êõ¥ÂÖ∑‰Ωì„ÄÅÊõ¥Á¨¶ÂêàÈ°πÁõÆÁöÑÂÆûÈôÖÊÉÖÂÜµ„ÄÇ

## È°πÁõÆ‰ø°ÊÅØ
{project_context}

## ‰ª£Á†ÅÈ£éÊ†ºÊåáÂçó
{style_guide}

## Áî®Êà∑ÊÑèÂõæ
- ÊÑèÂõæÁ±ªÂûã: {intent.get('type', 'unknown')}
- ÂÖ≥ÈîÆËØç: {', '.join(intent.get('keywords', []))}
- ÂÆû‰Ωì: {', '.join(intent.get('entities', []))}

## Áõ∏ÂÖ≥‰ª£Á†Å
"""
        if relevant_code:
            for code in relevant_code:
                if code['type'] == 'function':
                    optimization_prompt += f"- ÂáΩÊï∞: {code['name']} (Âú® {code['file']})"
                else:
                    optimization_prompt += f"- Á±ª: {code['name']} (Âú® {code['file']})"
        else:
            optimization_prompt += "- Êó†Áõ∏ÂÖ≥‰ª£Á†Å"

        optimization_prompt += f"""

## Áî®Êà∑ÂéüÂßãËæìÂÖ•
{user_input}

## ‰ªªÂä°
ËØ∑‰ºòÂåñÁî®Êà∑ÁöÑËæìÂÖ•Ê∂àÊÅØÔºå‰ΩøÂÖ∂Ôºö
1. ÂåÖÂê´È°πÁõÆËÉåÊôØ‰ø°ÊÅØ
2. ÂºïÁî®Áõ∏ÂÖ≥ÁöÑ‰ª£Á†ÅÔºàÂ¶ÇÊûúÊúâÔºâ
3. ÊòéÁ°Æ‰ª£Á†ÅÈ£éÊ†ºË¶ÅÊ±Ç
4. Ê†πÊçÆÊÑèÂõæÁ±ªÂûãÊ∑ªÂä†ÂÖ∑‰ΩìË¶ÅÊ±Ç
5. ËÆ© AI ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£È°πÁõÆ‰∏ä‰∏ãÊñáÂπ∂Êèê‰æõÂáÜÁ°ÆÁöÑËß£ÂÜ≥ÊñπÊ°à

ËØ∑Áõ¥Êé•ËæìÂá∫‰ºòÂåñÂêéÁöÑÊ∂àÊÅØÔºå‰∏çË¶ÅÂåÖÂê´‰ªª‰ΩïËß£ÈáäÊàñÈ¢ùÂ§ñÊñáÂ≠ó„ÄÇ"""

        logger.info("Ë∞ÉÁî®Â§ßÊ®°Âûã‰ºòÂåñÊ∂àÊÅØ...")

        # ÂàõÂª∫ iFlow ÂÆ¢Êà∑Á´Ø
        from backend.core.iflow_client import create_iflow_client
        iflow_client = create_iflow_client(
            cwd=project_path,
            mode=global_config.get("mode", "yolo"),
            model=global_config.get("model", "GLM-4.7")
        )

        # Ë∞ÉÁî®Â§ßÊ®°Âûã
        optimized_message = ""
        async for chunk in iflow_client.chat_stream(optimization_prompt):
            optimized_message += chunk

        optimized_message = optimized_message.strip()

        logger.info(f"Â§ßÊ®°Âûã‰ºòÂåñÂÆåÊàêÔºåÊ∂àÊÅØÈïøÂ∫¶: {len(optimized_message)}")

        return {
            "success": True,
            "analysis": analysis,
            "intent": intent,
            "relevantCode": relevant_code,
            "originalInput": user_input,
            "optimizedMessage": optimized_message,
            "projectContext": project_context,
            "codeStyleGuide": style_guide
        }

    except Exception as e:
        logger.exception(f"Êô∫ËÉΩÊ∂àÊÅØ‰ºòÂåñÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Êô∫ËÉΩÊ∂àÊÅØ‰ºòÂåñÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/generate-report")
async def generate_report(req: dict):
    """ÁîüÊàêÂ∑•‰ΩúÊä•Âëä"""
    try:
        project_path = req.get("projectPath")
        report_type = req.get("type", "daily")
        
        if not project_path:
            return JSONResponse(
                content={"error": "È°πÁõÆË∑ØÂæÑ‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )
        
        expanded_path = os.path.expanduser(project_path)
        
        # È™åËØÅË∑ØÂæÑ
        is_valid, error, normalized = PathValidator.validate_project_path(expanded_path)
        if not is_valid:
            return JSONResponse(
                content={"error": f"Êó†ÊïàÁöÑÈ°πÁõÆË∑ØÂæÑ: {error}"},
                status_code=400
            )
        
        analyzer = get_report_generator()
        report = analyzer.generate_report(normalized, report_type)
        
        return {"success": True, "report": report}
        
    except Exception as e:
        logger.exception(f"ÁîüÊàêÊä•ÂëäÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÁîüÊàêÊä•ÂëäÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/context-analyze")
async def analyze_context(req: dict):
    """ÂàÜÊûê‰ª£Á†Å‰∏ä‰∏ãÊñáÂíå‰æùËµñÂÖ≥Á≥ª"""
    try:
        project_path = req.get("projectPath")
        node_id = req.get("nodeId")
        max_depth = req.get("maxDepth", 2)
        
        if not project_path:
            return JSONResponse(
                content={"error": "È°πÁõÆË∑ØÂæÑ‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )
        
        expanded_path = os.path.expanduser(project_path)
        
        # È™åËØÅË∑ØÂæÑ
        is_valid, error, normalized = PathValidator.validate_project_path(expanded_path)
        if not is_valid:
            return JSONResponse(
                content={"error": f"Êó†ÊïàÁöÑÈ°πÁõÆË∑ØÂæÑ: {error}"},
                status_code=400
            )
        
        analyzer = get_dependency_analyzer()
        
        # ÂàÜÊûêÈ°πÁõÆ
        analyzer.analyze_project(normalized)
        
        # Â¶ÇÊûúÊåáÂÆö‰∫ÜËäÇÁÇπ IDÔºåËé∑Âèñ‰∏ä‰∏ãÊñáÂõæË∞±
        if node_id:
            context_graph = analyzer.get_context_graph(node_id, max_depth)
            return {"success": True, "graph": context_graph}
        
        # Âê¶ÂàôËøîÂõûÊâÄÊúâËäÇÁÇπÂàóË°®
        nodes = []
        for node_id, node in analyzer.nodes.items():
            nodes.append({
                'id': node_id,
                'name': node.name,
                'type': node.type.value,
                'file': os.path.basename(node.file_path),
                'line': node.line_number,
                'full_path': node.file_path
            })
        
        return {"success": True, "nodes": nodes[:100]}  # ÈôêÂà∂ËøîÂõûÊï∞Èáè
        
    except Exception as e:
        logger.exception(f"ÂàÜÊûê‰∏ä‰∏ãÊñáÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÂàÜÊûê‰∏ä‰∏ãÊñáÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/context-search")
async def search_context(req: dict):
    """ÊêúÁ¥¢‰ª£Á†ÅËäÇÁÇπ"""
    try:
        project_path = req.get("projectPath")
        query = req.get("query")
        limit = req.get("limit", 20)
        
        if not project_path or not query:
            return JSONResponse(
                content={"error": "È°πÁõÆË∑ØÂæÑÂíåÊü•ËØ¢ËØç‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )
        
        expanded_path = os.path.expanduser(project_path)
        
        # È™åËØÅË∑ØÂæÑ
        is_valid, error, normalized = PathValidator.validate_project_path(expanded_path)
        if not is_valid:
            return JSONResponse(
                content={"error": f"Êó†ÊïàÁöÑÈ°πÁõÆË∑ØÂæÑ: {error}"},
                status_code=400
            )
        
        analyzer = get_dependency_analyzer()
        analyzer.analyze_project(normalized)
        
        results = analyzer.search_nodes(query, limit)
        
        return {"success": True, "results": results}

    except Exception as e:
        logger.exception(f"ÊêúÁ¥¢‰∏ä‰∏ãÊñáÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÊêúÁ¥¢‰∏ä‰∏ãÊñáÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/query")
async def simple_query(req: dict):
    """ÁÆÄÂçïÁöÑÂêåÊ≠•Êü•ËØ¢ API - Âø´ÈÄüËé∑Âèñ AI ÂìçÂ∫î"""
    try:
        from backend.core.iflow_client import query_sync

        prompt = req.get("prompt")
        project = req.get("project")
        model = req.get("model")
        system_prompt = req.get("system_prompt")
        timeout = req.get("timeout", 300.0)

        if not prompt:
            return JSONResponse(
                content={"error": "prompt ‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        # Ëé∑ÂèñÈ°πÁõÆË∑ØÂæÑ
        cwd = None
        if project:
            cwd = get_project_path(project)

        # ÊâßË°åÊü•ËØ¢
        response = query_sync(
            prompt=prompt,
            cwd=cwd,
            model=model,
            system_prompt=system_prompt,
            timeout=timeout
        )

        return {"success": True, "response": response}

    except Exception as e:
        logger.exception(f"ÁÆÄÂçïÊü•ËØ¢Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Êü•ËØ¢Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.get("/api/mcp/config/read")
async def get_mcp_config():
    """ËØªÂèñ MCP ÈÖçÁΩÆ"""
    try:
        iflow_config_path = os.path.expanduser("~/.iflow/settings.json")
        
        if not os.path.exists(iflow_config_path):
            return {"success": False, "error": "iFlow ÈÖçÁΩÆÊñá‰ª∂‰∏çÂ≠òÂú®"}
        
        with open(iflow_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        mcp_servers = config.get("mcpServers", {})
        
        # ËΩ¨Êç¢‰∏∫ÂâçÁ´ØÈúÄË¶ÅÁöÑÊ†ºÂºè
        servers = []
        for name, server_config in mcp_servers.items():
            servers.append({
                "id": name,
                "name": name,
                "type": server_config.get("type", "stdio"),
                "scope": "user",
                "config": server_config,
                "created": datetime.now().isoformat(),
                "updated": datetime.now().isoformat()
            })
        
        return {"success": True, "servers": servers}
    except Exception as e:
        logger.exception(f"ËØªÂèñ MCP ÈÖçÁΩÆÂ§±Ë¥•: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/mcp/cli/list")
async def list_mcp_cli():
    """ÈÄöËøá CLI ÂàóÂá∫ MCP ÊúçÂä°Âô®"""
    try:
        # Â∞ùËØïÈÄöËøá iflow mcp list ÂëΩ‰ª§Ëé∑Âèñ
        result = subprocess.run(
            ["iflow", "mcp", "list"],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            # Ëß£Êûê CLI ËæìÂá∫
            servers = []
            # ËøôÈáåÈúÄË¶ÅÊ†πÊçÆÂÆûÈôÖËæìÂá∫Ê†ºÂºèËß£Êûê
            # ÊöÇÊó∂ËøîÂõûÁ©∫ÂàóË°®
            return {"success": True, "servers": servers}
        else:
            return {"success": False, "error": result.stderr}
    except Exception as e:
        logger.warning(f"ÈÄöËøá CLI ÂàóÂá∫ MCP ÊúçÂä°Âô®Â§±Ë¥•: {e}")
        return {"success": False, "error": str(e)}


@app.get("/api/mcp/servers")
async def get_mcp_servers(scope: str = "user"):
    """Ëé∑Âèñ MCP ÊúçÂä°Âô®ÂàóË°®"""
    try:
        # ‰ªé global_config Ëé∑Âèñ
        servers = global_config.get("mcp_servers", [])
        return {"success": True, "servers": servers}
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ MCP ÊúçÂä°Âô®Â§±Ë¥•: {e}")
        return {"success": False, "error": str(e)}


@app.post("/api/context/analyze-dependencies")
async def analyze_code_dependencies(request: Request):
    """ÂàÜÊûê‰ª£Á†Å‰æùËµñÂÖ≥Á≥ªÂπ∂ÁîüÊàêÂèØËßÜÂåñÊï∞ÊçÆ"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')

        if not project_path:
            return JSONResponse(
                content={"error": "È°πÁõÆË∑ØÂæÑ‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        # È™åËØÅË∑ØÂæÑ
        is_valid, error, normalized = PathValidator.validate_project_path(project_path)
        if not is_valid:
            return JSONResponse(
                content={"error": error},
                status_code=400
            )

        # Ëé∑Âèñ‰æùËµñÂàÜÊûêÂô®
        analyzer = get_code_dependency_analyzer(normalized)

        # ÂàÜÊûê‰æùËµñÂÖ≥Á≥ª
        result = analyzer.analyze_project_dependencies()

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        logger.exception(f"‰ª£Á†Å‰æùËµñÂàÜÊûêÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"‰ª£Á†Å‰æùËµñÂàÜÊûêÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/context/analyze-module")
async def analyze_module_dependencies(request: Request):
    """ÂàÜÊûêÁâπÂÆöÊ®°ÂùóÁöÑ‰æùËµñÂÖ≥Á≥ª"""
    try:
        data = await request.json()
        project_path = data.get('projectPath', '')
        module_name = data.get('moduleName', '')

        if not project_path:
            return JSONResponse(
                content={"error": "È°πÁõÆË∑ØÂæÑ‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        if not module_name:
            return JSONResponse(
                content={"error": "Ê®°ÂùóÂêçÁß∞‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )

        # È™åËØÅË∑ØÂæÑ
        is_valid, error, normalized = PathValidator.validate_project_path(project_path)
        if not is_valid:
            return JSONResponse(
                content={"error": error},
                status_code=400
            )

        # Ëé∑Âèñ‰æùËµñÂàÜÊûêÂô®
        analyzer = get_code_dependency_analyzer(normalized)

        # ÂàÜÊûêÊ®°Âùó‰æùËµñ
        result = analyzer.analyze_module_dependencies(module_name)

        return {
            "success": True,
            "data": result
        }

    except Exception as e:
        logger.exception(f"Ê®°Âùó‰æùËµñÂàÜÊûêÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ê®°Âùó‰æùËµñÂàÜÊûêÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


# --- TaskMaster API Á´ØÁÇπ ---

@app.get("/api/taskmaster/installation-status")
async def get_taskmaster_installation_status():
    """Ëé∑Âèñ TaskMaster ÂÆâË£ÖÁä∂ÊÄÅ"""
    return {
        "installation": {"isInstalled": False},
        "isReady": False
    }

@app.get("/api/taskmaster/tasks/{project_name}")
async def get_taskmaster_tasks(project_name: str):
    """Ëé∑ÂèñÈ°πÁõÆÁöÑ‰ªªÂä°ÂàóË°®"""
    try:
        project_path = get_project_path(project_name)

        # ‰ªé task_master_service Ëé∑Âèñ‰ªªÂä°ÂàóË°®
        tasks = task_master_service.get_tasks(project_name)

        # ÁªüËÆ°‰ªªÂä°Áä∂ÊÄÅ
        total = len(tasks)
        completed = sum(1 for task in tasks if task.get("status") == "completed")

        return {
            "success": True,
            "tasks": tasks,
            "total": total,
            "completed": completed
        }
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰ªªÂä°ÂàóË°®Â§±Ë¥•: {e}")
        return {
            "success": False,
            "error": str(e),
            "tasks": [],
            "total": 0,
            "completed": 0
        }

@app.get("/api/taskmaster/prd/{project_name}")
async def get_taskmaster_prd(project_name: str):
    """Ëé∑ÂèñÈ°πÁõÆÁöÑ PRD ÊñáÊ°£"""
    try:
        project_path = get_project_path(project_name)

        # Â∞ùËØïÊü•Êâæ PRD Êñá‰ª∂ÔºàÂ∏∏ËßÅÁöÑ PRD Êñá‰ª∂ÂêçÔºâ
        prd_filenames = [
            "PRD.md",
            "prd.md",
            "PRODUCT_REQUIREMENTS.md",
            "product_requirements.md",
            "REQUIREMENTS.md",
            "requirements.md"
        ]

        prd_content = None
        prd_file = None

        for filename in prd_filenames:
            prd_file_path = os.path.join(project_path, filename)
            if os.path.exists(prd_file_path) and os.path.isfile(prd_file_path):
                try:
                    with open(prd_file_path, 'r', encoding='utf-8') as f:
                        prd_content = f.read()
                    prd_file = filename
                    break
                except Exception as e:
                    logger.warning(f"ËØªÂèñ PRD Êñá‰ª∂ {filename} Â§±Ë¥•: {e}")
                    continue

        return {
            "success": True,
            "prd": prd_content,
            "exists": prd_content is not None,
            "file": prd_file
        }
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ PRD ÊñáÊ°£Â§±Ë¥•: {e}")
        return {
            "success": False,
            "error": str(e),
            "prd": None,
            "exists": False
        }

# --- Cursor Sessions API Á´ØÁÇπ ---

@app.get("/api/cursor/sessions")
async def get_cursor_sessions(projectPath: str = Query(...)):
    """Ëé∑Âèñ Cursor sessions ÂàóË°®"""
    # TODO: ÂÆûÁé∞ Cursor sessions ËØªÂèñÈÄªËæë
    # Cursor sessions ÈÄöÂ∏∏Â≠òÂÇ®Âú® ~/.cursor/sessions/ ÁõÆÂΩï‰∏ã
    return {
        "success": True,
        "sessions": []
    }

# --- Commands API Á´ØÁÇπ ---

@app.post("/api/commands/list")
async def list_commands(request: Request):
    """Ëé∑ÂèñÂèØÁî®ÁöÑÂëΩ‰ª§ÂàóË°®"""
    # TODO: ÂÆûÁé∞ÂëΩ‰ª§ÂàóË°®ËØªÂèñÈÄªËæë
    return {
        "commands": []
    }

# --- MCP Utils API Á´ØÁÇπ ---

@app.get("/api/mcp-utils/taskmaster-server")
async def get_taskmaster_server_status():
    """Ëé∑Âèñ TaskMaster MCP ÊúçÂä°Âô®Áä∂ÊÄÅ"""
    return {
        "status": "not-implemented",
        "message": "TaskMaster MCP server is not implemented"
    }

# --- RAG API Á´ØÁÇπ ---

@app.get("/api/rag/stats")
async def get_rag_stats(project_path: str = None, project_name: str = None):
    """Ëé∑Âèñ RAG ÁªüËÆ°‰ø°ÊÅØ"""
    try:
        # ‰ºòÂÖà‰ΩøÁî® project_pathÔºåÂ¶ÇÊûúÊ≤°ÊúâÂàô‰ΩøÁî® project_name
        if project_path:
            # Áõ¥Êé•‰ΩøÁî®Êèê‰æõÁöÑÈ°πÁõÆË∑ØÂæÑ
            final_project_path = project_path
        elif project_name:
            # ÈÄöËøáÈ°πÁõÆÂêçÁß∞Êü•ÊâæÈ°πÁõÆË∑ØÂæÑ
            final_project_path = get_project_path(project_name)
        else:
            return JSONResponse(
                content={"error": "Áº∫Â∞ë project_path Êàñ project_name ÂèÇÊï∞"},
                status_code=400
            )

        if final_project_path not in rag_cache:
            rag_cache[final_project_path] = get_rag_service(final_project_path)

        rag_service = rag_cache[final_project_path]
        stats = rag_service.get_stats()

        return {
            "success": True,
            "stats": stats
        }
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ RAG ÁªüËÆ°Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ëé∑Âèñ RAG ÁªüËÆ°Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.get("/api/rag/status")
async def get_rag_status():
    """Ëé∑Âèñ RAG ‰æùËµñÁä∂ÊÄÅ"""
    try:
        from backend.core.rag_service import CHROMADB_AVAILABLE, SKLEARN_AVAILABLE, SENTENCE_TRANSFORMERS_AVAILABLE
        
        return {
            "success": True,
            "dependencies": {
                "chromadb": CHROMADB_AVAILABLE,
                "sentence_transformers": SENTENCE_TRANSFORMERS_AVAILABLE,
                "sklearn": SKLEARN_AVAILABLE
            },
            "current_mode": global_config.get("rag_mode", "tfidf"),
            "available_retrievers": []
        }
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ RAG Áä∂ÊÄÅÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ëé∑Âèñ RAG Áä∂ÊÄÅÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/index")
async def index_project_rag(request: Request, project_path: str = None, project_name: str = None):
    """Á¥¢ÂºïÈ°πÁõÆÊñáÊ°£Âà∞ RAGÔºàÊîØÊåÅÂ¢ûÈáèÁ¥¢ÂºïÔºâ"""
    try:
        # ‰ºòÂÖà‰ΩøÁî® project_pathÔºåÂ¶ÇÊûúÊ≤°ÊúâÂàô‰ΩøÁî® project_name
        if project_path:
            # Áõ¥Êé•‰ΩøÁî®Êèê‰æõÁöÑÈ°πÁõÆË∑ØÂæÑ
            final_project_path = project_path
            logger.info(f"RAG indexing request for project_path: {project_path}")
        elif project_name:
            # ÈÄöËøáÈ°πÁõÆÂêçÁß∞Êü•ÊâæÈ°πÁõÆË∑ØÂæÑ
            final_project_path = get_project_path(project_name)
            logger.info(f"RAG indexing request for project_name: {project_name}, path: {final_project_path}")
        else:
            return JSONResponse(
                content={"error": "Áº∫Â∞ë project_path Êàñ project_name ÂèÇÊï∞"},
                status_code=400
            )

        logger.info(f"RAG indexing request for project: {final_project_path}")
        
        # Ëß£ÊûêËØ∑Ê±ÇÂèÇÊï∞
        try:
            data = await request.json() if request.method == "POST" else {}
            force_reindex = data.get("force_reindex", False)
        except Exception as e:
            logger.warning(f"Failed to parse request JSON: {e}")
            data = {}
            force_reindex = False
        
        # Ê£ÄÊü•‰æùËµñ
        from backend.core.rag_service import CHROMADB_AVAILABLE, SKLEARN_AVAILABLE
        
        if not CHROMADB_AVAILABLE and not SKLEARN_AVAILABLE:
            error_msg = "Áº∫Â∞ëÂøÖË¶ÅÁöÑ‰æùËµñÂ∫ì„ÄÇËØ∑ÂÆâË£Ö chromadb Êàñ scikit-learn:\n" \
                        "pip install chromadb sentence-transformers\n" \
                        "Êàñ\n" \
                        "pip install scikit-learn"
            logger.error(error_msg)
            return JSONResponse(
                content={"error": error_msg},
                status_code=500
            )
        
        logger.info(f"Dependencies check: CHROMADB_AVAILABLE={CHROMADB_AVAILABLE}, SKLEARN_AVAILABLE={SKLEARN_AVAILABLE}")
        
        cache_key = final_project_path
        # Á°Æ‰øù RAG ÊúçÂä°Ë¢´ÂàõÂª∫Ôºà‰ΩøÁî®È°πÁõÆË∑ØÂæÑ‰Ωú‰∏∫ÁºìÂ≠òÈîÆÔºâ
        if cache_key not in rag_cache:
            # Ê†πÊçÆÈÖçÁΩÆÈÄâÊã© RAG Ê®°Âºè
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            
            if use_chromadb and not CHROMADB_AVAILABLE:
                logger.warning("ChromaDB requested but not available, falling back to TF-IDF")
                use_chromadb = False
            
            rag_cache[cache_key] = get_rag_service(final_project_path, use_chromadb=use_chromadb)
            logger.info(f"Created new RAG service for {project_name} at {final_project_path} (mode: {'ChromaDB' if use_chromadb else 'TF-IDF'})")
        
        rag_service = rag_cache[cache_key]
        
        # ÂàõÂª∫ÂºÇÊ≠•ÁîüÊàêÂô®Áî®‰∫éËøõÂ∫¶Êõ¥Êñ∞
        async def progress_generator():
            try:
                logger.info(f"Starting progress generator for {project_name}")
                async for result in rag_service.index_project(force_reindex=force_reindex):
                    # ÂèëÈÄÅÊâÄÊúâÁ±ªÂûãÁöÑÁªìÊûú
                    msg = f"data: {json.dumps(result)}\n\n"
                    logger.debug(f"Yielding: {msg.strip()}")
                    yield msg
                    
                    # ÂÆåÊàêÂêéÈÄÄÂá∫
                    if result.get("type") == "complete":
                        logger.info(f"Indexing complete for {project_name}")
                        break
                    elif result.get("type") == "error":
                        logger.error(f"Indexing error for {project_name}: {result.get('message')}")
                        break
            except Exception as e:
                logger.exception(f"Progress generator error for {project_name}: {e}")
                error_msg = f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
                yield error_msg
        
        return StreamingResponse(progress_generator(), media_type="text/event-stream")
    
    except Exception as e:
        logger.exception(f"RAG Á¥¢ÂºïÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"RAG Á¥¢ÂºïÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/retrieve/{project_name}")
async def retrieve_rag(project_name: str, request: Request):
    """Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÊ°£ÔºàÊîØÊåÅÈ´òÁ∫ßÊ£ÄÁ¥¢ÈÄâÈ°πÔºâ"""
    try:
        data = await request.json()
        query = data.get("query", "")
        n_results = data.get("n_results", 5)
        
        # È´òÁ∫ßÊ£ÄÁ¥¢ÈÄâÈ°π
        similarity_threshold = data.get("similarity_threshold", 0.0)  # Áõ∏‰ººÂ∫¶ÈòàÂÄº
        file_types = data.get("file_types", [])  # Êñá‰ª∂Á±ªÂûãËøáÊª§
        languages = data.get("languages", [])  # ÁºñÁ®ãËØ≠Ë®ÄËøáÊª§
        min_chunk_size = data.get("min_chunk_size", 0)  # ÊúÄÂ∞èÂùóÂ§ßÂ∞è
        max_chunk_size = data.get("max_chunk_size", float('inf'))  # ÊúÄÂ§ßÂùóÂ§ßÂ∞è
        sort_by = data.get("sort_by", "similarity")  # ÊéíÂ∫èÊñπÂºè: similarity, date, size
        
        if not query:
            return JSONResponse(
                content={"error": "Êü•ËØ¢ÊñáÊú¨‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )
        
        project_path = get_project_path(project_name)
        
        if project_path not in rag_cache:
            rag_cache[project_path] = get_rag_service(project_path)
        
        rag_service = rag_cache[project_path]
        
        # ÊâßË°åÊ£ÄÁ¥¢
        results = rag_service.retrieve(query, n_results)
        
        # Â∫îÁî®ËøáÊª§ÂíåÊéíÂ∫è
        filtered_results = []
        for result in results:
            metadata = result.get('metadata', {})
            similarity = result.get('similarity', 0)
            
            # Áõ∏‰ººÂ∫¶ÈòàÂÄºËøáÊª§
            if similarity < similarity_threshold:
                continue
            
            # Êñá‰ª∂Á±ªÂûãËøáÊª§
            if file_types:
                file_ext = os.path.splitext(metadata.get('file_path', ''))[1].lower()
                if file_ext not in file_types:
                    continue
            
            # ÁºñÁ®ãËØ≠Ë®ÄËøáÊª§
            if languages:
                language = metadata.get('language', '')
                if language not in languages:
                    continue
            
            # ÂùóÂ§ßÂ∞èËøáÊª§
            content_size = len(result.get('content', ''))
            if content_size < min_chunk_size or content_size > max_chunk_size:
                continue
            
            filtered_results.append(result)
        
        # ÊéíÂ∫è
        if sort_by == "similarity":
            filtered_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        elif sort_by == "date":
            filtered_results.sort(key=lambda x: x.get('metadata', {}).get('timestamp', ''), reverse=True)
        elif sort_by == "size":
            filtered_results.sort(key=lambda x: len(x.get('content', '')), reverse=True)
        
        # ÈôêÂà∂ÁªìÊûúÊï∞Èáè
        final_results = filtered_results[:n_results]
        
        return {
            "success": True,
            "query": query,
            "results": final_results,
            "count": len(final_results),
            "total_filtered": len(filtered_results),
            "filters_applied": {
                "similarity_threshold": similarity_threshold,
                "file_types": file_types,
                "languages": languages,
                "min_chunk_size": min_chunk_size,
                "max_chunk_size": max_chunk_size,
                "sort_by": sort_by
            }
        }
    except Exception as e:
        logger.exception(f"RAG Ê£ÄÁ¥¢Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"RAG Ê£ÄÁ¥¢Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/reset/{project_name}")
async def reset_rag(project_name: str):
    """ÈáçÁΩÆ RAG Á¥¢Âºï"""
    try:
        project_path = get_project_path(project_name)
        
        if project_path in rag_cache:
            rag_service = rag_cache[project_path]
            rag_service.reset()
            del rag_cache[project_path]
        
        return {
            "success": True,
            "message": "RAG Á¥¢ÂºïÂ∑≤ÈáçÁΩÆ"
        }
    except Exception as e:
        logger.exception(f"RAG ÈáçÁΩÆÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"RAG ÈáçÁΩÆÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/clear-cache")
async def clear_rag_cache():
    """Ê∏ÖÈô§ RAG ÊúçÂä°ÁºìÂ≠ò"""
    try:
        count = len(rag_cache)
        rag_cache.clear()
        logger.info(f"Cleared RAG cache: {count} services removed")
        
        return {
            "success": True,
            "message": f"Â∑≤Ê∏ÖÈô§ {count} ‰∏™ RAG ÊúçÂä°ÁºìÂ≠ò"
        }
    except Exception as e:
        logger.exception(f"Ê∏ÖÈô§ RAG ÁºìÂ≠òÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ê∏ÖÈô§ RAG ÁºìÂ≠òÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/ask/{project_name}")
async def ask_rag_question(project_name: str, request: Request):
    """Âêë RAG Áü•ËØÜÂ∫ìÊèêÈóÆ"""
    try:
        data = await request.json()
        question = data.get("question", "")
        
        if not question:
            return JSONResponse(
                content={"error": "ÈóÆÈ¢ò‰∏çËÉΩ‰∏∫Á©∫"},
                status_code=400
            )
        
        project_path = get_project_path(project_name)
        
        # Ëé∑Âèñ RAG ÊúçÂä°
        if project_path not in rag_cache:
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            if use_chromadb and not CHROMADB_AVAILABLE:
                use_chromadb = False
            rag_cache[project_path] = get_rag_service(project_path, use_chromadb=use_chromadb)
        
        rag_service = rag_cache[project_path]
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÊñáÊ°£
        stats = rag_service.get_stats()
        if stats.get("document_count", 0) == 0:
            return JSONResponse(
                content={"answer": "Áü•ËØÜÂ∫ì‰∏≠ËøòÊ≤°ÊúâÊñáÊ°£„ÄÇËØ∑ÂÖàÊ∑ªÂä†ÊñáÊ°£ÊàñÁ¥¢ÂºïÈ°πÁõÆ„ÄÇ", "sources": []},
                status_code=200
            )
        
        # Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÊ°£
        results = rag_service.retrieve(question, n_results=5)
        
        if not results or len(results) == 0:
            return JSONResponse(
                content={"answer": "Áü•ËØÜÂ∫ì‰∏≠Ê≤°ÊúâÊâæÂà∞Áõ∏ÂÖ≥ÊñáÊ°£„ÄÇ", "sources": []},
                status_code=200
            )
        
        # ÊûÑÂª∫‰∏ä‰∏ãÊñá
        context_parts = []
        sources = []
        max_similarity = 0
        
        for i, result in enumerate(results):
            # result ÊòØÂ≠óÂÖ∏Ôºå‰∏çÊòØÂØπË±°
            metadata = result.get('metadata', {})
            similarity = result.get('similarity', 0)
            
            # ËÆ∞ÂΩïÊúÄÈ´òÁõ∏‰ººÂ∫¶
            if similarity > max_similarity:
                max_similarity = similarity
            
            # ÊèêÂèñÊõ¥ËØ¶ÁªÜÁöÑÊù•Ê∫ê‰ø°ÊÅØ
            file_path = metadata.get('file_path', 'Êú™Áü•Êñá‰ª∂')
            chunk_index = metadata.get('chunk_index', 0)
            total_chunks = metadata.get('total_chunks', 1)
            start_line = metadata.get('start_line', 1)
            end_line = metadata.get('end_line', 1)
            language = metadata.get('language', '')
            summary = metadata.get('summary', '')
            
            # ÊûÑÂª∫Êù•Ê∫êÊèèËø∞
            source_desc = f"{file_path}"
            if language:
                source_desc += f" ({language})"
            if start_line and end_line:
                source_desc += f" [Ë°å {start_line}-{end_line}]"
            
            context_parts.append(f"[ÊñáÊ°£ {i+1}] {source_desc}:\n{result['content']}")
            
            sources.append({
                "file_path": file_path,
                "content": result['content'][:200] + '...' if len(result['content']) > 200 else result['content'],
                "similarity": similarity,
                "chunk_index": chunk_index,
                "total_chunks": total_chunks,
                "start_line": start_line,
                "end_line": end_line,
                "language": language,
                "summary": summary,
                "source_desc": source_desc
            })
        
        logger.info(f"RAG ÈóÆÁ≠î: ‰∏∫ÈóÆÈ¢ò '{question}' ÊâæÂà∞ {len(sources)} ‰∏™Êù•Ê∫ê")
        logger.info("=" * 80)
        logger.info("ËøîÂõûÁªôÂâçÁ´ØÁöÑ sources Êï∞ÁªÑ:")
        logger.info("=" * 80)
        for i, source in enumerate(sources):
            logger.info(f"\nÊù•Ê∫ê #{i+1}:")
            logger.info(f"  file_path: {source['file_path']}")
            logger.info(f"  chunk_index: {source['chunk_index']}")
            logger.info(f"  total_chunks: {source['total_chunks']}")
            logger.info(f"  start_line: {source['start_line']}")
            logger.info(f"  end_line: {source['end_line']}")
            logger.info(f"  similarity: {source['similarity']}")
            logger.info(f"  content_length: {len(source['content'])}")
            logger.info(f"  content_preview: {source['content'][:150]}")
        logger.info("=" * 80)
        
        context = '\n\n'.join(context_parts)
        
        # ËÆ°ÁÆóÁΩÆ‰ø°Â∫¶ËØÑÂàÜÔºàÂü∫‰∫éÊ£ÄÁ¥¢ÁªìÊûúÁöÑÁõ∏‰ººÂ∫¶Ôºâ
        confidence_score = 0
        if sources:
            # ‰ΩøÁî®Âπ≥ÂùáÁõ∏‰ººÂ∫¶‰Ωú‰∏∫ÁΩÆ‰ø°Â∫¶
            avg_similarity = sum(s['similarity'] for s in sources) / len(sources)
            confidence_score = avg_similarity * 100
        
        # ‰ΩøÁî® AI ÁîüÊàêÂõûÁ≠î
        try:
            agent = get_agent(project_path, global_config.get("mode", "yolo"), global_config.get("model"))
            
            # ÊûÑÂª∫ÂåÖÂê´‰∏ä‰∏ãÊñáÁöÑÊèêÁ§∫
            rag_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™Êô∫ËÉΩÂä©ÊâãÔºåËØ∑Âü∫‰∫é‰ª•‰∏ãÁü•ËØÜÂ∫ìÂÜÖÂÆπÂõûÁ≠îÁî®Êà∑ÁöÑÈóÆÈ¢ò„ÄÇ

Áü•ËØÜÂ∫ìÂÜÖÂÆπÔºö
{context}

Áî®Êà∑ÈóÆÈ¢òÔºö{question}

ËØ∑Âü∫‰∫é‰ª•‰∏äÁü•ËØÜÂ∫ìÂÜÖÂÆπÂõûÁ≠îÈóÆÈ¢ò„ÄÇÂ¶ÇÊûúÁü•ËØÜÂ∫ì‰∏≠Ê≤°ÊúâÁõ∏ÂÖ≥‰ø°ÊÅØÔºåËØ∑ÊòéÁ°ÆËØ¥Êòé„ÄÇÂõûÁ≠îË¶ÅÂáÜÁ°Æ„ÄÅÁÆÄÊ¥Å„ÄÅÊúâÂ∏ÆÂä©„ÄÇ"""
            
            # Êî∂ÈõÜ AI ÂõûÁ≠î
            answer_parts = []
            async for msg in agent.chat_stream(rag_prompt):
                if isinstance(msg, str):
                    answer_parts.append(msg)
                elif isinstance(msg, dict) and msg.get("type") == "assistant":
                    answer_parts.append(msg.get("content", ""))
            
            answer = "".join(answer_parts)
            
            # Â¶ÇÊûúÊ≤°ÊúâÁîüÊàêÂõûÁ≠îÔºå‰ΩøÁî®ÈªòËÆ§ÂõûÁ≠î
            if not answer:
                answer = f"Âü∫‰∫éÁü•ËØÜÂ∫ìÊâæÂà∞ {len(results)} ‰∏™Áõ∏ÂÖ≥ÊñáÊ°£„ÄÇ\n\nÁõ∏ÂÖ≥ÊñáÊ°£Ôºö\n"
                for i, source in enumerate(sources):
                    answer += f"{i+1}. {source['file_path']}\n"
        
        except Exception as ai_error:
            logger.warning(f"AI ÁîüÊàêÂõûÁ≠îÂ§±Ë¥•Ôºå‰ΩøÁî®ÈªòËÆ§ÂõûÁ≠î: {ai_error}")
            answer = f"Âü∫‰∫éÁü•ËØÜÂ∫ìÊâæÂà∞ {len(results)} ‰∏™Áõ∏ÂÖ≥ÊñáÊ°£„ÄÇ\n\nÁõ∏ÂÖ≥ÊñáÊ°£Ôºö\n"
            for i, source in enumerate(sources):
                answer += f"{i+1}. {source['file_path']}\n"
        
        return JSONResponse(
            content={
                "answer": answer,
                "question": question,
                "sources": sources,
                "confidence": {
                    "score": round(confidence_score, 2),
                    "level": "high" if confidence_score > 70 else "medium" if confidence_score > 40 else "low"
                },
                "related_documents": [
                    {
                        "file_path": s['file_path'],
                        "similarity": s['similarity'],
                        "summary": s.get('summary', '')
                    }
                    for s in sources[:3]  # Êé®Ëçê top 3 Áõ∏ÂÖ≥ÊñáÊ°£
                ]
            },
            status_code=200
        )
        
    except Exception as e:
        logger.exception(f"RAG ÈóÆÁ≠îÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"RAG ÈóÆÁ≠îÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/upload/{project_name}")
async def upload_document_to_rag(project_name: str, request: Request):
    """‰∏ä‰º†ÊñáÊ°£Âà∞ RAG Áü•ËØÜÂ∫ì"""
    try:
        project_path = get_project_path(project_name)
        
        # Ëß£ÊûêË°®ÂçïÊï∞ÊçÆ
        form = await request.form()
        file = form.get("file")
        
        if not file:
            return JSONResponse(
                content={"error": "Êú™ÊâæÂà∞Êñá‰ª∂"},
                status_code=400
            )
        
        # ËØªÂèñÊñá‰ª∂ÂÜÖÂÆπ
        content = await file.read()
        text_content = content.decode('utf-8', errors='ignore')
        
        # Ëé∑Âèñ RAG ÊúçÂä°
        if project_path not in rag_cache:
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            if use_chromadb and not CHROMADB_AVAILABLE:
                use_chromadb = False
            rag_cache[project_path] = get_rag_service(project_path, use_chromadb=use_chromadb)
        
        rag_service = rag_cache[project_path]
        
        # Ê∑ªÂä†ÊñáÊ°£
        result = await rag_service.add_document(
            file_name=file.filename,
            content=text_content,
            file_type=os.path.splitext(file.filename)[1].lower()
        )
        
        return result
        
    except Exception as e:
        logger.exception(f"‰∏ä‰º†ÊñáÊ°£Âà∞ RAG Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"‰∏ä‰º†ÊñáÊ°£Âà∞ RAG Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/upload-batch/{project_name}")
async def upload_documents_batch_to_rag(project_name: str, request: Request):
    """ÊâπÈáè‰∏ä‰º†ÊñáÊ°£Âà∞ RAG Áü•ËØÜÂ∫ì"""
    try:
        project_path = get_project_path(project_name)
        
        # Ëß£ÊûêË°®ÂçïÊï∞ÊçÆ
        form = await request.form()
        files = form.getlist("files")
        
        if not files:
            return JSONResponse(
                content={"error": "Êú™ÊâæÂà∞Êñá‰ª∂"},
                status_code=400
            )
        
        # ‰øùÂ≠òÊñá‰ª∂Âà∞‰∏¥Êó∂ÁõÆÂΩï
        temp_dir = os.path.join(project_path, ".rag_temp")
        os.makedirs(temp_dir, exist_ok=True)
        
        file_paths = []
        for file in files:
            file_path = os.path.join(temp_dir, file.filename)
            with open(file_path, 'wb') as f:
                f.write(await file.read())
            file_paths.append(file_path)
        
        # Ëé∑Âèñ RAG ÊúçÂä°
        if project_path not in rag_cache:
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            if use_chromadb and not CHROMADB_AVAILABLE:
                use_chromadb = False
            rag_cache[project_path] = get_rag_service(project_path, use_chromadb=use_chromadb)
        
        rag_service = rag_cache[project_path]
        
        # ÂàõÂª∫ÊµÅÂºèÂìçÂ∫î
        async def progress_generator():
            try:
                async for result in rag_service.add_documents_from_files(file_paths):
                    yield f"data: {json.dumps(result)}\n\n"
                    
                    if result.get("type") == "complete":
                        # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
                        for fp in file_paths:
                            try:
                                os.remove(fp)
                            except:
                                pass
                        break
            except Exception as e:
                logger.exception(f"ÊâπÈáè‰∏ä‰º†ÊñáÊ°£Â§±Ë¥•: {e}")
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
        
        return StreamingResponse(progress_generator(), media_type="text/event-stream")
        
    except Exception as e:
        logger.exception(f"ÊâπÈáè‰∏ä‰º†ÊñáÊ°£Âà∞ RAG Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÊâπÈáè‰∏ä‰º†ÊñáÊ°£Âà∞ RAG Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/rag/add-files/{project_name}")
async def add_files_to_rag(project_name: str, request: Request):
    """Ê∑ªÂä†Á≥ªÁªüÊñá‰ª∂Ë∑ØÂæÑÂà∞ RAG Áü•ËØÜÂ∫ìÔºàÁõ¥Êé•ËØªÂèñÔºå‰∏ç‰∏ä‰º†Ôºâ"""
    try:
        data = await request.json()
        file_paths = data.get("file_paths", [])
        
        logger.info(f"Êî∂Âà∞Ê∑ªÂä†Êñá‰ª∂ËØ∑Ê±ÇÔºåÈ°πÁõÆ: {project_name}, Êñá‰ª∂Êï∞: {len(file_paths)}")
        logger.info(f"Êñá‰ª∂Ë∑ØÂæÑÂàóË°®: {file_paths}")
        
        if not file_paths:
            return JSONResponse(
                content={"error": "Êú™Êèê‰æõÊñá‰ª∂Ë∑ØÂæÑ"},
                status_code=400
            )
        
        project_path = get_project_path(project_name)
        
        # È™åËØÅË∑ØÂæÑÂÆâÂÖ®ÊÄßÔºàRAG ÂÖÅËÆ∏Êõ¥ÂÆΩÊùæÁöÑË∑ØÂæÑÈôêÂà∂Ôºâ
        valid_paths = []
        for file_path in file_paths:
            # ËßÑËåÉÂåñË∑ØÂæÑ
            file_path = os.path.abspath(file_path)
            logger.info(f"Â§ÑÁêÜÊñá‰ª∂: {file_path}")
            
            # Ê£ÄÊü•Ë∑ØÂæÑÊòØÂê¶Â≠òÂú®
            if not os.path.exists(file_path):
                logger.warning(f"Ë∑≥Ëøá‰∏çÂ≠òÂú®ÁöÑÊñá‰ª∂Ë∑ØÂæÑ: {file_path}")
                continue
            
            # Ê£ÄÊü•ÊòØÂê¶ÊòØÊñá‰ª∂
            if not os.path.isfile(file_path):
                logger.warning(f"Ë∑≥ËøáÈùûÊñá‰ª∂Ë∑ØÂæÑ: {file_path}")
                continue
            
            # Ê£ÄÊü•Êñá‰ª∂Â§ßÂ∞èÔºàÈôêÂà∂ 500MBÔºâ
            try:
                file_size = os.path.getsize(file_path)
                if file_size > 500 * 1024 * 1024:  # 500MB
                    logger.warning(f"Ë∑≥ËøáËøáÂ§ßÁöÑÊñá‰ª∂: {file_path} ({file_size} bytes)")
                    continue
            except:
                logger.warning(f"Êó†Ê≥ïËé∑ÂèñÊñá‰ª∂Â§ßÂ∞è: {file_path}")
                continue
            
            # Ê£ÄÊü•Êñá‰ª∂Á±ªÂûã
            allowed_extensions = {
                '.txt', '.md', '.rst', '.py', '.js', '.ts', '.jsx', '.tsx',
                '.java', '.go', '.rs', '.json', '.yaml', '.yml', '.html', '.css',
                '.xml', '.csv', '.log', '.sql', '.sh', '.bat', '.ps1',
                '.docx', '.xlsx', '.pptx', '.pdf'
            }
            ext = os.path.splitext(file_path)[1].lower()
            if ext not in allowed_extensions:
                logger.warning(f"Ë∑≥Ëøá‰∏çÊîØÊåÅÁöÑÊñá‰ª∂Á±ªÂûã: {file_path} ({ext})")
                continue
            
            valid_paths.append(file_path)
            logger.info(f"Êñá‰ª∂ÊúâÊïà: {file_path}")
        
        logger.info(f"ÊúâÊïàÊñá‰ª∂Êï∞: {len(valid_paths)}")
        
        if not valid_paths:
            return JSONResponse(
                content={"error": "Ê≤°ÊúâÊúâÊïàÁöÑÊñá‰ª∂Ë∑ØÂæÑÔºàÊñá‰ª∂‰∏çÂ≠òÂú®„ÄÅËøáÂ§ßÊàñ‰∏çÊîØÊåÅÁöÑÁ±ªÂûãÔºâ„ÄÇÊîØÊåÅÁöÑÊúÄÂ§ßÊñá‰ª∂Â§ßÂ∞è: 500MB"},
                status_code=400
            )
        
        # Ëé∑Âèñ RAG ÊúçÂä°
        if project_path not in rag_cache:
            rag_mode = global_config.get("rag_mode", "tfidf")
            use_chromadb = (rag_mode == "chromadb")
            if use_chromadb and not CHROMADB_AVAILABLE:
                use_chromadb = False
            rag_cache[project_path] = get_rag_service(project_path, use_chromadb=use_chromadb)
        
        rag_service = rag_cache[project_path]
        
        # ÂàõÂª∫ÊµÅÂºèÂìçÂ∫î
        async def progress_generator():
            try:
                async for result in rag_service.add_documents_from_files(valid_paths):
                    yield f"data: {json.dumps(result)}\n\n"
            except Exception as e:
                logger.exception(f"Ê∑ªÂä†Êñá‰ª∂Âà∞ RAG Â§±Ë¥•: {e}")
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
        
        return StreamingResponse(progress_generator(), media_type="text/event-stream")
        
    except Exception as e:
        logger.exception(f"Ê∑ªÂä†Êñá‰ª∂Âà∞ RAG Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ê∑ªÂä†Êñá‰ª∂Âà∞ RAG Â§±Ë¥•: {str(e)}"},
            status_code=500
        )

# ==================== ÊñáÊ°£ÁâàÊú¨ÁÆ°ÁêÜ API ====================

@app.get("/api/document-versions/{project_name}/{file_path:path}")
async def get_document_versions(project_name: str, file_path: str):
    """Ëé∑ÂèñÊñáÊ°£ÁöÑÊâÄÊúâÁâàÊú¨"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # ÊûÑÂª∫ÂÆåÊï¥ÁöÑÊñá‰ª∂Ë∑ØÂæÑ
        full_file_path = os.path.join(project_path, file_path)
        
        if not os.path.exists(full_file_path):
            return JSONResponse(
                content={"error": "Êñá‰ª∂‰∏çÂ≠òÂú®"},
                status_code=404
            )
        
        versions = version_manager.get_versions(full_file_path)
        
        return {
            "success": True,
            "file_path": file_path,
            "versions": versions,
            "total": len(versions)
        }
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊñáÊ°£ÁâàÊú¨Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ëé∑ÂèñÊñáÊ°£ÁâàÊú¨Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.get("/api/document-versions/{project_name}/{file_path:path}/{version_id}")
async def get_document_version(project_name: str, file_path: str, version_id: str):
    """Ëé∑ÂèñÁâπÂÆöÁâàÊú¨ÁöÑÊñáÊ°£ÂÜÖÂÆπ"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # ÊûÑÂª∫ÂÆåÊï¥ÁöÑÊñá‰ª∂Ë∑ØÂæÑ
        full_file_path = os.path.join(project_path, file_path)
        
        version = version_manager.get_version(full_file_path, version_id)
        
        if not version:
            return JSONResponse(
                content={"error": "ÁâàÊú¨‰∏çÂ≠òÂú®"},
                status_code=404
            )
        
        return {
            "success": True,
            "version": version
        }
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊñáÊ°£ÁâàÊú¨ÂÜÖÂÆπÂ§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ëé∑ÂèñÊñáÊ°£ÁâàÊú¨ÂÜÖÂÆπÂ§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.post("/api/document-versions/{project_name}/{file_path:path}/record")
async def record_document_version(project_name: str, file_path: str, request: Request):
    """ËÆ∞ÂΩïÊñáÊ°£ÁâàÊú¨"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # ÊûÑÂª∫ÂÆåÊï¥ÁöÑÊñá‰ª∂Ë∑ØÂæÑ
        full_file_path = os.path.join(project_path, file_path)
        
        if not os.path.exists(full_file_path):
            return JSONResponse(
                content={"error": "Êñá‰ª∂‰∏çÂ≠òÂú®"},
                status_code=404
            )
        
        # Ëé∑ÂèñÂÖÉÊï∞ÊçÆ
        try:
            data = await request.json()
            metadata = data.get("metadata", {})
        except:
            metadata = {}
        
        # ËÆ∞ÂΩïÁâàÊú¨
        version = version_manager.record_version(full_file_path, metadata=metadata)
        
        if not version:
            return JSONResponse(
                content={"error": "ËÆ∞ÂΩïÁâàÊú¨Â§±Ë¥•"},
                status_code=500
            )
        
        return {
            "success": True,
            "version": version
        }
    except Exception as e:
        logger.exception(f"ËÆ∞ÂΩïÊñáÊ°£ÁâàÊú¨Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ËÆ∞ÂΩïÊñáÊ°£ÁâàÊú¨Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.get("/api/document-versions/{project_name}/{file_path:path}/compare/{version_id1}/{version_id2}")
async def compare_document_versions(project_name: str, file_path: str, version_id1: str, version_id2: str):
    """ÊØîËæÉ‰∏§‰∏™ÊñáÊ°£ÁâàÊú¨"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # ÊûÑÂª∫ÂÆåÊï¥ÁöÑÊñá‰ª∂Ë∑ØÂæÑ
        full_file_path = os.path.join(project_path, file_path)
        
        comparison = version_manager.compare_versions(full_file_path, version_id1, version_id2)
        
        if not comparison:
            return JSONResponse(
                content={"error": "ÊØîËæÉÁâàÊú¨Â§±Ë¥•"},
                status_code=500
            )
        
        return {
            "success": True,
            "comparison": comparison
        }
    except Exception as e:
        logger.exception(f"ÊØîËæÉÊñáÊ°£ÁâàÊú¨Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"ÊØîËæÉÊñáÊ°£ÁâàÊú¨Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.delete("/api/document-versions/{project_name}/{file_path:path}/{version_id}")
async def delete_document_version(project_name: str, file_path: str, version_id: str):
    """Âà†Èô§ÁâπÂÆöÁâàÊú¨"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        # ÊûÑÂª∫ÂÆåÊï¥ÁöÑÊñá‰ª∂Ë∑ØÂæÑ
        full_file_path = os.path.join(project_path, file_path)
        
        success = version_manager.delete_version(full_file_path, version_id)
        
        return {
            "success": success,
            "message": "ÁâàÊú¨Â∑≤Âà†Èô§" if success else "Âà†Èô§ÁâàÊú¨Â§±Ë¥•"
        }
    except Exception as e:
        logger.exception(f"Âà†Èô§ÊñáÊ°£ÁâàÊú¨Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Âà†Èô§ÊñáÊ°£ÁâàÊú¨Â§±Ë¥•: {str(e)}"},
            status_code=500
        )


@app.get("/api/document-versions/{project_name}/statistics")
async def get_version_statistics(project_name: str):
    """Ëé∑ÂèñÁâàÊú¨ÁªüËÆ°‰ø°ÊÅØ"""
    try:
        project_path = get_project_path(project_name)
        version_manager = get_version_manager(project_path)
        
        stats = version_manager.get_statistics()
        
        return {
            "success": True,
            "statistics": stats
        }
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÁâàÊú¨ÁªüËÆ°Â§±Ë¥•: {e}")
        return JSONResponse(
            content={"error": f"Ëé∑ÂèñÁâàÊú¨ÁªüËÆ°Â§±Ë¥•: {str(e)}"},
            status_code=500
        )

# ============================================================================
# ÂºÄÂèëËÄÖÂ∑•ÂÖ∑ API
# ============================================================================

import sqlite3
import json
from datetime import datetime
from typing import List, Optional, Dict, Any
from pydantic import BaseModel

# Êï∞ÊçÆÂ∫ìË∑ØÂæÑ
DB_PATH = os.path.join(os.path.dirname(__file__), "developer_tools.db")

def get_db_connection():
    """Ëé∑ÂèñÊï∞ÊçÆÂ∫ìËøûÊé•"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    """ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìË°®"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # ‰ª£Á†ÅÁâáÊÆµË°®
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS snippets (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            code TEXT NOT NULL,
            language TEXT DEFAULT 'javascript',
            category TEXT DEFAULT 'ÈÄöÁî®',
            description TEXT,
            tags TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            is_favorite INTEGER DEFAULT 0,
            usage_count INTEGER DEFAULT 0
        )
    ''')
    
    # ÂëΩ‰ª§Âø´Êç∑ÊñπÂºèË°®
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS command_shortcuts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            command TEXT NOT NULL,
            category TEXT DEFAULT 'ÈÄöÁî®',
            description TEXT,
            tags TEXT,
            working_dir TEXT,
            timeout INTEGER DEFAULT 60,
            parameters TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            is_favorite INTEGER DEFAULT 0,
            usage_count INTEGER DEFAULT 0
        )
    ''')
    
    # ÊèêÁ§∫ËØçË°®
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS prompts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            content TEXT NOT NULL,
            category TEXT DEFAULT 'Ëá™ÂÆö‰πâ',
            description TEXT,
            tags TEXT,
            parameters TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            is_favorite INTEGER DEFAULT 0,
            usage_count INTEGER DEFAULT 0
        )
    ''')
    
    # ÊñπÊ°àË°®
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS solutions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            requirement TEXT NOT NULL,
            solution TEXT NOT NULL,
            template_type TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # ÊâßË°åÂéÜÂè≤Ë°®
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS execution_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            shortcut_id INTEGER,
            command TEXT,
            working_dir TEXT,
            status TEXT,
            output TEXT,
            error TEXT,
            exit_code INTEGER,
            duration REAL,
            executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (shortcut_id) REFERENCES command_shortcuts(id)
        )
    ''')
    
    conn.commit()
    conn.close()

# ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ì
try:
    init_db()
    task_master_service.init_tables()
    logger.info("ÂºÄÂèëËÄÖÂ∑•ÂÖ∑Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñÊàêÂäü")
except Exception as e:
    logger.error(f"Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñÂ§±Ë¥•: {e}")

# ============================================================================
# ‰ª£Á†ÅÁâáÊÆµÁÆ°ÁêÜÂô® API
# ============================================================================

class SnippetCreate(BaseModel):
    title: str
    code: str
    language: str = "javascript"
    category: str = "ÈÄöÁî®"
    description: str = ""
    tags: List[str] = []

class SnippetUpdate(BaseModel):
    title: Optional[str] = None
    code: Optional[str] = None
    language: Optional[str] = None
    category: Optional[str] = None
    description: Optional[str] = None
    tags: Optional[List[str]] = None

@app.get("/api/snippets")
async def get_snippets(
    search: Optional[str] = None,
    category: Optional[str] = None,
    language: Optional[str] = None,
    favorite_only: bool = False
):
    """Ëé∑Âèñ‰ª£Á†ÅÁâáÊÆµÂàóË°®"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = "SELECT * FROM snippets WHERE 1=1"
        params = []
        
        if search:
            query += " AND (title LIKE ? OR description LIKE ? OR code LIKE ?)"
            params.extend([f"%{search}%", f"%{search}%", f"%{search}%"])
        
        if category:
            query += " AND category = ?"
            params.append(category)
        
        if language:
            query += " AND language = ?"
            params.append(language)
        
        if favorite_only:
            query += " AND is_favorite = 1"
        
        query += " ORDER BY updated_at DESC"
        
        cursor.execute(query, params)
        rows = cursor.fetchall()
        
        snippets = []
        for row in rows:
            snippet = dict(row)
            snippet['tags'] = json.loads(snippet['tags']) if snippet['tags'] else []
            snippets.append(snippet)
        
        # Ëé∑ÂèñÂàÜÁ±ªÂíåÊ†áÁ≠æ
        categories = [row[0] for row in cursor.execute("SELECT DISTINCT category FROM snippets ORDER BY category")]
        tags = set()
        for snippet in snippets:
            tags.update(snippet['tags'])
        
        conn.close()
        
        return JSONResponse({
            "snippets": snippets,
            "categories": categories,
            "tags": list(tags)
        })
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰ª£Á†ÅÁâáÊÆµÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/snippets")
async def create_snippet(snippet: SnippetCreate):
    """ÂàõÂª∫‰ª£Á†ÅÁâáÊÆµ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO snippets (title, code, language, category, description, tags)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            snippet.title,
            snippet.code,
            snippet.language,
            snippet.category,
            snippet.description,
            json.dumps(snippet.tags)
        ))
        
        snippet_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return JSONResponse({"id": snippet_id, "message": "‰ª£Á†ÅÁâáÊÆµÂàõÂª∫ÊàêÂäü"})
    except Exception as e:
        logger.exception(f"ÂàõÂª∫‰ª£Á†ÅÁâáÊÆµÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/categories")
async def get_snippet_categories():
    """Ëé∑Âèñ‰ª£Á†ÅÁâáÊÆµÂàÜÁ±ª"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT DISTINCT category FROM snippets ORDER BY category")
        categories = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return JSONResponse({"categories": categories})
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰ª£Á†ÅÁâáÊÆµÂàÜÁ±ªÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/tags")
async def get_snippet_tags():
    """Ëé∑Âèñ‰ª£Á†ÅÁâáÊÆµÊ†áÁ≠æ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT tags FROM snippets")
        all_tags = set()
        for row in cursor.fetchall():
            if row[0]:
                tags = json.loads(row[0])
                all_tags.update(tags)
        
        conn.close()
        
        return JSONResponse({"tags": list(all_tags)})
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰ª£Á†ÅÁâáÊÆµÊ†áÁ≠æÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/{snippet_id}")
async def get_snippet(snippet_id: int):
    """Ëé∑ÂèñÂçï‰∏™‰ª£Á†ÅÁâáÊÆµ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM snippets WHERE id = ?", (snippet_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return JSONResponse({"error": "‰ª£Á†ÅÁâáÊÆµ‰∏çÂ≠òÂú®"}, status_code=404)
        
        snippet = dict(row)
        snippet['tags'] = json.loads(snippet['tags']) if snippet['tags'] else []
        
        # Â¢ûÂä†‰ΩøÁî®Ê¨°Êï∞
        cursor.execute("UPDATE snippets SET usage_count = usage_count + 1 WHERE id = ?", (snippet_id,))
        conn.commit()
        
        conn.close()
        
        return JSONResponse(snippet)
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰ª£Á†ÅÁâáÊÆµÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.put("/api/snippets/{snippet_id}")
async def update_snippet(snippet_id: int, snippet: SnippetUpdate):
    """Êõ¥Êñ∞‰ª£Á†ÅÁâáÊÆµ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Ê£ÄÊü•ÊòØÂê¶Â≠òÂú®
        cursor.execute("SELECT id FROM snippets WHERE id = ?", (snippet_id,))
        if not cursor.fetchone():
            conn.close()
            return JSONResponse({"error": "‰ª£Á†ÅÁâáÊÆµ‰∏çÂ≠òÂú®"}, status_code=404)
        
        # ÊûÑÂª∫Êõ¥Êñ∞ËØ≠Âè•
        updates = []
        params = []
        
        if snippet.title is not None:
            updates.append("title = ?")
            params.append(snippet.title)
        if snippet.code is not None:
            updates.append("code = ?")
            params.append(snippet.code)
        if snippet.language is not None:
            updates.append("language = ?")
            params.append(snippet.language)
        if snippet.category is not None:
            updates.append("category = ?")
            params.append(snippet.category)
        if snippet.description is not None:
            updates.append("description = ?")
            params.append(snippet.description)
        if snippet.tags is not None:
            updates.append("tags = ?")
            params.append(json.dumps(snippet.tags))
        
        updates.append("updated_at = CURRENT_TIMESTAMP")
        params.append(snippet_id)
        
        query = f"UPDATE snippets SET {', '.join(updates)} WHERE id = ?"
        cursor.execute(query, params)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "‰ª£Á†ÅÁâáÊÆµÊõ¥Êñ∞ÊàêÂäü"})
    except Exception as e:
        logger.exception(f"Êõ¥Êñ∞‰ª£Á†ÅÁâáÊÆµÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/snippets/{snippet_id}")
async def delete_snippet(snippet_id: int):
    """Âà†Èô§‰ª£Á†ÅÁâáÊÆµ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM snippets WHERE id = ?", (snippet_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "‰ª£Á†ÅÁâáÊÆµ‰∏çÂ≠òÂú®"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "‰ª£Á†ÅÁâáÊÆµÂà†Èô§ÊàêÂäü"})
    except Exception as e:
        logger.exception(f"Âà†Èô§‰ª£Á†ÅÁâáÊÆµÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/popular")
async def get_popular_snippets(limit: int = 10):
    """Ëé∑ÂèñÁÉ≠Èó®‰ª£Á†ÅÁâáÊÆµ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM snippets ORDER BY usage_count DESC, updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        snippets = []
        for row in rows:
            snippet = dict(row)
            snippet['tags'] = json.loads(snippet['tags']) if snippet['tags'] else []
            snippets.append(snippet)
        
        conn.close()
        
        return JSONResponse({"snippets": snippets})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÁÉ≠Èó®‰ª£Á†ÅÁâáÊÆµÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/snippets/recent")
async def get_recent_snippets(limit: int = 10):
    """Ëé∑ÂèñÊúÄËøë‰ª£Á†ÅÁâáÊÆµ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM snippets ORDER BY updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        snippets = []
        for row in rows:
            snippet = dict(row)
            snippet['tags'] = json.loads(snippet['tags']) if snippet['tags'] else []
            snippets.append(snippet)
        
        conn.close()
        
        return JSONResponse({"snippets": snippets})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊúÄËøë‰ª£Á†ÅÁâáÊÆµÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/snippets/{snippet_id}/usage")
async def increment_snippet_usage(snippet_id: int):
    """Â¢ûÂä†‰ª£Á†ÅÁâáÊÆµ‰ΩøÁî®Ê¨°Êï∞"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("UPDATE snippets SET usage_count = usage_count + 1 WHERE id = ?", (snippet_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "‰ª£Á†ÅÁâáÊÆµ‰∏çÂ≠òÂú®"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "‰ΩøÁî®Ê¨°Êï∞Â∑≤Êõ¥Êñ∞"})
    except Exception as e:
        logger.exception(f"Êõ¥Êñ∞‰ΩøÁî®Ê¨°Êï∞Â§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# ÂëΩ‰ª§Âø´Êç∑ÊñπÂºè API
# ============================================================================

class CommandShortcutCreate(BaseModel):
    name: str
    command: str
    category: str = "ÈÄöÁî®"
    description: str = ""
    tags: List[str] = []
    working_dir: str = ""
    timeout: int = 60
    parameters: List[Dict[str, Any]] = []

class CommandShortcutUpdate(BaseModel):
    name: Optional[str] = None
    command: Optional[str] = None
    category: Optional[str] = None
    description: Optional[str] = None
    tags: Optional[List[str]] = None
    working_dir: Optional[str] = None
    timeout: Optional[int] = None
    parameters: Optional[List[Dict[str, Any]]] = None

@app.get("/api/command-shortcuts")
async def get_command_shortcuts(
    search: Optional[str] = None,
    category: Optional[str] = None,
    favorite_only: bool = False
):
    """Ëé∑ÂèñÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÂàóË°®"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = "SELECT * FROM command_shortcuts WHERE 1=1"
        params = []
        
        if search:
            query += " AND (name LIKE ? OR description LIKE ? OR command LIKE ?)"
            params.extend([f"%{search}%", f"%{search}%", f"%{search}%"])
        
        if category:
            query += " AND category = ?"
            params.append(category)
        
        if favorite_only:
            query += " AND is_favorite = 1"
        
        query += " ORDER BY usage_count DESC, updated_at DESC"
        
        cursor.execute(query, params)
        rows = cursor.fetchall()
        
        shortcuts = []
        for row in rows:
            shortcut = dict(row)
            shortcut['tags'] = json.loads(shortcut['tags']) if shortcut['tags'] else []
            shortcut['parameters'] = json.loads(shortcut['parameters']) if shortcut['parameters'] else []
            shortcuts.append(shortcut)
        
        # Ëé∑ÂèñÂàÜÁ±ªÂíåÊ†áÁ≠æ
        categories = [row[0] for row in cursor.execute("SELECT DISTINCT category FROM command_shortcuts ORDER BY category")]
        tags = set()
        for shortcut in shortcuts:
            tags.update(shortcut['tags'])
        
        conn.close()
        
        return JSONResponse({
            "shortcuts": shortcuts,
            "categories": categories,
            "tags": list(tags)
        })
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/command-shortcuts")
async def create_command_shortcut(shortcut: CommandShortcutCreate):
    """ÂàõÂª∫ÂëΩ‰ª§Âø´Êç∑ÊñπÂºè"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO command_shortcuts (name, command, category, description, tags, working_dir, timeout, parameters)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            shortcut.name,
            shortcut.command,
            shortcut.category,
            shortcut.description,
            json.dumps(shortcut.tags),
            shortcut.working_dir,
            shortcut.timeout,
            json.dumps(shortcut.parameters)
        ))
        
        shortcut_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return JSONResponse({"id": shortcut_id, "message": "ÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÂàõÂª∫ÊàêÂäü"})
    except Exception as e:
        logger.exception(f"ÂàõÂª∫ÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.put("/api/command-shortcuts/{shortcut_id}")
async def update_command_shortcut(shortcut_id: int, shortcut: CommandShortcutUpdate):
    """Êõ¥Êñ∞ÂëΩ‰ª§Âø´Êç∑ÊñπÂºè"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT id FROM command_shortcuts WHERE id = ?", (shortcut_id,))
        if not cursor.fetchone():
            conn.close()
            return JSONResponse({"error": "ÂëΩ‰ª§Âø´Êç∑ÊñπÂºè‰∏çÂ≠òÂú®"}, status_code=404)
        
        updates = []
        params = []
        
        if shortcut.name is not None:
            updates.append("name = ?")
            params.append(shortcut.name)
        if shortcut.command is not None:
            updates.append("command = ?")
            params.append(shortcut.command)
        if shortcut.category is not None:
            updates.append("category = ?")
            params.append(shortcut.category)
        if shortcut.description is not None:
            updates.append("description = ?")
            params.append(shortcut.description)
        if shortcut.tags is not None:
            updates.append("tags = ?")
            params.append(json.dumps(shortcut.tags))
        if shortcut.working_dir is not None:
            updates.append("working_dir = ?")
            params.append(shortcut.working_dir)
        if shortcut.timeout is not None:
            updates.append("timeout = ?")
            params.append(shortcut.timeout)
        if shortcut.parameters is not None:
            updates.append("parameters = ?")
            params.append(json.dumps(shortcut.parameters))
        
        updates.append("updated_at = CURRENT_TIMESTAMP")
        params.append(shortcut_id)
        
        query = f"UPDATE command_shortcuts SET {', '.join(updates)} WHERE id = ?"
        cursor.execute(query, params)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "ÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÊõ¥Êñ∞ÊàêÂäü"})
    except Exception as e:
        logger.exception(f"Êõ¥Êñ∞ÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/command-shortcuts/{shortcut_id}")
async def delete_command_shortcut(shortcut_id: int):
    """Âà†Èô§ÂëΩ‰ª§Âø´Êç∑ÊñπÂºè"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM command_shortcuts WHERE id = ?", (shortcut_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "ÂëΩ‰ª§Âø´Êç∑ÊñπÂºè‰∏çÂ≠òÂú®"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "ÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÂà†Èô§ÊàêÂäü"})
    except Exception as e:
        logger.exception(f"Âà†Èô§ÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/command-shortcuts/categories")
async def get_command_shortcut_categories():
    """Ëé∑ÂèñÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÂàÜÁ±ª"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT DISTINCT category FROM command_shortcuts ORDER BY category")
        categories = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return JSONResponse({"categories": categories})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÂàÜÁ±ªÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/command-shortcuts/tags")
async def get_command_shortcut_tags():
    """Ëé∑ÂèñÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÊ†áÁ≠æ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT tags FROM command_shortcuts")
        all_tags = set()
        for row in cursor.fetchall():
            if row[0]:
                tags = json.loads(row[0])
                all_tags.update(tags)
        
        conn.close()
        
        return JSONResponse({"tags": list(all_tags)})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÂëΩ‰ª§Âø´Êç∑ÊñπÂºèÊ†áÁ≠æÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/command-shortcuts/history")
async def get_execution_history(limit: int = 50):
    """Ëé∑ÂèñÊâßË°åÂéÜÂè≤"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT eh.*, cs.name as shortcut_name
            FROM execution_history eh
            LEFT JOIN command_shortcuts cs ON eh.shortcut_id = cs.id
            ORDER BY eh.executed_at DESC
            LIMIT ?
        ''', (limit,))
        
        history = [dict(row) for row in cursor.fetchall()]
        conn.close()
        
        return JSONResponse({"history": history})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊâßË°åÂéÜÂè≤Â§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/command-shortcuts/{shortcut_id}/execute")
async def execute_command_shortcut(shortcut_id: int, params: Optional[Dict[str, Any]] = None):
    """ÊâßË°åÂëΩ‰ª§Âø´Êç∑ÊñπÂºè"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM command_shortcuts WHERE id = ?", (shortcut_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return JSONResponse({"error": "ÂëΩ‰ª§Âø´Êç∑ÊñπÂºè‰∏çÂ≠òÂú®"}, status_code=404)
        
        shortcut = dict(row)
        command = shortcut['command']
        working_dir = shortcut['working_dir'] or os.getcwd()
        timeout = shortcut['timeout']
        
        # ÊõøÊç¢ÂèÇÊï∞
        if params:
            for key, value in params.items():
                command = command.replace(f"${{{key}}}", str(value))
        
        # Â¢ûÂä†‰ΩøÁî®Ê¨°Êï∞
        cursor.execute("UPDATE command_shortcuts SET usage_count = usage_count + 1 WHERE id = ?", (shortcut_id,))
        conn.commit()
        
        conn.close()
        
        # ÊâßË°åÂëΩ‰ª§
        import subprocess
        start_time = time.time()
        
        try:
            result = subprocess.run(
                command,
                shell=True,
                cwd=working_dir,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            duration = time.time() - start_time
            
            # ‰øùÂ≠òÊâßË°åÂéÜÂè≤
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO execution_history (shortcut_id, command, working_dir, status, output, error, exit_code, duration)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                shortcut_id,
                command,
                working_dir,
                "success" if result.returncode == 0 else "failed",
                result.stdout,
                result.stderr,
                result.returncode,
                duration
            ))
            conn.commit()
            conn.close()
            
            return JSONResponse({
                "status": "success" if result.returncode == 0 else "failed",
                "output": result.stdout,
                "error": result.stderr,
                "exit_code": result.returncode,
                "duration": duration
            })
        except subprocess.TimeoutExpired:
            return JSONResponse({
                "status": "timeout",
                "error": f"ÂëΩ‰ª§ÊâßË°åË∂ÖÊó∂Ôºà{timeout}ÁßíÔºâ"
            }, status_code=408)
        except Exception as e:
            return JSONResponse({
                "status": "error",
                "error": str(e)
            }, status_code=500)
            
    except Exception as e:
        logger.exception(f"ÊâßË°åÂëΩ‰ª§Â§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# ÊèêÁ§∫ËØçÁÆ°ÁêÜÂô® API
# ============================================================================

class PromptCreate(BaseModel):
    title: str
    content: str
    category: str = "Ëá™ÂÆö‰πâ"
    description: str = ""
    tags: List[str] = []
    parameters: List[Dict[str, Any]] = []

class PromptUpdate(BaseModel):
    title: Optional[str] = None
    content: Optional[str] = None
    category: Optional[str] = None
    description: Optional[str] = None
    tags: Optional[List[str]] = None
    parameters: Optional[List[Dict[str, Any]]] = None

@app.get("/api/prompts")
async def get_prompts(
    search: Optional[str] = None,
    category: Optional[str] = None,
    favorite_only: bool = False
):
    """Ëé∑ÂèñÊèêÁ§∫ËØçÂàóË°®"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        query = "SELECT * FROM prompts WHERE 1=1"
        params = []
        
        if search:
            query += " AND (title LIKE ? OR description LIKE ? OR content LIKE ?)"
            params.extend([f"%{search}%", f"%{search}%", f"%{search}%"])
        
        if category:
            query += " AND category = ?"
            params.append(category)
        
        if favorite_only:
            query += " AND is_favorite = 1"
        
        query += " ORDER BY usage_count DESC, updated_at DESC"
        
        cursor.execute(query, params)
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        # Ëé∑ÂèñÂàÜÁ±ªÂíåÊ†áÁ≠æ
        categories = [row[0] for row in cursor.execute("SELECT DISTINCT category FROM prompts ORDER BY category")]
        tags = set()
        for prompt in prompts:
            tags.update(prompt['tags'])
        
        conn.close()
        
        return JSONResponse({
            "prompts": prompts,
            "categories": categories,
            "tags": list(tags)
        })
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊèêÁ§∫ËØçÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/prompts")
async def create_prompt(prompt: PromptCreate):
    """ÂàõÂª∫ÊèêÁ§∫ËØç"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO prompts (title, content, category, description, tags, parameters)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            prompt.title,
            prompt.content,
            prompt.category,
            prompt.description,
            json.dumps(prompt.tags),
            json.dumps(prompt.parameters)
        ))
        
        prompt_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return JSONResponse({"id": prompt_id, "message": "ÊèêÁ§∫ËØçÂàõÂª∫ÊàêÂäü"})
    except Exception as e:
        logger.exception(f"ÂàõÂª∫ÊèêÁ§∫ËØçÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/categories")
async def get_prompt_categories():
    """Ëé∑ÂèñÊèêÁ§∫ËØçÂàÜÁ±ª"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT DISTINCT category FROM prompts ORDER BY category")
        categories = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return JSONResponse({"categories": categories})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊèêÁ§∫ËØçÂàÜÁ±ªÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/tags")
async def get_prompt_tags():
    """Ëé∑ÂèñÊèêÁ§∫ËØçÊ†áÁ≠æ"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT tags FROM prompts")
        all_tags = set()
        for row in cursor.fetchall():
            if row[0]:
                tags = json.loads(row[0])
                all_tags.update(tags)
        
        conn.close()
        
        return JSONResponse({"tags": list(all_tags)})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊèêÁ§∫ËØçÊ†áÁ≠æÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/popular")
async def get_popular_prompts(limit: int = 10):
    """Ëé∑ÂèñÁÉ≠Èó®ÊèêÁ§∫ËØç"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts ORDER BY usage_count DESC, updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        conn.close()
        
        return JSONResponse({"prompts": prompts})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÁÉ≠Èó®ÊèêÁ§∫ËØçÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/recent")
async def get_recent_prompts(limit: int = 10):
    """Ëé∑ÂèñÊúÄËøëÊèêÁ§∫ËØç"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts ORDER BY updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        conn.close()
        
        return JSONResponse({"prompts": prompts})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊúÄËøëÊèêÁ§∫ËØçÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/favorite")
async def get_favorite_prompts(limit: int = 10):
    """Ëé∑ÂèñÊî∂ËóèÁöÑÊèêÁ§∫ËØç"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts WHERE is_favorite = 1 ORDER BY updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        conn.close()
        
        return JSONResponse({"prompts": prompts})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊî∂ËóèÊèêÁ§∫ËØçÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/{prompt_id}")
async def get_prompt(prompt_id: int):
    """Ëé∑ÂèñÂçï‰∏™ÊèêÁ§∫ËØç"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts WHERE id = ?", (prompt_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return JSONResponse({"error": "ÊèêÁ§∫ËØç‰∏çÂ≠òÂú®"}, status_code=404)
        
        prompt = dict(row)
        prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
        prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
        
        # Â¢ûÂä†‰ΩøÁî®Ê¨°Êï∞
        cursor.execute("UPDATE prompts SET usage_count = usage_count + 1 WHERE id = ?", (prompt_id,))
        conn.commit()
        
        conn.close()
        
        return JSONResponse(prompt)
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊèêÁ§∫ËØçÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.put("/api/prompts/{prompt_id}")
async def update_prompt(prompt_id: int, prompt: PromptUpdate):
    """Êõ¥Êñ∞ÊèêÁ§∫ËØç"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT id FROM prompts WHERE id = ?", (prompt_id,))
        if not cursor.fetchone():
            conn.close()
            return JSONResponse({"error": "ÊèêÁ§∫ËØç‰∏çÂ≠òÂú®"}, status_code=404)
        
        updates = []
        params = []
        
        if prompt.title is not None:
            updates.append("title = ?")
            params.append(prompt.title)
        if prompt.content is not None:
            updates.append("content = ?")
            params.append(prompt.content)
        if prompt.category is not None:
            updates.append("category = ?")
            params.append(prompt.category)
        if prompt.description is not None:
            updates.append("description = ?")
            params.append(prompt.description)
        if prompt.tags is not None:
            updates.append("tags = ?")
            params.append(json.dumps(prompt.tags))
        if prompt.parameters is not None:
            updates.append("parameters = ?")
            params.append(json.dumps(prompt.parameters))
        
        updates.append("updated_at = CURRENT_TIMESTAMP")
        params.append(prompt_id)
        
        query = f"UPDATE prompts SET {', '.join(updates)} WHERE id = ?"
        cursor.execute(query, params)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "ÊèêÁ§∫ËØçÊõ¥Êñ∞ÊàêÂäü"})
    except Exception as e:
        logger.exception(f"Êõ¥Êñ∞ÊèêÁ§∫ËØçÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/prompts/{prompt_id}")
async def delete_prompt(prompt_id: int):
    """Âà†Èô§ÊèêÁ§∫ËØç"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM prompts WHERE id = ?", (prompt_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "ÊèêÁ§∫ËØç‰∏çÂ≠òÂú®"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "ÊèêÁ§∫ËØçÂà†Èô§ÊàêÂäü"})
    except Exception as e:
        logger.exception(f"Âà†Èô§ÊèêÁ§∫ËØçÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/prompts/popular")
async def get_popular_prompts(limit: int = 10):
    """Ëé∑ÂèñÁÉ≠Èó®ÊèêÁ§∫ËØç"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM prompts ORDER BY usage_count DESC, updated_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        prompts = []
        for row in rows:
            prompt = dict(row)
            prompt['tags'] = json.loads(prompt['tags']) if prompt['tags'] else []
            prompt['parameters'] = json.loads(prompt['parameters']) if prompt['parameters'] else []
            prompts.append(prompt)
        
        conn.close()
        
        return JSONResponse({"prompts": prompts})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÁÉ≠Èó®ÊèêÁ§∫ËØçÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/prompts/{prompt_id}/usage")
async def increment_prompt_usage(prompt_id: int):
    """Â¢ûÂä†ÊèêÁ§∫ËØç‰ΩøÁî®Ê¨°Êï∞"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("UPDATE prompts SET usage_count = usage_count + 1 WHERE id = ?", (prompt_id,))
        
        if cursor.rowcount == 0:
            conn.close()
            return JSONResponse({"error": "ÊèêÁ§∫ËØç‰∏çÂ≠òÂú®"}, status_code=404)
        
        conn.commit()
        conn.close()
        
        return JSONResponse({"message": "‰ΩøÁî®Ê¨°Êï∞Â∑≤Êõ¥Êñ∞"})
    except Exception as e:
        logger.exception(f"Êõ¥Êñ∞‰ΩøÁî®Ê¨°Êï∞Â§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# ÊñπÊ°àÁîüÊàêÂô® API
# ============================================================================

class SolutionGenerate(BaseModel):
    requirement: str
    template_type: Optional[str] = None

@app.post("/api/solutions/generate")
async def generate_solution(request: Request, req: SolutionGenerate):
    """ÁîüÊàêÊñπÊ°à"""
    try:
        project_name = request.query_params.get("project")
        if not project_name:
            return JSONResponse({"error": "Áº∫Â∞ëÈ°πÁõÆÂêçÁß∞"}, status_code=400)
        
        project_path = get_project_path(project_name)
        logger.info(f"[generate_solution] È°πÁõÆË∑ØÂæÑ: {project_path}")
        
        # ‰ΩøÁî® iFlow Agent ÁîüÊàêÊñπÊ°à
        agent = get_agent(project_path)
        logger.info(f"[generate_solution] Agent ÂàõÂª∫ÊàêÂäü")
        
        prompt = f"""ËØ∑Ê†πÊçÆ‰ª•‰∏ãÈúÄÊ±ÇÔºåÁîüÊàê‰∏Ä‰∏™ËØ¶ÁªÜÁöÑÊäÄÊúØÊñπÊ°àÔºö

ÈúÄÊ±ÇÔºö{req.requirement}
{f'Ê®°ÊùøÁ±ªÂûãÔºö{req.template_type}' if req.template_type else ''}

ËØ∑Êèê‰æõÔºö
1. ÊäÄÊúØÊ†àÈÄâÊã©
2. Êû∂ÊûÑËÆæËÆ°
3. ÂÆûÁé∞Ê≠•È™§
4. ÂÖ≥ÈîÆ‰ª£Á†ÅÁ§∫‰æã
5. Ê≥®ÊÑè‰∫ãÈ°π

ËØ∑Áî® Markdown Ê†ºÂºèËæìÂá∫„ÄÇ"""
        
        logger.info(f"[generate_solution] ÂºÄÂßãÁîüÊàêÊñπÊ°àÔºåÈúÄÊ±Ç: {req.requirement}")
        
        solution_content = ""
        message_count = 0
        async for msg in agent.chat_stream(prompt):
            message_count += 1
            msg_type = msg.get("type")
            logger.debug(f"[generate_solution] Êî∂Âà∞Ê∂àÊÅØ {message_count}: {msg_type}, ÂÆåÊï¥Ê∂àÊÅØ: {msg}")
            
            # Â§ÑÁêÜ‰∏çÂêåÁ±ªÂûãÁöÑÊ∂àÊÅØ
            if msg_type == "content":
                content = msg.get("content", "")
                solution_content += content
                logger.debug(f"[generate_solution] Á¥ØËÆ°ÂÜÖÂÆπÈïøÂ∫¶: {len(solution_content)}")
            elif msg_type == "text":
                content = msg.get("text", "")
                solution_content += content
                logger.debug(f"[generate_solution] Á¥ØËÆ°ÂÜÖÂÆπÈïøÂ∫¶: {len(solution_content)}")
            elif msg_type == "assistant":
                # assistant Ê∂àÊÅØÂèØËÉΩÂåÖÂê´ÂÜÖÂÆπ
                if "content" in msg:
                    content = msg["content"]
                    if isinstance(content, str):
                        solution_content += content
                    elif isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict) and "text" in item:
                                solution_content += item["text"]
                    logger.debug(f"[generate_solution] Á¥ØËÆ°ÂÜÖÂÆπÈïøÂ∫¶: {len(solution_content)}")
            elif msg_type == "message":
                # message Á±ªÂûã
                content = msg.get("message", "")
                solution_content += content
                logger.debug(f"[generate_solution] Á¥ØËÆ°ÂÜÖÂÆπÈïøÂ∫¶: {len(solution_content)}")
        
        logger.info(f"[generate_solution] ÁîüÊàêÂÆåÊàêÔºåÂÖ± {message_count} Êù°Ê∂àÊÅØÔºåÂÜÖÂÆπÈïøÂ∫¶: {len(solution_content)}")
        
        # ‰øùÂ≠òÂà∞Êï∞ÊçÆÂ∫ì
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO solutions (requirement, solution, template_type)
            VALUES (?, ?, ?)
        ''', (req.requirement, solution_content, req.template_type))
        solution_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        logger.info(f"[generate_solution] ÊñπÊ°àÂ∑≤‰øùÂ≠òÔºåID: {solution_id}")
        
        return JSONResponse({
            "id": solution_id,
            "requirement": req.requirement,
            "solution": solution_content,
            "template_type": req.template_type
        })
    except Exception as e:
        logger.exception(f"ÁîüÊàêÊñπÊ°àÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/solutions/generate-stream")
async def generate_solution_stream(request: Request, req: SolutionGenerate):
    """ÊµÅÂºèÁîüÊàêÊñπÊ°à"""
    async def event_generator():
        try:
            project_name = request.query_params.get("project")
            if not project_name:
                yield f"data: {json.dumps({'error': 'Áº∫Â∞ëÈ°πÁõÆÂêçÁß∞'})}\n\n"
                return
            
            project_path = get_project_path(project_name)
            logger.info(f"[generate_solution_stream] È°πÁõÆË∑ØÂæÑ: {project_path}")
            
            agent = get_agent(project_path)
            logger.info(f"[generate_solution_stream] Agent ÂàõÂª∫ÊàêÂäü")
            
            prompt = f"""ËØ∑Ê†πÊçÆ‰ª•‰∏ãÈúÄÊ±ÇÔºåÁîüÊàê‰∏Ä‰∏™ËØ¶ÁªÜÁöÑÊäÄÊúØÊñπÊ°àÔºö

ÈúÄÊ±ÇÔºö{req.requirement}
{f'Ê®°ÊùøÁ±ªÂûãÔºö{req.template_type}' if req.template_type else ''}

ËØ∑Êèê‰æõÔºö
1. ÊäÄÊúØÊ†àÈÄâÊã©
2. Êû∂ÊûÑËÆæËÆ°
3. ÂÆûÁé∞Ê≠•È™§
4. ÂÖ≥ÈîÆ‰ª£Á†ÅÁ§∫‰æã
5. Ê≥®ÊÑè‰∫ãÈ°π

ËØ∑Áî® Markdown Ê†ºÂºèËæìÂá∫„ÄÇ"""
            
            logger.info(f"[generate_solution_stream] ÂºÄÂßãÁîüÊàêÊñπÊ°àÔºåÈúÄÊ±Ç: {req.requirement}")
            
            solution_content = ""
            message_count = 0
            async for msg in agent.chat_stream(prompt):
                message_count += 1
                msg_type = msg.get("type")
                logger.debug(f"[generate_solution_stream] Êî∂Âà∞Ê∂àÊÅØ {message_count}: {msg_type}")
                
                # Â§ÑÁêÜ‰∏çÂêåÁ±ªÂûãÁöÑÊ∂àÊÅØ
                if msg_type == "content":
                    content = msg.get("content", "")
                    solution_content += content
                    # ÊµÅÂºèÂèëÈÄÅÂÜÖÂÆπ
                    yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                elif msg_type == "text":
                    content = msg.get("text", "")
                    solution_content += content
                    yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                elif msg_type == "assistant":
                    if "content" in msg:
                        content = msg["content"]
                        if isinstance(content, str):
                            solution_content += content
                            yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                        elif isinstance(content, list):
                            for item in content:
                                if isinstance(item, dict) and "text" in item:
                                    solution_content += item["text"]
                                    yield f"data: {json.dumps({'type': 'content', 'content': item['text']})}\n\n"
            
            logger.info(f"[generate_solution_stream] ÁîüÊàêÂÆåÊàêÔºåÂÖ± {message_count} Êù°Ê∂àÊÅØÔºåÂÜÖÂÆπÈïøÂ∫¶: {len(solution_content)}")
            
            # ‰øùÂ≠òÂà∞Êï∞ÊçÆÂ∫ì
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO solutions (requirement, solution, template_type)
                VALUES (?, ?, ?)
            ''', (req.requirement, solution_content, req.template_type))
            solution_id = cursor.lastrowid
            conn.commit()
            conn.close()
            
            logger.info(f"[generate_solution_stream] ÊñπÊ°àÂ∑≤‰øùÂ≠òÔºåID: {solution_id}")
            
            # ÂèëÈÄÅÂÆåÊàê‰∫ã‰ª∂
            yield f"data: {json.dumps({'type': 'done', 'solution_id': solution_id, 'solution': solution_content})}\n\n"
            
        except Exception as e:
            logger.exception(f"[generate_solution_stream] ÁîüÊàêÊñπÊ°àÂ§±Ë¥•: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

@app.get("/api/solutions")
async def get_solutions(limit: int = 10):
    """Ëé∑ÂèñÂ∑≤‰øùÂ≠òÁöÑÊñπÊ°à"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM solutions ORDER BY created_at DESC LIMIT ?", (limit,))
        rows = cursor.fetchall()
        
        solutions = [dict(row) for row in rows]
        conn.close()
        
        return JSONResponse({"solutions": solutions})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊñπÊ°àÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/solutions/templates")
async def get_solution_templates():
    """Ëé∑ÂèñÊñπÊ°àÊ®°ÊùøÂàóË°®"""
    try:
        templates = [
            {
                "id": "web-app",
                "name": "Web Â∫îÁî®ÂºÄÂèë",
                "description": "ÈÄÇÁî®‰∫é Web Â∫îÁî®ÂºÄÂèëÁöÑÊäÄÊúØÊñπÊ°àÊ®°Êùø",
                "icon": "üåê"
            },
            {
                "id": "mobile-app",
                "name": "ÁßªÂä®Â∫îÁî®ÂºÄÂèë",
                "description": "ÈÄÇÁî®‰∫éÁßªÂä®Â∫îÁî®ÂºÄÂèëÁöÑÊäÄÊúØÊñπÊ°àÊ®°Êùø",
                "icon": "üì±"
            },
            {
                "id": "api-service",
                "name": "API ÊúçÂä°ÂºÄÂèë",
                "description": "ÈÄÇÁî®‰∫é API ÊúçÂä°ÂºÄÂèëÁöÑÊäÄÊúØÊñπÊ°àÊ®°Êùø",
                "icon": "üîå"
            },
            {
                "id": "data-analysis",
                "name": "Êï∞ÊçÆÂàÜÊûêÂπ≥Âè∞",
                "description": "ÈÄÇÁî®‰∫éÊï∞ÊçÆÂàÜÊûêÂπ≥Âè∞ÁöÑÊäÄÊúØÊñπÊ°àÊ®°Êùø",
                "icon": "üìä"
            },
            {
                "id": "microservices",
                "name": "ÂæÆÊúçÂä°Êû∂ÊûÑ",
                "description": "ÈÄÇÁî®‰∫éÂæÆÊúçÂä°Êû∂ÊûÑÁöÑÊäÄÊúØÊñπÊ°àÊ®°Êùø",
                "icon": "üîó"
            }
        ]
        
        return JSONResponse({"templates": templates})
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊñπÊ°àÊ®°ÊùøÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/solutions/{solution_id}")
async def get_solution(solution_id: int):
    """Ëé∑ÂèñÂçï‰∏™ÊñπÊ°à"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM solutions WHERE id = ?", (solution_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return JSONResponse({"error": "ÊñπÊ°à‰∏çÂ≠òÂú®"}, status_code=404)
        
        solution = dict(row)
        conn.close()
        
        return JSONResponse(solution)
    except Exception as e:
        logger.exception(f"Ëé∑ÂèñÊñπÊ°àÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# ‰∏öÂä°ÊµÅÁ®ãÊÄªÁªì API
# ============================================================================

@app.get("/api/business-flow/summary")
async def get_business_flow_summary(request: Request, limit: int = 50):
    """Ëé∑Âèñ‰∏öÂä°ÊµÅÁ®ãÊÄªÁªì"""
    try:
        project_name = request.query_params.get("project")
        if not project_name:
            return JSONResponse({"error": "Áº∫Â∞ëÈ°πÁõÆÂêçÁß∞"}, status_code=400)
        
        project_path = get_project_path(project_name)
        
        # Ëé∑Âèñ Git ÂéÜÂè≤
        import subprocess
        result = subprocess.run(
            ["git", "log", "--pretty=format:%H|%an|%ae|%ad|%s", f"-{limit}", "--date=iso"],
            cwd=project_path,
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            return JSONResponse({"error": "Êó†Ê≥ïËé∑Âèñ Git ÂéÜÂè≤"}, status_code=500)
        
        commits = []
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split('|', 4)
                if len(parts) == 5:
                    commits.append({
                        "hash": parts[0],
                        "author": parts[1],
                        "email": parts[2],
                        "date": parts[3],
                        "message": parts[4]
                    })
        
        # ‰ΩøÁî® AI ÊÄªÁªì‰∏öÂä°ÊµÅÁ®ã
        agent = get_agent(project_path)
        
        prompt = f"""ËØ∑ÂàÜÊûê‰ª•‰∏ã Git Êèê‰∫§ÂéÜÂè≤ÔºåÊÄªÁªìÈ°πÁõÆÁöÑ‰∏öÂä°ÊµÅÁ®ãÂíåÂäüËÉΩÊºîËøõÔºö

{json.dumps(commits[:20], ensure_ascii=False, indent=2)}

ËØ∑Êèê‰æõÔºö
1. ‰∏ªË¶ÅÂäüËÉΩÊ®°Âùó
2. ‰∏öÂä°ÊµÅÁ®ãÂõæ
3. ÂÖ≥ÈîÆÈáåÁ®ãÁ¢ë
4. ÊäÄÊúØÊºîËøõ

ËØ∑Áî® Markdown Ê†ºÂºèËæìÂá∫„ÄÇ"""
        
        summary_content = ""
        async for msg in agent.chat_stream(prompt):
            if msg.get("type") == "content":
                summary_content += msg.get("content", "")
        
        return JSONResponse({
            "business_flow": summary_content,
            "commits": commits
        })
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰∏öÂä°ÊµÅÁ®ãÊÄªÁªìÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/business-flow/timeline")
async def get_business_flow_timeline(request: Request, limit: int = 100):
    """Ëé∑Âèñ‰∏öÂä°ÊµÅÁ®ãÊó∂Èó¥Á∫ø"""
    try:
        project_name = request.query_params.get("project")
        if not project_name:
            return JSONResponse({"error": "Áº∫Â∞ëÈ°πÁõÆÂêçÁß∞"}, status_code=400)
        
        project_path = get_project_path(project_name)
        
        # Ëé∑Âèñ Git ÂéÜÂè≤
        import subprocess
        result = subprocess.run(
            ["git", "log", "--pretty=format:%H|%an|%ad|%s", f"-{limit}", "--date=iso"],
            cwd=project_path,
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            return JSONResponse({"error": "Êó†Ê≥ïËé∑Âèñ Git ÂéÜÂè≤"}, status_code=500)
        
        timeline = []
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split('|', 3)
                if len(parts) == 4:
                    timeline.append({
                        "hash": parts[0],
                        "author": parts[1],
                        "date": parts[2],
                        "message": parts[3]
                    })
        
        return JSONResponse({"timeline": timeline})
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰∏öÂä°ÊµÅÁ®ãÊó∂Èó¥Á∫øÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# ‰ª£Á†ÅÂÆ°Êü• API
# ============================================================================

class CodeReviewRequest(BaseModel):
    project_name: str
    file_path: str
    check_types: List[str] = ["quality", "style", "security", "performance"]

@app.post("/api/review/code")
async def review_code(req: CodeReviewRequest):
    """ÂÆ°Êü•‰ª£Á†Å"""
    try:
        project_path = get_project_path(req.project_name)
        file_path = os.path.join(project_path, req.file_path)
        
        # ËØªÂèñÊñá‰ª∂ÂÜÖÂÆπ
        if not os.path.exists(file_path):
            return JSONResponse({"error": "Êñá‰ª∂‰∏çÂ≠òÂú®"}, status_code=404)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ‰ΩøÁî® AI ÂÆ°Êü•‰ª£Á†Å
        agent = get_agent(project_path)
        
        check_types_str = ", ".join(req.check_types)
        prompt = f"""ËØ∑ÂØπ‰ª•‰∏ã‰ª£Á†ÅËøõË°å‰ª£Á†ÅÂÆ°Êü•ÔºåÊ£ÄÊü•‰ª•‰∏ãÊñπÈù¢Ôºö{check_types_str}

Êñá‰ª∂Ë∑ØÂæÑÔºö{req.file_path}

‰ª£Á†ÅÂÜÖÂÆπÔºö
```
{content}
```

ËØ∑Êèê‰æõÔºö
1. ÂèëÁé∞ÁöÑÈóÆÈ¢òÔºàÊåâ‰∏•ÈáçÁ®ãÂ∫¶ÂàÜÁ±ªÔºâ
2. ÊîπËøõÂª∫ËÆÆ
3. ÊúÄ‰Ω≥ÂÆûË∑µÂª∫ËÆÆ

ËØ∑Áî® JSON Ê†ºÂºèËæìÂá∫ÔºåÊ†ºÂºèÂ¶Ç‰∏ãÔºö
{{
  "summary": {{"total_issues": 0, "by_severity": {{"critical": 0, "high": 0, "medium": 0, "low": 0}}}},
  "issues": [
    {{
      "id": "1",
      "severity": "high",
      "category": "quality",
      "message": "ÈóÆÈ¢òÊèèËø∞",
      "line": 10,
      "suggestion": "ÊîπËøõÂª∫ËÆÆ"
    }}
  ]
}}"""
        
        review_result = ""
        async for msg in agent.chat_stream(prompt):
            if msg.get("type") == "content":
                review_result += msg.get("content", "")
        
        # Â∞ùËØïËß£Êûê JSON
        try:
            # ÊèêÂèñ JSON ÈÉ®ÂàÜ
            json_start = review_result.find('{')
            json_end = review_result.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                json_str = review_result[json_start:json_end]
                review_data = json.loads(json_str)
            else:
                # Â¶ÇÊûúÊó†Ê≥ïËß£ÊûêÔºåËøîÂõûÂéüÂßãÊñáÊú¨
                review_data = {
                    "summary": {"total_issues": 0, "by_severity": {}},
                    "issues": [],
                    "raw_output": review_result
                }
        except:
            review_data = {
                "summary": {"total_issues": 0, "by_severity": {}},
                "issues": [],
                "raw_output": review_result
            }
        
        return JSONResponse(review_data)
    except Exception as e:
        logger.exception(f"‰ª£Á†ÅÂÆ°Êü•Â§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

from backend.core.business_flow_summarizer import business_flow_summarizer

# ============================================================================
# ‰∏öÂä°ÊµÅÁ®ãÊÄªÁªì API
# ============================================================================

@app.get("/api/business-flow/summary")
async def get_business_flow_summary(limit: int = 50):
    """Ëé∑Âèñ‰∏öÂä°ÊµÅÁ®ãÊÄªÁªì"""
    try:
        # Á°Æ‰øù limit ÊòØÂêàÁêÜÁöÑÊï¥Êï∞
        if limit < 1:
            limit = 50
        if limit > 500:
            limit = 500
            
        result = business_flow_summarizer.generate_business_flow(limit)
        return {"success": True, "business_flow": result}
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰∏öÂä°ÊµÅÁ®ãÊÄªÁªìÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/business-flow/stats")
async def get_business_flow_stats():
    """Ëé∑Âèñ‰∏öÂä°ÊµÅÁ®ãÁªüËÆ°"""
    try:
        # Â§çÁî® generate_business_flow ÁöÑÁªìÊûúÔºåÊàñËÄÖÂÆûÁé∞‰∏ìÈó®ÁöÑ stats ÊñπÊ≥ï
        # ËøôÈáå‰∏∫‰∫ÜÁÆÄÂçïÔºåÁõ¥Êé•Â§çÁî® summary ÁöÑ summary ÈÉ®ÂàÜ
        result = business_flow_summarizer.generate_business_flow(limit=1)
        return {"success": True, "stats": result.get("summary", {})}
    except Exception as e:
        logger.exception(f"Ëé∑Âèñ‰∏öÂä°ÊµÅÁ®ãÁªüËÆ°Â§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# Êô∫ËÉΩÈúÄÊ±ÇÂàÜÊûê API
# ============================================================================

class RequirementAnalysisRequest(BaseModel):
    text: str
    image_path: Optional[str] = None
    project_name: Optional[str] = None

class MatchModulesRequest(BaseModel):
    keywords: List[str]
    project_name: Optional[str] = None

class GenerateSolutionRequest(BaseModel):
    analysis: Dict[str, Any]
    matched_modules: List[Dict[str, Any]]
    project_root: str = "."

class OptimizeRequirementRequest(BaseModel):
    text: str
    project_name: str = ""

class ProjectOptimizationRequest(BaseModel):
    focus: str = ""
    project_name: str

@app.post("/api/smart-requirement/optimize-project")
async def optimize_project(req: ProjectOptimizationRequest):
    try:
        project_path = get_project_path(req.project_name)
        result = await smart_requirement_service.analyze_project_optimization(req.focus, project_path)
        return {"success": True, "result": result}
    except Exception as e:
        logger.exception(f"Project Optimization failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/optimize")
async def optimize_requirement(req: OptimizeRequirementRequest):
    try:
        result = await smart_requirement_service.optimize_requirement(req.text, req.project_name)
        return {"success": True, "result": result}
    except Exception as e:
        logger.exception(f"Optimization failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/step1-analyze")
async def analyze_requirement_step1(req: RequirementAnalysisRequest):
    try:
        project_path = get_project_path(req.project_name)
        analysis = await smart_requirement_service.analyze_requirement(req.text, req.image_path, project_path)
        return {"success": True, "analysis": analysis}
    except Exception as e:
        logger.exception(f"Step 1 failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/step2-match")
async def match_modules_step2(req: MatchModulesRequest):
    try:
        project_path = get_project_path(req.project_name)
        matched_modules = await smart_requirement_service.match_modules(req.keywords, project_path)
        return {"success": True, "matched_modules": matched_modules}
    except Exception as e:
        logger.exception(f"Step 2 failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

class RefineSolutionRequest(BaseModel):
    previous_solution: Dict[str, Any]
    feedback: str

@app.post("/api/smart-requirement/refine")
async def refine_solution(req: RefineSolutionRequest):
    try:
        updated_solution = await smart_requirement_service.refine_solution(req.previous_solution, req.feedback)
        return {"success": True, "updated_solution": updated_solution}
    except Exception as e:
        logger.exception(f"Refinement failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/step2-5-context")
async def generate_business_context(req: GenerateSolutionRequest):
    try:
        # We reuse GenerateSolutionRequest but only need matched_modules
        # Assuming project_root is accessible or passed. Here we use "."
        project_root = "." 
        context_data = await smart_requirement_service.generate_business_context(req.matched_modules, project_root)
        return {
            "success": True, 
            "context": context_data
        }
    except Exception as e:
        logger.exception(f"Step 2.5 failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/step3-solution")
async def generate_solution_step3(req: GenerateSolutionRequest):
    try:
        solution_data = await smart_requirement_service.generate_solution(req.analysis, req.matched_modules)
        return {
            "success": True, 
            "solution_doc": solution_data.get("solution_doc", ""),
            "execution_plan": solution_data.get("execution_plan", {}),
            "api_design": solution_data.get("api_design", [])
        }
    except Exception as e:
        logger.exception(f"Step 3 failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/project/file-content")
async def get_project_file_content(path: str, project_name: Optional[str] = None):
    """Securely read a file from the project directory"""
    try:
        # Determine root
        if project_name:
            root = get_project_path(project_name)
        else:
            root = os.getcwd() # Fallback to current working dir if no project specified
        
        # Sanitize path to prevent directory traversal
        safe_path = os.path.normpath(os.path.join(root, path))
        if not safe_path.startswith(os.path.abspath(root)):
             return JSONResponse({"error": "Access denied: Path outside project root"}, status_code=403)
             
        if not os.path.exists(safe_path) or not os.path.isfile(safe_path):
             return JSONResponse({"error": "File not found"}, status_code=404)
             
        # Read content (limit size)
        file_size = os.path.getsize(safe_path)
        if file_size > 1024 * 1024: # 1MB limit
             return JSONResponse({"error": "File too large to preview"}, status_code=400)
             
        with open(safe_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            
        return {"content": content, "path": path, "size": file_size}
    except Exception as e:
        logger.error(f"Error reading file {path}: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

class SaveAnalysisRequest(BaseModel):
    project_name: str
    title: str
    content: str
    folder: str = "docs/requirements"

@app.post("/api/smart-requirement/save")
async def save_analysis_doc(req: SaveAnalysisRequest):
    """Save analysis result as a markdown file"""
    try:
        root = get_project_path(req.project_name)
        target_dir = os.path.join(root, req.folder)
        os.makedirs(target_dir, exist_ok=True)
        
        # Sanitize filename
        safe_title = "".join([c for c in req.title if c.isalnum() or c in (' ', '-', '_')]).strip()
        safe_title = safe_title.replace(' ', '_')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{safe_title}_{timestamp}.md"
        
        file_path = os.path.join(target_dir, filename)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(req.content)
            
        return {"success": True, "path": f"{req.folder}/{filename}"}
    except Exception as e:
        logger.exception(f"Failed to save analysis: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/smart-requirement/analyze")
async def analyze_smart_requirement(req: RequirementAnalysisRequest):
    """Êô∫ËÉΩÂàÜÊûêÈúÄÊ±Ç"""
    try:
        project_path = get_project_path(req.project_name)
        
        # 1. Analyze Requirement
        analysis = await smart_requirement_service.analyze_requirement(req.text, req.image_path, project_path)
        
        # 2. Match Modules
        keywords = analysis.get("keywords", [])
        matched_modules = await smart_requirement_service.match_modules(keywords, project_path)
        
        # 3. Generate Solution
        solution_data = await smart_requirement_service.generate_solution(analysis, matched_modules)
        
        return {
            "success": True,
            "analysis": analysis,
            "matched_modules": matched_modules,
            "solution_doc": solution_data.get("solution_doc", ""),
            "execution_plan": solution_data.get("execution_plan", {})
        }
    except Exception as e:
        logger.exception(f"Êô∫ËÉΩÈúÄÊ±ÇÂàÜÊûêÂ§±Ë¥•: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================================
# CI/CD Generator API
# ============================================================================

@app.get("/api/cicd/platforms")
async def get_cicd_platforms():
    """Get supported CI/CD platforms"""
    return {
        "success": True,
        "platforms": [
            {"id": "github", "name": "GitHub Actions", "icon": "github"},
            {"id": "gitlab", "name": "GitLab CI", "icon": "gitlab"},
            {"id": "jenkins", "name": "Jenkins", "icon": "jenkins"}
        ]
    }

class CICDGenerateRequest(BaseModel):
    platform: str
    projectType: str
    projectName: str
    config: Optional[Dict[str, Any]] = None

@app.post("/api/cicd/generate")
async def generate_cicd_config(req: CICDGenerateRequest):
    """Generate CI/CD configuration files"""
    try:
        result = cicd_generator.generate(
            req.platform, 
            req.projectType, 
            req.projectName, 
            req.config
        )
        return result
    except Exception as e:
        logger.exception(f"CI/CD generation failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


# ============================================================================
# Project Template API
# ============================================================================

@app.get("/api/templates")
async def get_project_templates():
    """Get available project templates"""
    service = get_project_template_service()
    return service.get_templates()

class TemplateGenerateRequest(BaseModel):
    templateId: str
    projectName: str
    path: str
    config: Optional[Dict[str, Any]] = None

@app.post("/api/templates/generate")
async def generate_project_from_template(req: TemplateGenerateRequest):
    """Generate a new project from a template"""
    try:
        service = get_project_template_service()
        # Verify path exists
        if not os.path.exists(req.path):
            return JSONResponse({"error": "Target path does not exist"}, status_code=400)
            
        result = service.generate_project(
            req.templateId,
            req.projectName,
            req.path,
            req.config
        )
        return result
    except Exception as e:
        logger.exception(f"Template generation failed: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


# ============================================================================
# TaskMaster API (Real Implementation)
# ============================================================================

@app.get("/api/taskmaster/tasks/{project_name}")
async def get_project_tasks(project_name: str):
    """Get tasks for a project"""
    tasks = task_master_service.get_tasks(project_name)
    return {"tasks": tasks}

@app.post("/api/taskmaster/tasks/{project_name}")
async def create_project_task(project_name: str, task: TaskModel):
    """Create a new task"""
    try:
        # Ensure project name matches
        task.project_name = project_name 
        created_task = task_master_service.create_task(task)
        return {"success": True, "task": created_task}
    except Exception as e:
        logger.error(f"Error creating task: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.put("/api/taskmaster/tasks/{project_name}/{task_id}")
async def update_project_task(project_name: str, task_id: str, updates: Dict[str, Any]):
    """Update a task"""
    try:
        updated_task = task_master_service.update_task(task_id, updates)
        if not updated_task:
             return JSONResponse({"error": "Task not found"}, status_code=404)
        return {"success": True, "task": updated_task}
    except Exception as e:
        logger.error(f"Error updating task: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/taskmaster/tasks/{project_name}/{task_id}")
async def delete_project_task(project_name: str, task_id: str):
    """Delete a task"""
    success = task_master_service.delete_task(task_id)
    return {"success": success}

@app.get("/api/taskmaster/prd/{project_name}")
async def get_project_prds(project_name: str):
    """Get list of PRDs for a project"""
    prds = task_master_service.get_prds(project_name)
    return prds

@app.get("/api/taskmaster/prd/{project_name}/{prd_name}")
async def get_prd_details(project_name: str, prd_name: str):
    """Get PRD content"""
    prd = task_master_service.get_prd_content(project_name, prd_name)
    if not prd:
        return JSONResponse({"error": "PRD not found"}, status_code=404)
    return prd

class PRDSaveRequest(BaseModel):
    title: str
    content: str

@app.post("/api/taskmaster/prd/{project_name}")
async def save_project_prd(project_name: str, req: PRDSaveRequest):
    """Save/Update a PRD"""
    try:
        saved_prd = task_master_service.save_prd(project_name, req.title, req.content)
        return {"success": True, "prd": saved_prd}
    except Exception as e:
         logger.error(f"Error saving PRD: {e}")
         return JSONResponse({"error": str(e)}, status_code=500)

# --- Database Query API ---
  
class DatabaseConnectRequest(BaseModel):
    db_type: str = "sqlite"  # sqlite, mysql, postgresql, sqlserver, oracle
    db_path: Optional[str] = None  # SQLite ‰∏ìÁî®
    host: Optional[str] = None
    port: Optional[int] = None
    database: Optional[str] = None
    username: Optional[str] = None
    password: Optional[str] = None
    connection_name: Optional[str] = None

@app.post("/api/database/connect")
async def connect_database(req: DatabaseConnectRequest):
    """ËøûÊé•Êï∞ÊçÆÂ∫ìÔºàÊîØÊåÅÂ§öÁßçÁ±ªÂûãÔºâ"""
    try:
        success = False
        
        if req.db_type == "sqlite":
            if not req.db_path:
                return JSONResponse({"error": "db_path is required for SQLite"}, status_code=400)
            success = database_query_service.connect_sqlite(req.db_path, req.connection_name)
        elif req.db_type == "mysql":
            if not all([req.host, req.port, req.database, req.username, req.password]):
                return JSONResponse({"error": "host, port, database, username, password are required for MySQL"}, status_code=400)
            success = database_query_service.connect_mysql(
                req.host, req.port, req.database, req.username, req.password, req.connection_name
            )
        elif req.db_type == "postgresql":
            if not all([req.host, req.port, req.database, req.username, req.password]):
                return JSONResponse({"error": "host, port, database, username, password are required for PostgreSQL"}, status_code=400)
            success = database_query_service.connect_postgresql(
                req.host, req.port, req.database, req.username, req.password, req.connection_name
            )
        elif req.db_type == "sqlserver":
            if not all([req.host, req.port, req.database, req.username, req.password]):
                return JSONResponse({"error": "host, port, database, username, password are required for SQL Server"}, status_code=400)
            success = database_query_service.connect_sqlserver(
                req.host, req.port, req.database, req.username, req.password, req.connection_name
            )
        elif req.db_type == "oracle":
            if not all([req.host, req.port, req.database, req.username, req.password]):
                return JSONResponse({"error": "host, port, database, username, password are required for Oracle"}, status_code=400)
            success = database_query_service.connect_oracle(
                req.host, req.port, req.database, req.username, req.password, req.connection_name
            )
        else:
            return JSONResponse({"error": f"Unsupported database type: {req.db_type}"}, status_code=400)
        
        if success:
            return {"success": True, "message": "Database connected successfully"}
        else:
            return JSONResponse({"error": "Failed to connect to database"}, status_code=400)
    except Exception as e:
        logger.error(f"Error connecting to database: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)
@app.post("/api/database/disconnect/{connection_name}")
async def disconnect_database(connection_name: str):
    """Êñ≠ÂºÄÊï∞ÊçÆÂ∫ìËøûÊé•"""
    try:
        success = database_query_service.disconnect(connection_name)
        if success:
            return {"success": True, "message": "Database disconnected successfully"}
        else:
            return JSONResponse({"error": "Connection not found"}, status_code=404)
    except Exception as e:
        logger.error(f"Error disconnecting database: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/connections")
async def get_database_connections():
    """Ëé∑ÂèñÊâÄÊúâÊï∞ÊçÆÂ∫ìËøûÊé•"""
    try:
        connections = database_query_service.get_connections()
        return {"connections": connections}
    except Exception as e:
        logger.error(f"Error getting connections: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/tables/{connection_name}")
async def get_database_tables(connection_name: str):
    """Ëé∑ÂèñÊï∞ÊçÆÂ∫ì‰∏≠ÁöÑÊâÄÊúâË°®"""
    try:
        tables = database_query_service.get_tables(connection_name)
        return {"tables": tables}
    except Exception as e:
        logger.error(f"Error getting tables: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/table/{connection_name}/{table_name}")
async def get_table_info(connection_name: str, table_name: str):
      """Ëé∑ÂèñË°®ÁöÑËØ¶ÁªÜ‰ø°ÊÅØ"""
      try:
          logger.info(f"Getting table info - connection: {connection_name}, table: {table_name}")
          table_info = database_query_service.get_table_info(connection_name, table_name)
          if table_info:
              return {
                  "name": table_info.name,
                  "type": table_info.type,
                  "row_count": table_info.row_count,
                  "columns": table_info.columns,
                  "indexes": table_info.indexes
              }
          else:
              logger.warning(f"Table not found: {table_name}")
              return JSONResponse({"error": "Table not found"}, status_code=404)
      except Exception as e:
          logger.error(f"Error getting table info: {e}")
          return JSONResponse({"error": str(e)}, status_code=500)
class DatabaseQueryRequest(BaseModel):
    connection_name: str
    sql: str
    params: Optional[Dict[str, Any]] = None

@app.post("/api/database/query")
async def execute_database_query(req: DatabaseQueryRequest):
    """ÊâßË°å SQL Êü•ËØ¢"""
    try:
        result = database_query_service.execute_query(req.connection_name, req.sql, req.params)
        if result.success:
            return {
                "success": True,
                "columns": result.columns,
                "rows": result.rows,
                "row_count": result.row_count,
                "execution_time": result.execution_time
            }
        else:
            return JSONResponse({"error": result.error}, status_code=400)
    except Exception as e:
        logger.error(f"Error executing query: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/export/{connection_name}/{format}")
async def export_query_result(connection_name: str, format: str, sql: str, params: Optional[str] = None):
    """ÂØºÂá∫Êü•ËØ¢ÁªìÊûú"""
    try:
        params_dict = json.loads(params) if params else None
        
        if format == "csv":
            data = database_query_service.export_to_csv(connection_name, sql, params_dict)
            return Response(content=data, media_type="text/csv", headers={
                "Content-Disposition": f"attachment; filename=query_result.csv"
            })
        elif format == "json":
            data = database_query_service.export_to_json(connection_name, sql, params_dict)
            return Response(content=data, media_type="application/json", headers={
                "Content-Disposition": f"attachment; filename=query_result.json"
            })
        elif format == "excel":
            data = database_query_service.export_to_excel(connection_name, sql, params_dict)
            return Response(content=data, media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", headers={
                "Content-Disposition": f"attachment; filename=query_result.xlsx"
            })
        else:
            return JSONResponse({"error": "Unsupported format. Use csv, json, or excel"}, status_code=400)
    except Exception as e:
        logger.error(f"Error exporting query result: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/templates")
async def get_query_templates():
    """Ëé∑ÂèñÊü•ËØ¢Ê®°Êùø"""
    try:
        templates = database_query_service.get_query_templates()
        return {"templates": templates}
    except Exception as e:
        logger.error(f"Error getting templates: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

class AddTemplateRequest(BaseModel):
    name: str
    sql: str
    description: Optional[str] = ""
    category: Optional[str] = "Ëá™ÂÆö‰πâ"
    params: Optional[List[str]] = None

@app.post("/api/database/templates")
async def add_query_template(req: AddTemplateRequest):
    """Ê∑ªÂä†Êü•ËØ¢Ê®°Êùø"""
    try:
        template = database_query_service.add_query_template(
            req.name, req.sql, req.description, req.category, req.params
        )
        return {"success": True, "template": template}
    except Exception as e:
        logger.error(f"Error adding template: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/history")
async def get_query_history(limit: int = 50):
    """Ëé∑ÂèñÊü•ËØ¢ÂéÜÂè≤"""
    try:
        history = database_query_service.get_query_history(limit)
        return {"history": history}
    except Exception as e:
        logger.error(f"Error getting query history: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

class DatabaseConfigRequest(BaseModel):
    project_name: str
    config_name: str
    db_type: str
    config: Dict[str, Any]

@app.post("/api/database/save-config")
async def save_database_config(req: DatabaseConfigRequest):
    """‰øùÂ≠òÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÂà∞È°πÁõÆ"""
    try:
        import os
        import json
        
        project_path = get_project_path(req.project_name)
        if not project_path:
            return JSONResponse({"error": "Project not found"}, status_code=404)
        
        # ÂàõÂª∫Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÁõÆÂΩï
        config_dir = os.path.join(project_path, ".database")
        os.makedirs(config_dir, exist_ok=True)
        
        # ‰øùÂ≠òÈÖçÁΩÆ
        config_file = os.path.join(config_dir, f"{req.config_name}.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump({
                "name": req.config_name,
                "db_type": req.db_type,
                "config": req.config,
                "created_at": datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved database config: {req.config_name} for project: {req.project_name}")
        return {"success": True, "message": "Database config saved successfully"}
    except Exception as e:
        logger.error(f"Error saving database config: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/database/configs/{project_name}")
async def get_database_configs(project_name: str):
    """Ëé∑ÂèñÈ°πÁõÆÁöÑÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÂàóË°®"""
    try:
        import os
        import json
        
        project_path = get_project_path(project_name)
        if not project_path:
            return JSONResponse({"error": "Project not found"}, status_code=404)
        
        config_dir = os.path.join(project_path, ".database")
        configs = []
        
        if os.path.exists(config_dir):
            for filename in os.listdir(config_dir):
                if filename.endswith('.json'):
                    config_file = os.path.join(config_dir, filename)
                    try:
                        with open(config_file, 'r', encoding='utf-8') as f:
                            config = json.load(f)
                            configs.append(config)
                    except Exception as e:
                        logger.error(f"Error loading config {filename}: {e}")
        
        return {"configs": configs}
    except Exception as e:
        logger.error(f"Error getting database configs: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/database/config/{project_name}/{config_name}")
async def delete_database_config(project_name: str, config_name: str):
    """Âà†Èô§Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ"""
    try:
        import os
        
        project_path = get_project_path(project_name)
        if not project_path:
            return JSONResponse({"error": "Project not found"}, status_code=404)
        
        config_file = os.path.join(project_path, ".database", f"{config_name}.json")
        
        if os.path.exists(config_file):
            os.remove(config_file)
            logger.info(f"Deleted database config: {config_name} for project: {project_name}")
            return {"success": True, "message": "Database config deleted successfully"}
        else:
            return JSONResponse({"error": "Config not found"}, status_code=404)
    except Exception as e:
        logger.error(f"Error deleting database config: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

def parse_gorm_dsn(dsn: str) -> dict:
    """Ëß£Êûê GORM DSN Ê†ºÂºèÁöÑËøûÊé•Â≠óÁ¨¶‰∏≤
    
    ÊîØÊåÅÊ†ºÂºè:
    - mysql:user:password@tcp(host:port)/database
    - postgresql://user:password@host:port/database
    """
    import re
    
    result = {
        'type': 'unknown',
        'host': '',
        'port': '',
        'user': '',
        'password': '',
        'database': ''
    }
    
    try:
        # MySQL Ê†ºÂºè: mysql:user:password@tcp(host:port)/database
        if dsn.startswith('mysql:'):
            result['type'] = 'mysql'
            # ÁßªÈô§ mysql: ÂâçÁºÄ
            dsn = dsn[6:]
            
            # Ëß£Êûê user:password@tcp(host:port)/database
            match = re.match(r'([^:]*):([^@]*)@tcp\(([^:]+):(\d+)\)/(.+)', dsn)
            if match:
                result['user'] = match.group(1)
                result['password'] = match.group(2)
                result['host'] = match.group(3)
                result['port'] = int(match.group(4))
                result['database'] = match.group(5)
                logger.info(f"Ëß£Êûê MySQL DSN ÊàêÂäü: {result}")
        
        # PostgreSQL Ê†ºÂºè: postgresql://user:password@host:port/database
        elif dsn.startswith('postgresql://'):
            result['type'] = 'postgresql'
            # ÁßªÈô§ postgresql:// ÂâçÁºÄ
            dsn = dsn[11:]
            
            # Ëß£Êûê user:password@host:port/database
            match = re.match(r'([^:]*):([^@]*)@([^:]+):(\d+)/(.+)', dsn)
            if match:
                result['user'] = match.group(1)
                result['password'] = match.group(2)
                result['host'] = match.group(3)
                result['port'] = int(match.group(4))
                result['database'] = match.group(5)
                logger.info(f"Ëß£Êûê PostgreSQL DSN ÊàêÂäü: {result}")
        
        # ÁÆÄÂçïÊ†ºÂºè: user:password@host:port/database
        elif '@' in dsn and '/' in dsn:
            match = re.match(r'([^:]*):([^@]*)@([^:]+):(\d+)/(.+)', dsn)
            if match:
                result['user'] = match.group(1)
                result['password'] = match.group(2)
                result['host'] = match.group(3)
                result['port'] = int(match.group(4))
                result['database'] = match.group(5)
                logger.info(f"Ëß£ÊûêÁÆÄÂçï DSN ÊàêÂäü: {result}")
        
        return result
    except Exception as e:
        logger.error(f"Ëß£Êûê DSN Â§±Ë¥•: {e}")
        return result


def parse_database_config(config_data: dict, config_type: str) -> list:
    """‰ªéÈÖçÁΩÆÊï∞ÊçÆ‰∏≠Ëß£ÊûêÊï∞ÊçÆÂ∫ìËøûÊé•‰ø°ÊÅØ"""
    db_connections = []
    
    try:
        logger.info(f"ÂºÄÂßãËß£ÊûêÈÖçÁΩÆÊï∞ÊçÆÔºåÁ±ªÂûã: {config_type}")
        logger.info(f"ÈÖçÁΩÆÊï∞ÊçÆÈîÆ: {list(config_data.keys())}")
        
        # Â∏∏ËßÅÁöÑÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÈîÆÂêç
        db_keys = ['database', 'db', 'sql', 'mysql', 'postgres', 'postgresql', 'mongodb', 'redis']
        
        def extract_db_info(data, prefix=''):
            """ÈÄíÂΩíÊèêÂèñÊï∞ÊçÆÂ∫ì‰ø°ÊÅØ"""
            if isinstance(data, dict):
                for key, value in data.items():
                    key_lower = key.lower()
                    
                    # Ê£ÄÊü•ÊòØÂê¶ÊòØÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ
                    if any(db_key in key_lower for db_key in db_keys):
                        if isinstance(value, dict):
                            db_info = {
                                'name': key,
                                'type': 'unknown',
                                'config': {}
                            }
                            
                            logger.info(f"ÊâæÂà∞Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ: {key}, ÂÄº: {value}")
                            
                            # Â∞ùËØïËØÜÂà´Êï∞ÊçÆÂ∫ìÁ±ªÂûã
                            for db_type in ['mysql', 'postgres', 'postgresql', 'mongodb', 'redis', 'sqlite']:
                                if db_type in key_lower:
                                    db_info['type'] = db_type
                                    break
                            
                            # ÊèêÂèñËøûÊé•ÂèÇÊï∞
                            for param in ['host', 'port', 'user', 'username', 'password', 'database', 'dbname', 'name', 'path', 'dsn', 'url', 'address']:
                                if param in value:
                                    db_info['config'][param] = value[param]
                                    logger.info(f"ÊèêÂèñÂèÇÊï∞ {param}: {value[param]}")
                            
                            # Â¶ÇÊûúÈÖçÁΩÆ‰∏∫Á©∫‰ΩÜÊúâÊï∞ÊçÆÔºåÂ∞ùËØï‰ªéÊï¥‰∏™ÂØπË±°‰∏≠ÊèêÂèñ
                            if not db_info['config']:
                                db_info['config'] = value
                                logger.info(f"‰ΩøÁî®ÂÆåÊï¥ÈÖçÁΩÆ: {value}")
                            
                            if db_info['config']:
                                db_connections.append(db_info)
                                logger.info(f"Ê∑ªÂä†Êï∞ÊçÆÂ∫ìËøûÊé•: {db_info}")
                    else:
                        extract_db_info(value, f'{prefix}.{key}' if prefix else key)
            elif isinstance(data, list):
                for item in data:
                    extract_db_info(item, prefix)
        
        # ÂÖàËøõË°åÁâπÊÆäÂ§ÑÁêÜÔºàÈÅøÂÖçÈáçÂ§çÔºâ
        if config_type in ['yaml', 'toml']:
            # Êü•Êâæ datasource ÈÖçÁΩÆ
            if 'datasource' in config_data:
                datasource = config_data['datasource']
                if isinstance(datasource, dict):
                    for ds_name, ds_config in datasource.items():
                        if isinstance(ds_config, dict):
                            db_info = {
                                'name': ds_name,
                                'type': 'unknown',
                                'config': {}
                            }
                            for param in ['host', 'port', 'user', 'password', 'database', 'dbname', 'driver']:
                                if param in ds_config:
                                    db_info['config'][param] = ds_config[param]
                            
                            # Â∞ùËØïËß£Êûê GORM DSN Ê†ºÂºèÁöÑ link Â≠óÊÆµ
                            if 'link' in ds_config:
                                dsn_info = parse_gorm_dsn(ds_config['link'])
                                if dsn_info:
                                    db_info['config'].update(dsn_info)
                                    db_info['type'] = dsn_info.get('type', 'unknown')
                            
                            if db_info['config']:
                                db_connections.append(db_info)
                                logger.info(f"Ê∑ªÂä† datasource ÈÖçÁΩÆ: {db_info}")
            
            # Êü•Êâæ database ÈÖçÁΩÆÔºàGo È°πÁõÆÂ∏∏ËßÅÁªìÊûÑÔºâ
            if 'database' in config_data:
                database = config_data['database']
                if isinstance(database, dict):
                    # Ê£ÄÊü•ÊòØÂê¶ÊúâÂµåÂ•óÁöÑÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàÂ¶Ç defaultRead, backup, sysolinÔºâ
                    for db_name, db_config in database.items():
                        if db_name in ['logger', 'cacheKey']:
                            continue  # Ë∑≥ËøáÈùûÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ
                        
                        if isinstance(db_config, dict) and 'link' in db_config:
                            # ËøôÊòØ‰∏Ä‰∏™Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ
                            db_info = {
                                'name': db_name,
                                'type': 'unknown',
                                'config': {}
                            }
                            
                            # Ëß£Êûê GORM DSN Ê†ºÂºèÁöÑ link Â≠óÊÆµ
                            dsn_info = parse_gorm_dsn(db_config['link'])
                            if dsn_info:
                                db_info['config'].update(dsn_info)
                                db_info['type'] = dsn_info.get('type', 'unknown')
                            
                            if db_info['config']:
                                db_connections.append(db_info)
                                logger.info(f"Ê∑ªÂä†Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ {db_name}: {db_info}")
                        
                        elif isinstance(db_config, list):
                            # Â§ÑÁêÜÊï∞ÁªÑÁ±ªÂûãÁöÑÈÖçÁΩÆÔºàÂ¶Ç default: [{role: 'master', link: '...'}, {role: 'slave', link: '...'}]Ôºâ
                            for idx, item in enumerate(db_config):
                                if isinstance(item, dict) and 'link' in item:
                                    db_info = {
                                        'name': f"{db_name}_{item.get('role', idx)}",
                                        'type': 'unknown',
                                        'config': {}
                                    }
                                    
                                    # Ëß£Êûê GORM DSN Ê†ºÂºèÁöÑ link Â≠óÊÆµ
                                    dsn_info = parse_gorm_dsn(item['link'])
                                    if dsn_info:
                                        db_info['config'].update(dsn_info)
                                        db_info['type'] = dsn_info.get('type', 'unknown')
                                    
                                    if db_info['config']:
                                        db_connections.append(db_info)
                                        logger.info(f"Ê∑ªÂä†Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ {db_info['name']}: {db_info}")
                    
                    # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞‰ªª‰ΩïÂµåÂ•óÈÖçÁΩÆÔºåÂ∞ùËØïÁõ¥Êé•Ëß£Êûê database ÂØπË±°
                    if not db_connections:
                        db_info = {
                            'name': 'database',
                            'type': database.get('type', 'unknown'),
                            'config': {}
                        }
                        
                        # ÊèêÂèñÊâÄÊúâÂèØËÉΩÁöÑÈÖçÁΩÆÂèÇÊï∞
                        for param in ['host', 'port', 'user', 'username', 'password', 'database', 'dbname', 'name', 'address', 'dsn', 'url', 'charset', 'link']:
                            if param in database:
                                if param == 'link':
                                    # Â∞ùËØïËß£Êûê GORM DSN Ê†ºÂºè
                                    dsn_info = parse_gorm_dsn(database[param])
                                    if dsn_info:
                                        db_info['config'].update(dsn_info)
                                        db_info['type'] = dsn_info.get('type', 'unknown')
                                else:
                                    db_info['config'][param] = database[param]
                        
                        # Â¶ÇÊûúÈÖçÁΩÆ‰∏∫Á©∫‰ΩÜÊúâÊï∞ÊçÆÔºå‰ΩøÁî®Êï¥‰∏™ÂØπË±°
                        if not db_info['config']:
                            db_info['config'] = database
                        
                        if db_info['config']:
                            db_connections.append(db_info)
                            logger.info(f"Ê∑ªÂä† database ÈÖçÁΩÆ: {db_info}")
            
            # Êü•Êâæ mysql, postgres Á≠âÁõ¥Êé•ÈÖçÁΩÆ
            for db_type in ['mysql', 'postgres', 'postgresql', 'mongodb', 'redis']:
                if db_type in config_data:
                    db_config = config_data[db_type]
                    if isinstance(db_config, dict):
                        db_info = {
                            'name': db_type,
                            'type': db_type,
                            'config': {}
                        }
                        
                        # Ê£ÄÊü•ÊòØÂê¶Êúâ link Â≠óÊÆµÔºàGORM DSN Ê†ºÂºèÔºâ
                        if 'link' in db_config:
                            dsn_info = parse_gorm_dsn(db_config['link'])
                            if dsn_info:
                                db_info['config'].update(dsn_info)
                        
                        # ÊèêÂèñÂÖ∂‰ªñÂèÇÊï∞
                        for param in ['host', 'port', 'user', 'username', 'password', 'database', 'dbname', 'name', 'address']:
                            if param in db_config:
                                db_info['config'][param] = db_config[param]
                        
                        if db_info['config']:
                            db_connections.append(db_info)
                            logger.info(f"Ê∑ªÂä† {db_type} ÈÖçÁΩÆ: {db_info}")
        
        # Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞‰ªª‰ΩïÈÖçÁΩÆÔºåÂÜçËøõË°åÈÄíÂΩíÊèêÂèñ
        if not db_connections:
            logger.info("Êú™ÊâæÂà∞ÁâπÊÆäÈÖçÁΩÆÔºåËøõË°åÈÄíÂΩíÊèêÂèñ")
            extract_db_info(config_data)
    
    except Exception as e:
        logger.error(f"Ëß£ÊûêÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÊó∂Âá∫Èîô: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    except Exception as e:
        logger.warning(f"Failed to parse database config: {e}")
    
    return db_connections

@app.get("/api/database/project-databases/{project_name}")
async def get_project_databases(project_name: str):
    """Ëé∑ÂèñÈ°πÁõÆ‰∏≠ÁöÑÊâÄÊúâÊï∞ÊçÆÂ∫ìÊñá‰ª∂ÂíåÈÖçÁΩÆ"""
    try:
        import glob
        import yaml
        import toml
        
        project_path = get_project_path(project_name)
        
        if not project_path or not os.path.exists(project_path):
            return JSONResponse({"error": "Project not found"}, status_code=404)
        
        db_files = []
        db_configs = []
        
        # ÈÄíÂΩíÊêúÁ¥¢Êï∞ÊçÆÂ∫ìÊñá‰ª∂ÂíåÈÖçÁΩÆÊñá‰ª∂
        for root, dirs, files in os.walk(project_path):
            # Ë∑≥ËøáÂ∏∏ËßÅÁöÑÈùûÊï∞ÊçÆÂ∫ìÁõÆÂΩï
            dirs[:] = [d for d in dirs if d not in ['node_modules', '.git', '__pycache__', 'dist', 'build', 'vendor']]
            
            for file in files:
                full_path = os.path.join(root, file)
                relative_path = os.path.relpath(full_path, project_path)
                
                # ÊêúÁ¥¢ SQLite Êï∞ÊçÆÂ∫ìÊñá‰ª∂
                if file.endswith('.db') or file.endswith('.sqlite') or file.endswith('.sqlite3'):
                    file_size = os.path.getsize(full_path) if os.path.exists(full_path) else 0
                    
                    # È™åËØÅÊòØÂê¶ÊòØÊúâÊïàÁöÑ SQLite Êï∞ÊçÆÂ∫ì
                    is_valid = False
                    try:
                        conn = sqlite3.connect(full_path)
                        conn.execute("SELECT name FROM sqlite_master WHERE type='table' LIMIT 1;")
                        conn.close()
                        is_valid = True
                    except Exception:
                        pass
                    
                    db_files.append({
                        "name": file,
                        "path": relative_path,
                        "full_path": full_path,
                        "size": file_size,
                        "is_valid": is_valid,
                        "type": "sqlite"
                    })
                
                # ÊêúÁ¥¢ Go È°πÁõÆÁöÑÈÖçÁΩÆÊñá‰ª∂
                elif file.endswith(('.yaml', '.yml', '.toml')) or file in ['.env', 'go.mod']:
                    # ÊîØÊåÅÂ∏¶ÁéØÂ¢ÉÂêéÁºÄÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºàÂ¶Ç config.dev.toml, config.pro.tomlÔºâ
                    is_config_file = (
                        file in ['config.yaml', 'config.yml', 'config.toml', '.env', 'go.mod'] or
                        file.startswith('config.') and file.endswith(('.yaml', '.yml', '.toml'))
                    )
                    
                    # Âè™Â§ÑÁêÜÊ†πÁõÆÂΩïÁöÑÈÖçÁΩÆÊñá‰ª∂
                    if is_config_file and (root == project_path or relative_path.count('/') <= 1):
                        try:
                            config_data = None
                            config_type = None
                            
                            if file.endswith(('.yaml', '.yml')):
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    config_data = yaml.safe_load(f)
                                config_type = 'yaml'
                            elif file.endswith('.toml'):
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    config_data = toml.load(f)
                                config_type = 'toml'
                            elif file == '.env':
                                from dotenv import load_dotenv
                                config_data = {}
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    for line in f:
                                        line = line.strip()
                                        if line and not line.startswith('#') and '=' in line:
                                            key, value = line.split('=', 1)
                                            config_data[key.strip()] = value.strip()
                                config_type = 'env'
                            elif file == 'go.mod':
                                with open(full_path, 'r', encoding='utf-8') as f:
                                    config_data = {'module': '', 'go_version': ''}
                                    for line in f:
                                        if line.startswith('module '):
                                            config_data['module'] = line.split()[1]
                                        elif line.startswith('go '):
                                            config_data['go_version'] = line.split()[1]
                                config_type = 'go'
                            
                            if config_data:
                                # Ëß£ÊûêÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ
                                db_connections = parse_database_config(config_data, config_type)
                                
                                # ÊèêÂèñÁéØÂ¢É‰ø°ÊÅØ
                                env_info = None
                                if file.startswith('config.') and '.' in file[:-5]:
                                    # ÊèêÂèñÁéØÂ¢ÉÂêçÁß∞ÔºàÂ¶Ç config.dev.toml -> devÔºâ
                                    parts = file.split('.')
                                    if len(parts) >= 3:
                                        env_info = parts[1]
                                
                                db_configs.append({
                                    "name": file,
                                    "path": relative_path,
                                    "full_path": full_path,
                                    "type": config_type,
                                    "data": config_data,
                                    "db_connections": db_connections,
                                    "environment": env_info
                                })
                        except Exception as e:
                            logger.warning(f"Failed to read config file {file}: {e}")
        
        # ÊåâÊñá‰ª∂ÂêçÊéíÂ∫è
        db_files.sort(key=lambda x: x["name"])
        db_configs.sort(key=lambda x: x["name"])
        
        return {
            "project_name": project_name,
            "project_path": project_path,
            "databases": db_files,
            "configs": db_configs,
            "database_count": len(db_files),
            "config_count": len(db_configs)
        }
    except Exception as e:
        logger.error(f"Error getting project databases: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# --- Workflow API ---

class WorkflowSaveRequest(BaseModel):
    project_name: str
    workflow_name: str
    nodes: List[Dict[str, Any]]
    edges: List[Dict[str, Any]]

class WorkflowGenerateRequest(BaseModel):
    prompt: str

@app.post("/api/workflows/save")
async def save_workflow(req: WorkflowSaveRequest):
    """‰øùÂ≠òÂ∑•‰ΩúÊµÅ"""
    try:
        from backend.core.workflow_service import Workflow
        
        workflow = Workflow(
            id=None,
            name=req.workflow_name,
            nodes=req.nodes,
            edges=req.edges,
            project_name=req.project_name,
            created_at=datetime.now().isoformat(),
            updated_at=datetime.now().isoformat()
        )
        
        workflow_id = workflow_service.save_workflow(workflow)
        
        return {
            "success": True,
            "workflow_id": workflow_id,
            "message": "Â∑•‰ΩúÊµÅ‰øùÂ≠òÊàêÂäü"
        }
    except Exception as e:
        logger.error(f"Error saving workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/workflows/{project_name}")
async def get_workflows(project_name: str):
    """Ëé∑ÂèñÈ°πÁõÆÁöÑÊâÄÊúâÂ∑•‰ΩúÊµÅ"""
    try:
        workflows = workflow_service.get_workflows_by_project(project_name)
        return {
            "workflows": [
                {
                    "id": w.id,
                    "name": w.name,
                    "created_at": w.created_at,
                    "updated_at": w.updated_at,
                    "nodes_count": len(w.nodes),
                    "edges_count": len(w.edges)
                }
                for w in workflows
            ]
        }
    except Exception as e:
        logger.error(f"Error getting workflows: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.get("/api/workflows/{project_name}/{workflow_id}")
async def get_workflow(project_name: str, workflow_id: str):
    """Ëé∑ÂèñÂ∑•‰ΩúÊµÅËØ¶ÊÉÖ"""
    try:
        workflow = workflow_service.get_workflow(workflow_id)
        if not workflow:
            return JSONResponse({"error": "Workflow not found"}, status_code=404)
        
        return {
            "id": workflow.id,
            "name": workflow.name,
            "nodes": workflow.nodes,
            "edges": workflow.edges,
            "created_at": workflow.created_at,
            "updated_at": workflow.updated_at
        }
    except Exception as e:
        logger.error(f"Error getting workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.delete("/api/workflows/{project_name}/{workflow_id}")
async def delete_workflow(project_name: str, workflow_id: str):
    """Âà†Èô§Â∑•‰ΩúÊµÅ"""
    try:
        success = workflow_service.delete_workflow(workflow_id)
        if success:
            return {"success": True, "message": "Â∑•‰ΩúÊµÅÂà†Èô§ÊàêÂäü"}
        else:
            return JSONResponse({"error": "Workflow not found"}, status_code=404)
    except Exception as e:
        logger.error(f"Error deleting workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/workflows/generate")
async def generate_workflow(req: WorkflowGenerateRequest):
    """AI ÁîüÊàêÂ∑•‰ΩúÊµÅ"""
    try:
        result = workflow_service.generate_workflow_from_prompt(req.prompt)
        return result
    except Exception as e:
        logger.error(f"Error generating workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

@app.post("/api/workflows/{workflow_id}/execute")
async def execute_workflow(workflow_id: str, context: Dict[str, Any] = None):
    """ÊâßË°åÂ∑•‰ΩúÊµÅ"""
    try:
        workflow = workflow_service.get_workflow(workflow_id)
        if not workflow:
            return JSONResponse({"error": "Workflow not found"}, status_code=404)

        # Ëé∑ÂèñÈ°πÁõÆË∑ØÂæÑ
        project_path = project_registry.get_project_path(workflow.project_name)
        if not project_path:
            return JSONResponse({"error": "Project path not found"}, status_code=404)

        def normalize_workflow_graph(nodes: List[Dict[str, Any]], edges: List[Dict[str, Any]]) -> Dict[str, Any]:
            type_mapping = {
                "readFile": "fileRead",
                "writeFile": "fileWrite",
                "searchFiles": "search",
                "gitCommit": "git",
                "gitBranch": "git",
            }
            normalized_nodes = []
            for node in nodes:
                if not isinstance(node, dict):
                    continue
                node_type = node.get("type")
                mapped_type = type_mapping.get(node_type, node_type)
                if mapped_type == node_type:
                    normalized_nodes.append(node)
                else:
                    normalized_nodes.append({**node, "type": mapped_type})
            return {"nodes": normalized_nodes, "edges": edges}

        # ÊâßË°åÂ∑•‰ΩúÊµÅ
        result = await workflow_executor.execute_workflow(
            workflow_id,
            normalize_workflow_graph(workflow.nodes, workflow.edges),
            project_path,
            context
        )

        return {
            "success": result.success,
            "steps_completed": result.steps_completed,
            "steps_total": result.steps_total,
            "logs": result.logs,
            "output": result.output,
            "error": result.error
        }
    except Exception as e:
        logger.error(f"Error executing workflow: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

async def execute_workflow_stream(workflow_id: str, project_name: str = Query(None)):
    """ÊµÅÂºèÊâßË°åÂ∑•‰ΩúÊµÅÔºàSSEÔºâ- ÂÜÖÈÉ®ÂÆûÁé∞ÔºàÁî®‰∏çÂÜ≤Á™ÅÁöÑË∑ØÁî±Ë∞ÉÁî®Ôºâ"""
    def normalize_workflow_graph(nodes: List[Dict[str, Any]], edges: List[Dict[str, Any]]) -> Dict[str, Any]:
        type_mapping = {
            "readFile": "fileRead",
            "writeFile": "fileWrite",
            "searchFiles": "search",
            "gitCommit": "git",
            "gitBranch": "git",
        }
        normalized_nodes = []
        for node in nodes:
            if not isinstance(node, dict):
                continue
            node_type = node.get("type")
            mapped_type = type_mapping.get(node_type, node_type)
            if mapped_type == node_type:
                normalized_nodes.append(node)
            else:
                normalized_nodes.append({**node, "type": mapped_type})
        return {"nodes": normalized_nodes, "edges": edges}

    async def event_generator():
        execution_id = None
        steps_total = 0
        steps_completed = 0
        started_at = datetime.now().isoformat()
        try:
            workflow = workflow_service.get_workflow(workflow_id)
            workflow_data = None
            if workflow:
                workflow_data = {
                    "project_name": workflow.project_name,
                    "nodes": workflow.nodes,
                    "edges": workflow.edges
                }
            else:
                file_path = os.path.join(workflow_service.storage_dir, f"{workflow_id}.json")
                if os.path.exists(file_path):
                    with open(file_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    workflow_data = {
                        "project_name": data.get("project_name"),
                        "nodes": data.get("nodes", []),
                        "edges": data.get("edges", [])
                    }

            if not workflow_data:
                execution_id = f"exec_{workflow_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                workflow_execution_store.create(execution_id, {
                    "workflow_id": workflow_id,
                    "workflow_name": None,
                    "project_name": project_name,
                    "status": "failed",
                    "started_at": started_at,
                    "ended_at": datetime.now().isoformat(),
                    "error": "Workflow not found"
                })
                err_event = {'type': 'error', 'error': 'Workflow not found', 'execution_id': execution_id, 'timestamp': datetime.now().isoformat()}
                workflow_execution_store.append_event(execution_id, err_event)
                yield f"data: {json.dumps(err_event, ensure_ascii=False)}\n\n"
                return

            resolved_project_name = project_name or workflow_data.get("project_name")
            project_path = get_project_path(resolved_project_name)

            normalized_graph = normalize_workflow_graph(workflow_data.get("nodes", []), workflow_data.get("edges", []))

            execution_id = f"exec_{workflow_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            workflow_execution_store.create(execution_id, {
                "workflow_id": workflow_id,
                "workflow_name": (workflow.name if workflow else None),
                "project_name": resolved_project_name,
                "status": "running",
                "started_at": started_at,
                "steps_total": 0,
                "steps_completed": 0
            })

            async for update in workflow_executor.execute_workflow_stream(
                workflow_id,
                normalized_graph,
                project_path,
                context={"project_name": resolved_project_name}
            ):
                if isinstance(update, dict):
                    update = {**update, "execution_id": execution_id}

                if isinstance(update, dict) and update.get("type") == "plan":
                    steps_total = int(update.get("steps_total") or 0)
                    workflow_execution_store.update(execution_id, {"steps_total": steps_total})

                if isinstance(update, dict) and update.get("type") == "step_complete":
                    steps_completed += 1
                    workflow_execution_store.update(execution_id, {"steps_completed": steps_completed})

                if isinstance(update, dict) and update.get("type") == "error":
                    workflow_execution_store.update(execution_id, {
                        "status": "failed",
                        "ended_at": datetime.now().isoformat(),
                        "error": update.get("error")
                    })

                if isinstance(update, dict) and update.get("type") == "complete":
                    workflow_execution_store.update(execution_id, {
                        "status": "completed",
                        "ended_at": datetime.now().isoformat(),
                        "steps_total": steps_total,
                        "steps_completed": steps_completed
                    })

                if isinstance(update, dict):
                    workflow_execution_store.append_event(execution_id, update)

                yield f"data: {json.dumps(update, ensure_ascii=False)}\n\n"
        except Exception as e:
            logger.error(f"Error executing workflow stream: {e}")
            if not execution_id:
                execution_id = f"exec_{workflow_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                workflow_execution_store.create(execution_id, {
                    "workflow_id": workflow_id,
                    "workflow_name": (workflow.name if workflow else None) if 'workflow' in locals() else None,
                    "project_name": project_name,
                    "status": "failed",
                    "started_at": started_at,
                    "ended_at": datetime.now().isoformat(),
                    "error": str(e)
                })
            err_event = {'type': 'error', 'error': str(e), 'execution_id': execution_id, 'timestamp': datetime.now().isoformat()}
            workflow_execution_store.append_event(execution_id, err_event)
            yield f"data: {json.dumps(err_event, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    )

@app.get("/api/workflows/stream/{workflow_id}/execute")
async def execute_workflow_stream_route(workflow_id: str, project_name: str = Query(None)):
    return await execute_workflow_stream(workflow_id, project_name)

@app.get("/api/workflows/executions")
async def list_workflow_executions(
    limit: int = Query(50, ge=1, le=200),
    workflow_id: str = Query(None),
    project_name: str = Query(None)
):
    items = workflow_execution_store.list(limit=limit, workflow_id=workflow_id, project_name=project_name)
    return {"success": True, "executions": items}

@app.get("/api/workflows/executions/{execution_id}")
async def get_workflow_execution(execution_id: str):
    record = workflow_execution_store.get(execution_id)
    if not record:
        return JSONResponse({"error": "Execution not found"}, status_code=404)
    return {"success": True, "execution": record}


# --- Catch-all Ë∑ØÁî± ---

@app.api_route("/api/{path_name:path}", methods=["GET", "POST", "PUT", "DELETE"])
async def catch_all(path_name: str, request: Request):
    """Catch-all Ë∑ØÁî± - Â§ÑÁêÜÊú™ÂÆûÁé∞ÁöÑ API Á´ØÁÇπ"""
    logger.warning(f"Êú™Â§ÑÁêÜÁöÑ API ËØ∑Ê±Ç: {request.method} /api/{path_name}")

    # MCP Áõ∏ÂÖ≥ÁöÑ API
    if path_name.startswith("mcp-utils/"):
        return JSONResponse(content={
            "status": "not-implemented",
            "message": f"MCP endpoint '{path_name}' is not implemented"
        }, status_code=200)

    # ÈªòËÆ§ÂìçÂ∫î
    return JSONResponse(content={"status": "mocked", "sessions": [], "hasMore": False}, status_code=200)


if __name__ == "__main__":
    import uvicorn
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    uvicorn.run(app, host="0.0.0.0", port=8000)
