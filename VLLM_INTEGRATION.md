# vLLM é›†æˆæŒ‡å—

## vLLM ç®€ä»‹

**vLLM** (virtual LLM) æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’ŒæœåŠ¡åº“,ç”± UC Berkeley çš„ Sky Computing Lab å¼€å‘ã€‚

### æ ¸å¿ƒä¼˜åŠ¿

1. **é«˜ååé‡** - é€šè¿‡ PagedAttention æœºåˆ¶,å®ç°é«˜è¾¾ 24x çš„ååé‡æå‡
2. **å†…å­˜é«˜æ•ˆ** - ä¼˜åŒ–å†…å­˜ç®¡ç†,æ”¯æŒåœ¨æœ‰é™èµ„æºä¸Šè¿è¡Œå¤§æ¨¡å‹
3. **OpenAI å…¼å®¹** - æä¾›ä¸ OpenAI API å…¼å®¹çš„ HTTP æœåŠ¡æ¥å£
4. **å¤šæ¨¡å‹æ”¯æŒ** - æ”¯æŒ LLaMAã€Qwenã€GLMã€DeepSeek ç­‰ä¸»æµæ¨¡å‹
5. **æ˜“äºéƒ¨ç½²** - ä¸€è¡Œå‘½ä»¤å¯åŠ¨æœåŠ¡

### é€‚ç”¨åœºæ™¯

- âœ… æœ¬åœ°éƒ¨ç½²å¤§æ¨¡å‹
- âœ… é«˜å¹¶å‘æ¨ç†æœåŠ¡
- âœ… ç§æœ‰åŒ– AI åº”ç”¨
- âœ… æˆæœ¬æ•æ„Ÿçš„åœºæ™¯
- âœ… éœ€è¦ä½å»¶è¿Ÿçš„åº”ç”¨

## ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚

**æœ€ä½é…ç½®**:
- GPU: NVIDIA GPU with 20GB+ VRAM
- å†…å­˜: 16GB+ RAM
- å­˜å‚¨: 50GB+ SSD

**æ¨èé…ç½®**:
- GPU: NVIDIA RTX 4090 æˆ– A100
- å†…å­˜: 32GB+ RAM
- å­˜å‚¨: 500GB+ NVMe SSD

### è½¯ä»¶è¦æ±‚

- æ“ä½œç³»ç»Ÿ: Linux/macOS/Windows
- Python: 3.8 - 3.12
- CUDA: 11.8+ (å¦‚æœä½¿ç”¨ GPU)
- PyTorch: 2.0+

## å®‰è£…æ­¥éª¤

### 1. åˆ›å»º Python ç¯å¢ƒ

```bash
# ä½¿ç”¨ conda åˆ›å»ºæ–°ç¯å¢ƒ(æ¨è)
conda create -n vllm_env python=3.12
conda activate vllm_env

# æˆ–ä½¿ç”¨ uv(æ›´å¿«)
pip install uv
uv venv vllm_env
source vllm_env/bin/activate
```

### 2. å®‰è£… vLLM

```bash
# ä½¿ç”¨ pip å®‰è£…
pip install vllm

# éªŒè¯å®‰è£…
vllm --version
```

**é‡è¦æç¤º**:
- âš ï¸ å»ºè®®ä½¿ç”¨å…¨æ–°çš„ conda ç¯å¢ƒ,é¿å… PyTorch ç‰ˆæœ¬å†²çª
- âš ï¸ vLLM éœ€è¦ç¼–è¯‘ CUDA å†…æ ¸,å®‰è£…æ—¶é—´è¾ƒé•¿(5-10åˆ†é’Ÿ)
- âš ï¸ Windows æ”¯æŒæœ‰é™,å¼ºçƒˆå»ºè®®ä½¿ç”¨ Linux æˆ– WSL2

### 3. å®‰è£…ä¾èµ–

```bash
# å®‰è£…å¿…è¦çš„ä¾èµ–
pip install transformers accelerate huggingface_hub
pip install ipywidgets  # å¦‚æœä½¿ç”¨ Jupyter
```

## éƒ¨ç½²æ–¹å¼

### æ–¹å¼ 1: å‘½ä»¤è¡Œå¯åŠ¨(æ¨è)

```bash
# åŸºæœ¬å¯åŠ¨
vllm serve Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --api-key token-abc123

# å¸¦å‚æ•°å¯åŠ¨
vllm serve Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype auto \
  --gpu-memory-utilization 0.95 \
  --max-model-len 131072 \
  --api-key token-abc123
```

**å‚æ•°è¯´æ˜**:
- `--host`: æœåŠ¡ç›‘å¬åœ°å€
- `--port`: æœåŠ¡ç«¯å£
- `--dtype`: æ•°æ®ç±»å‹(auto/bfloat16/float16)
- `--gpu-memory-utilization`: GPU å†…å­˜åˆ©ç”¨ç‡(0-1)
- `--max-model-len`: æœ€å¤§æ¨¡å‹é•¿åº¦
- `--api-key`: API å¯†é’¥

### æ–¹å¼ 2: Python API ä½¿ç”¨

```python
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    gpu_memory_utilization=0.95,
    max_model_len=131072,
    trust_remote_code=True
)

# åˆå§‹åŒ– tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=512
)

# ç”Ÿæˆæ–‡æœ¬
prompts = ["ä½ å¥½,è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
```

### æ–¹å¼ 3: OpenAI å…¼å®¹ API

å¯åŠ¨æœåŠ¡å,å¯ä»¥ä½¿ç”¨ OpenAI SDK è°ƒç”¨:

```python
from openai import OpenAI

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="token-abc123",
)

# è°ƒç”¨èŠå¤©æ¥å£
response = client.chat.completions.create(
    model="Qwen/Qwen2.5-7B-Instruct",
    messages=[
        {"role": "user", "content": "ä½ å¥½,è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ],
    temperature=0.7,
    max_tokens=512
)

print(response.choices[0].message.content)
```

## é›†æˆåˆ°é¡¹ç›®

### 1. åç«¯é›†æˆ

åœ¨ `backend/server.py` ä¸­æ·»åŠ  vLLM æ”¯æŒ:

```python
from vllm import LLM, SamplingParams
import asyncio

# vLLM æœåŠ¡ç±»
class VLLMService:
    def __init__(self):
        self.llm = None
        self.tokenizer = None
        self._initialize()
    
    def _initialize(self):
        try:
            self.llm = LLM(
                model="Qwen/Qwen2.5-7B-Instruct",
                gpu_memory_utilization=0.95,
                max_model_len=131072,
                trust_remote_code=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
            logger.info("vLLM æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"vLLM æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.llm = None
    
    async def generate(self, prompt: str, **kwargs):
        """ç”Ÿæˆæ–‡æœ¬"""
        if not self.llm:
            raise Exception("vLLM æœåŠ¡æœªåˆå§‹åŒ–")
        
        sampling_params = SamplingParams(
            temperature=kwargs.get('temperature', 0.7),
            top_p=kwargs.get('top_p', 0.95),
            max_tokens=kwargs.get('max_tokens', 512)
        )
        
        outputs = self.llm.generate([prompt], sampling_params)
        return outputs[0].outputs[0].text

# åˆ›å»ºå…¨å±€å®ä¾‹
vllm_service = VLLMService()

# æ·»åŠ  API ç«¯ç‚¹
@app.post("/api/vllm/generate")
async def vllm_generate(request: Request):
    """vLLM ç”Ÿæˆæ¥å£"""
    try:
        data = await request.json()
        prompt = data.get("prompt")
        result = await vllm_service.generate(prompt, **data)
        return JSONResponse(content={"success": True, "text": result})
    except Exception as e:
        return JSONResponse(content={"success": False, "error": str(e)}, status_code=500)
```

### 2. å‰ç«¯é›†æˆ

åœ¨ `frontend/src/utils/api.js` ä¸­æ·»åŠ  vLLM API è°ƒç”¨:

```javascript
export async function vllmGenerate(prompt, options = {}) {
  const response = await authenticatedFetch('/api/vllm/generate', {
    method: 'POST',
    body: JSON.stringify({
      prompt,
      ...options
    }),
  });
  
  if (response.ok) {
    const data = await response.json();
    if (data.success) {
      return data.text;
    }
  }
  throw new Error('vLLM ç”Ÿæˆå¤±è´¥');
}
```

### 3. åœ¨ Chat ç»„ä»¶ä¸­ä½¿ç”¨

```jsx
import { vllmGenerate } from '../utils/api';

function ChatInterface() {
  const [useVLLM, setUseVLLM] = useState(false);
  
  const handleSendMessage = async () => {
    if (useVLLM) {
      // ä½¿ç”¨ vLLM
      const response = await vllmGenerate(input, {
        temperature: 0.7,
        max_tokens: 512
      });
      // å¤„ç†å“åº”...
    } else {
      // ä½¿ç”¨åŸæœ‰æ–¹å¼
      // ...
    }
  };
  
  return (
    <div>
      <button onClick={() => setUseVLLM(!useVLLM)}>
        {useVLLM ? 'ä½¿ç”¨ vLLM' : 'ä½¿ç”¨ iFlow'}
      </button>
      {/* èŠå¤©ç•Œé¢ */}
    </div>
  );
}
```

## é…ç½®é€‰é¡¹

### æ¨¡å‹é€‰æ‹©

æ¨èæ¨¡å‹:

1. **Qwen2.5-7B-Instruct** - ä¸­æ–‡ä¼˜ç§€,7B å‚æ•°
2. **GLM-4-9B-Chat** - æ™ºè°± AI,9B å‚æ•°
3. **DeepSeek-R1** - æ¨ç†èƒ½åŠ›å¼º
4. **Llama-3.1-8B** - Meta å¼€æº

### æ€§èƒ½è°ƒä¼˜

```bash
# é«˜ååé‡é…ç½®
vllm serve Qwen/Qwen2.5-7B-Instruct \
  --tensor-parallel-size 2 \
  --max-num-batched-tokens 8192 \
  --enable-chunked-prefill

# ä½å»¶è¿Ÿé…ç½®
vllm serve Qwen/Qwen2.5-7B-Instruct \
  --max-num-seqs 1 \
  --disable-log-requests

# å†…å­˜ä¼˜åŒ–é…ç½®
vllm serve Qwen/Qwen2.5-7B-Instruct \
  --gpu-memory-utilization 0.90 \
  --max-model-len 65536 \
  --dtype float16
```

## ç›‘æ§å’Œæ—¥å¿—

### å¯ç”¨ Prometheus ç›‘æ§

```bash
vllm serve Qwen/Qwen2.5-7B-Instruct \
  --metrics-port 8001
```

### æŸ¥çœ‹æ—¥å¿—

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
vllm serve Qwen/Qwen2.5-7B-Instruct \
  --disable-log-requests \
  --log-level DEBUG
```

## æ•…éšœæ’é™¤

### 1. OOM (Out of Memory)

è§£å†³æ–¹æ¡ˆ:
- å‡å°‘ `max_model_len`
- å‡å°‘ `gpu-memory-utilization`
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹
- å¢åŠ  GPU æ•°é‡(`--tensor-parallel-size`)

### 2. CUDA ç‰ˆæœ¬ä¸å…¼å®¹

è§£å†³æ–¹æ¡ˆ:
- ä½¿ç”¨å…¨æ–°çš„ conda ç¯å¢ƒ
- ä»æºä»£ç ç¼–è¯‘å®‰è£… vLLM
- æ£€æŸ¥ CUDA ç‰ˆæœ¬æ˜¯å¦åŒ¹é…

### 3. æ¨¡å‹ä¸‹è½½å¤±è´¥

è§£å†³æ–¹æ¡ˆ:
- ä½¿ç”¨ ModelScope(å›½å†…)
- æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å¹¶æŒ‡å®šæœ¬åœ°è·¯å¾„
- è®¾ç½®ä»£ç†

### 4. Windows å…¼å®¹æ€§

è§£å†³æ–¹æ¡ˆ:
- ä½¿ç”¨ WSL2
- ä½¿ç”¨ Linux æœåŠ¡å™¨
- ä½¿ç”¨ Docker å®¹å™¨

## æ€§èƒ½å¯¹æ¯”

æ ¹æ®åŸºå‡†æµ‹è¯•:

| åœºæ™¯ | vLLM | HuggingFace TGI | æå‡å€æ•° |
|------|------|-----------------|---------|
| é«˜å¹¶å‘åå | 24x | 1x | 24x |
| å•ç”¨æˆ·å»¶è¿Ÿ | 1x | 0.8x | 0.8x |
| GPU å†…å­˜åˆ©ç”¨ç‡ | 95% | 70% | 1.36x |

## æœ€ä½³å®è·µ

1. **ç¯å¢ƒéš”ç¦»**: ä½¿ç”¨ç‹¬ç«‹çš„ conda ç¯å¢ƒ
2. **æ¨¡å‹ç¼“å­˜**: æå‰ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
3. **æ‰¹é‡å¤„ç†**: åˆ©ç”¨ vLLM çš„æ‰¹å¤„ç†èƒ½åŠ›
4. **ç›‘æ§**: å¯ç”¨ Prometheus ç›‘æ§
5. **æ—¥å¿—**: è®°å½•è¯¦ç»†çš„æ¨ç†æ—¥å¿—
6. **å¤‡ä»½**: å®šæœŸå¤‡ä»½æ¨¡å‹å’Œé…ç½®

## æ€»ç»“

vLLM æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ LLM æ¨ç†å¼•æ“,ç‰¹åˆ«é€‚åˆ:

- âœ… éœ€è¦é«˜ååé‡çš„åœºæ™¯
- âœ… æˆæœ¬æ•æ„Ÿçš„éƒ¨ç½²
- âœ… ç§æœ‰åŒ–éœ€æ±‚
- âœ… å¤šæ¨¡å‹åˆ‡æ¢

é€šè¿‡é›†æˆ vLLM,ä½ çš„ç³»ç»Ÿå¯ä»¥è·å¾—:
- ğŸš€ æ›´å¿«çš„æ¨ç†é€Ÿåº¦
- ğŸ’° æ›´ä½çš„æˆæœ¬
- ğŸ”’ æ›´å¥½çš„éšç§ä¿æŠ¤
- ğŸ¯ æ›´çµæ´»çš„éƒ¨ç½²é€‰é¡¹